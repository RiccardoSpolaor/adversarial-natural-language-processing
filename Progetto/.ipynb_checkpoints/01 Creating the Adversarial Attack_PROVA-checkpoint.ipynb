{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "\n",
    "SEED = 123\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.blackBox import BlackBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "black_box = BlackBox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('pickle\\\\data.pickle', 'rb') as f:\n",
    "    x_test, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Unhinged follows the typical plot of the early 80\\'s slasher trend. Pretty Young Girls In Peril. I have to give it up for the filmmaker who used a helicopter for some of the early road-trip shots, you actually think for a second there\\'s going to be quality in the production. Watching \"Unhinged\" was like seeing an amateur acting class go through it\\'s warm-up. Some of the most awkward, badly lit, overlong scenes are played out with the gusto of a Valium overdose. I wondered why they didn\\'t just put the cue-cards on camera so the actresses wouldn\\'t have to constantly shift their gaze. The two main girls were obviously chosen for their T&A factor rather than talent. Laurel Munson as the main chick Terry is as exciting as watching paint dry. Two nude scenes make for an adolescent thrill. Janet Penner and Virginia Settle as the crazy/creepy daughter and mother the chicks find themselves stranded with compete for Worst Acting Ever. Long pauses, weird expressions, emphasis on the wrong word, it\\'s all there and is a delight for those of us out there who love bad films. The scenes shift suddenly with long black-outs you could drive a Mack truck through. Cartoon lightning crashes across shots without even bothering to show the sky. Eighties eyeshadow assaults the viewer. But ya know, it grew on me. I felt sorry for it. I wanted to hug it, kiss it\\'s boo-boos and make it better. The ending doesn\\'t make up for the damage it\\'s caused but I grinned anyway. I have my own theories regarding the whole \"banned\" hype and hope that anyone who chooses to view this film does so with substantial substance abuse and a sense of humor. Otherwise pass.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0061943284\n"
     ]
    }
   ],
   "source": [
    "print(black_box.predict_sentiment(x_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturb Algorythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = list(zip(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Unhinged follows the typical plot of the early 80\\'s slasher trend. Pretty Young Girls In Peril. I have to give it up for the filmmaker who used a helicopter for some of the early road-trip shots, you actually think for a second there\\'s going to be quality in the production. Watching \"Unhinged\" was like seeing an amateur acting class go through it\\'s warm-up. Some of the most awkward, badly lit, overlong scenes are played out with the gusto of a Valium overdose. I wondered why they didn\\'t just put the cue-cards on camera so the actresses wouldn\\'t have to constantly shift their gaze. The two main girls were obviously chosen for their T&A factor rather than talent. Laurel Munson as the main chick Terry is as exciting as watching paint dry. Two nude scenes make for an adolescent thrill. Janet Penner and Virginia Settle as the crazy/creepy daughter and mother the chicks find themselves stranded with compete for Worst Acting Ever. Long pauses, weird expressions, emphasis on the wrong word, it\\'s all there and is a delight for those of us out there who love bad films. The scenes shift suddenly with long black-outs you could drive a Mack truck through. Cartoon lightning crashes across shots without even bothering to show the sky. Eighties eyeshadow assaults the viewer. But ya know, it grew on me. I felt sorry for it. I wanted to hug it, kiss it\\'s boo-boos and make it better. The ending doesn\\'t make up for the damage it\\'s caused but I grinned anyway. I have my own theories regarding the whole \"banned\" hype and hope that anyone who chooses to view this film does so with substantial substance abuse and a sense of humor. Otherwise pass.',\n",
       "  0),\n",
       " ('Wow. I saw this movie and \"Up\" on the same day within an hour of each other at different theaters. I saw \"Mr Bug\" first, and was then totally disappointed in \"Up\"\\'s follow-up. What a beautiful and touching film! Movies of the 1930s and 40s to us nowadays can be irking with their melodramatic acting and dialog, but as animation the same melodrama and groaning humor can be wonderful. And the soft \"organic\" lines of 30s drawing AND the music just puts you in a nice comfortable mood and you can enjoy the show with all its little characters: ladybugs, grasshoppers, bees, snails, stinkbugs, flies, mosquitoes, beetles, crickets, and more each with all their own cute little (but not overbearing) idiosyncrasies. The interaction with the human world, from nemesis (cigar smokers, high-heel wearers, innocent kick-the-can playing kids) to the kind-hearted, and to the unknown destroyers, is realistic and fascinating. You care for the bugs, AND Dick and Mary. The protagonist Hoppity is not some perfect superman who comes to \"set things right\" but a starry-eyed optimist who leads everyone down the garden path (literally!), and every time you think it\\'s going to end happily in 1930s style, along comes another roadblock...! I was on the edge of my seat much more than with \"Up.\" I walked out of the movie theater grinning and chuckling: something that hasn\\'t happened in a long long long long time!',\n",
       "  1),\n",
       " (\"I accept that most 50's horror aren't scary by today's standards, but what the hell is this? When you see a title like this you expect to see blood and a blood thirsty beast. Instead we get no blood at all and a beast who either wants to take over the world or live in peace on Earth....yeah which is what the people wanted.<br /><br />The overall story is fine with the astronaut coming back to life and being one with the beast....but the title really kills the movie. Night of the Beast would have made the fans more happy because there really isn't any blood to speak of.<br /><br />I like how the 50's movies had endings that left room for a sequel but wisely never made one. This movie isn't the worst i've ever seen but its almost up there.<br /><br />2 out of 10\",\n",
       "  0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population[0][0]\n",
    "population[0][1]\n",
    "#a,b = zip(*population)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISTANCE MATRIX CALCULATION\n",
    "\n",
    "```\n",
    "using counter fitted word vectors\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "\n",
    "with open(\"counter_fitted_word_vectors\\\\counter-fitted-vectors.txt\", \"r\",errors ='ignore', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_dict[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.preprocessing import Preprocesser\n",
    "\n",
    "x_test = [Preprocesser.raw_text_preprocessing(sentence) for sentence in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40570\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAXLEN = 50_000 #60_000\n",
    "\n",
    "tokenizer = Tokenizer() #(MAXLEN)\n",
    "\n",
    "tokenizer.fit_on_texts([Preprocesser.test_data_preprocessing_for_tokenization(text, embeddings_dict) for text in x_test])\n",
    "\n",
    "print(len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef words_in_embedding(embeddings_dict, tokenizer):\\n    elem = []\\n    for w in tokenizer.word_index.keys():\\n        if w not in embeddings_dict.keys():\\n            elem += [w]\\n    print(elem)\\n\\nwords_in_embedding(embeddings_dict, tokenizer)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def words_in_embedding(embeddings_dict, tokenizer):\n",
    "    elem = []\n",
    "    for w in tokenizer.word_index.keys():\n",
    "        if w not in embeddings_dict.keys():\n",
    "            elem += [w]\n",
    "    print(elem)\n",
    "\n",
    "words_in_embedding(embeddings_dict, tokenizer)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fawn',\n",
       " 'schlegel',\n",
       " 'tilton',\n",
       " 'clotted',\n",
       " 'trawling',\n",
       " 'kalmar',\n",
       " 'tasos',\n",
       " 'canes',\n",
       " 'sprague',\n",
       " 'brockton',\n",
       " 'mutinies',\n",
       " 'vano',\n",
       " 'crossbar',\n",
       " 'hermano',\n",
       " 'jemmy',\n",
       " 'grenadiers',\n",
       " 'stipulate',\n",
       " 'capoeira',\n",
       " 'broward',\n",
       " 'caramels',\n",
       " 'chameleons',\n",
       " 'asami',\n",
       " 'immunities',\n",
       " 'fuera',\n",
       " 'thrace',\n",
       " 'kublai',\n",
       " 'gaskets',\n",
       " 'snuggles',\n",
       " 'splendiferous',\n",
       " 'scraper',\n",
       " 'ffor',\n",
       " 'deadheads',\n",
       " 'selassie',\n",
       " 'centimeter',\n",
       " 'opportunists',\n",
       " 'warmongering',\n",
       " 'numeral',\n",
       " 'widget',\n",
       " 'zlotys',\n",
       " 'chine',\n",
       " 'chino',\n",
       " 'sheung',\n",
       " 'quart',\n",
       " 'naturel',\n",
       " 'kumbaya',\n",
       " 'kido',\n",
       " 'millimetres',\n",
       " 'topography',\n",
       " 'jäger',\n",
       " 'battista',\n",
       " 'ramstein',\n",
       " 'caned',\n",
       " 'grahams',\n",
       " 'excu',\n",
       " 'borstal',\n",
       " 'hermana',\n",
       " 'expeditionary',\n",
       " 'unpack',\n",
       " 'murchison',\n",
       " 'lomax',\n",
       " 'matilde',\n",
       " 'zinnias',\n",
       " 'hyatt',\n",
       " 'wudang',\n",
       " 'pooper',\n",
       " 'pinta',\n",
       " 'carew',\n",
       " 'rayon',\n",
       " 'cocksucker',\n",
       " 'mcmuffin',\n",
       " 'sugarless',\n",
       " 'clews',\n",
       " 'cutback',\n",
       " 'essie',\n",
       " 'canaries',\n",
       " 'shaitan',\n",
       " 'stoller',\n",
       " 'pigment',\n",
       " 'domed',\n",
       " 'souci',\n",
       " 'amaya',\n",
       " 'tulio',\n",
       " 'farmlands',\n",
       " 'disengaging',\n",
       " 'kuen',\n",
       " 'peux',\n",
       " 'assing',\n",
       " 'patricide',\n",
       " 'peut',\n",
       " 'airbags',\n",
       " 'oceana',\n",
       " 'activating',\n",
       " 'avondale',\n",
       " 'burleigh',\n",
       " 'fiv',\n",
       " 'fiy',\n",
       " 'fib',\n",
       " 'fif',\n",
       " 'fie',\n",
       " 'menachem',\n",
       " 'fii',\n",
       " 'fio',\n",
       " 'fil',\n",
       " 'bristle',\n",
       " 'foolin',\n",
       " 'vouchers',\n",
       " 'hypoxia',\n",
       " 'preemie',\n",
       " 'être',\n",
       " 'defecate',\n",
       " 'bartok',\n",
       " 'mcgonagall',\n",
       " 'lucero',\n",
       " 'klinger',\n",
       " 'arroz',\n",
       " 'platelets',\n",
       " 'phoenicians',\n",
       " 'grapefruits',\n",
       " 'veracruz',\n",
       " 'stabilised',\n",
       " 'buttercups',\n",
       " 'smelters',\n",
       " 'whaddya',\n",
       " 'indiscretion',\n",
       " 'mazur',\n",
       " 'constants',\n",
       " 'nastya',\n",
       " 'guitarists',\n",
       " 'frenchie',\n",
       " 'nightdress',\n",
       " 'fansub',\n",
       " 'berwick',\n",
       " 'chickamauga',\n",
       " 'parkson',\n",
       " 'chiara',\n",
       " 'ministries',\n",
       " 'miaow',\n",
       " 'mendacity',\n",
       " 'curiouser',\n",
       " 'suhani',\n",
       " 'mesmer',\n",
       " 'maladies',\n",
       " 'kashima',\n",
       " 'chevron',\n",
       " 'dupuis',\n",
       " 'antilles',\n",
       " 'viets',\n",
       " 'reuben',\n",
       " 'olda',\n",
       " 'caitlyn',\n",
       " 'cisterns',\n",
       " 'aaaahhh',\n",
       " 'knowwhere',\n",
       " 'awacs',\n",
       " 'regulator',\n",
       " 'shampoos',\n",
       " 'dozer',\n",
       " 'whitcomb',\n",
       " 'sukie',\n",
       " 'longo',\n",
       " 'resignations',\n",
       " 'soave',\n",
       " 'comandante',\n",
       " 'britches',\n",
       " 'workmates',\n",
       " 'greenback',\n",
       " 'concedes',\n",
       " 'sugarcane',\n",
       " 'volkan',\n",
       " 'baldy',\n",
       " 'purges',\n",
       " 'maja',\n",
       " 'ragnarok',\n",
       " 'nettle',\n",
       " 'cumbia',\n",
       " 'ofhis',\n",
       " 'trompe',\n",
       " 'hereford',\n",
       " 'terese',\n",
       " 'halpern',\n",
       " 'locos',\n",
       " 'fennel',\n",
       " 'ulcer',\n",
       " 'chelios',\n",
       " 'faure',\n",
       " 'bullshitted',\n",
       " 'excavator',\n",
       " 'pianissimo',\n",
       " 'kerns',\n",
       " 'pawel',\n",
       " 'neighs',\n",
       " 'hoeing',\n",
       " 'venez',\n",
       " 'plata',\n",
       " 'puchi',\n",
       " 'umbilical',\n",
       " 'firenze',\n",
       " 'iate',\n",
       " 'mcnab',\n",
       " 'chyron',\n",
       " 'platz',\n",
       " 'onight',\n",
       " 'goomba',\n",
       " 'conant',\n",
       " 'cohan',\n",
       " 'heirloom',\n",
       " 'clarified',\n",
       " 'tlhe',\n",
       " 'sudanese',\n",
       " 'pinot',\n",
       " 'eriko',\n",
       " 'lalita',\n",
       " 'youngstown',\n",
       " 'irv',\n",
       " 'irn',\n",
       " 'conductive',\n",
       " 'lota',\n",
       " 'awway',\n",
       " 'overfed',\n",
       " 'pharisee',\n",
       " 'gwyn',\n",
       " 'tendons',\n",
       " 'flathead',\n",
       " 'airflow',\n",
       " 'miyake',\n",
       " 'lookit',\n",
       " 'windham',\n",
       " 'lookie',\n",
       " 'fondled',\n",
       " 'charnel',\n",
       " 'blondi',\n",
       " 'godhead',\n",
       " 'fra',\n",
       " 'fre',\n",
       " 'fri',\n",
       " 'frm',\n",
       " 'sommelier',\n",
       " 'tyr',\n",
       " 'sextus',\n",
       " 'speedin',\n",
       " 'spic',\n",
       " 'davi',\n",
       " 'kennan',\n",
       " 'zalman',\n",
       " 'contingencies',\n",
       " 'droppin',\n",
       " 'majid',\n",
       " 'unos',\n",
       " 'taxicabs',\n",
       " 'ditching',\n",
       " 'kohl',\n",
       " 'kayama',\n",
       " 'preachin',\n",
       " 'memoriam',\n",
       " 'honk',\n",
       " 'codename',\n",
       " 'dunkirk',\n",
       " 'altas',\n",
       " 'myocardial',\n",
       " 'cloquet',\n",
       " 'featherweight',\n",
       " 'chidori',\n",
       " 'guaran',\n",
       " 'kamini',\n",
       " 'smallness',\n",
       " 'tomkins',\n",
       " 'lonelier',\n",
       " 'dirtying',\n",
       " 'haberdashery',\n",
       " 'spassky',\n",
       " 'maketh',\n",
       " 'lassi',\n",
       " 'brandish',\n",
       " 'haa',\n",
       " 'hab',\n",
       " 'insubordination',\n",
       " 'haf',\n",
       " 'brays',\n",
       " 'sulfa',\n",
       " 'osman',\n",
       " 'dagan',\n",
       " 'lron',\n",
       " 'clustered',\n",
       " 'gade',\n",
       " 'lacroix',\n",
       " 'misdemeanors',\n",
       " 'trudie',\n",
       " 'bboys',\n",
       " 'postmaster',\n",
       " 'proportionally',\n",
       " 'deflection',\n",
       " 'tooken',\n",
       " 'altoona',\n",
       " 'choctaw',\n",
       " 'lockdown',\n",
       " 'varicose',\n",
       " 'christabel',\n",
       " 'laci',\n",
       " 'considerin',\n",
       " 'anomalous',\n",
       " 'oneness',\n",
       " 'adel',\n",
       " 'adem',\n",
       " 'lemmings',\n",
       " 'quaaludes',\n",
       " 'ginevra',\n",
       " 'bonsoir',\n",
       " 'mangos',\n",
       " 'talc',\n",
       " 'addington',\n",
       " 'handmaidens',\n",
       " 'tama',\n",
       " 'raper',\n",
       " 'avocados',\n",
       " 'contusions',\n",
       " 'bogies',\n",
       " 'verde',\n",
       " 'perfumed',\n",
       " 'bridgework',\n",
       " 'paneled',\n",
       " 'rousted',\n",
       " 'arrays',\n",
       " 'musik',\n",
       " 'layin',\n",
       " 'baze',\n",
       " 'smasher',\n",
       " 'servings',\n",
       " 'azt',\n",
       " 'moderne',\n",
       " 'minibar',\n",
       " 'megahertz',\n",
       " 'wacken',\n",
       " 'minar',\n",
       " 'minas',\n",
       " 'roly',\n",
       " 'oxygenated',\n",
       " 'schemin',\n",
       " 'rolo',\n",
       " 'tunas',\n",
       " 'suwon',\n",
       " 'moresby',\n",
       " 'devoe',\n",
       " 'hawker',\n",
       " 'eilat',\n",
       " 'aomori',\n",
       " 'cootchie',\n",
       " 'ordination',\n",
       " 'candiru',\n",
       " 'steelhead',\n",
       " 'rickson',\n",
       " 'chaim',\n",
       " 'disassociate',\n",
       " 'bandito',\n",
       " 'osu',\n",
       " 'seabees',\n",
       " 'sigismund',\n",
       " 'oso',\n",
       " 'supervises',\n",
       " 'ose',\n",
       " 'macht',\n",
       " 'amplification',\n",
       " 'netherworlds',\n",
       " 'unbeaten',\n",
       " 'precognition',\n",
       " 'silencers',\n",
       " 'laundryman',\n",
       " 'flatbed',\n",
       " 'lard',\n",
       " 'olympus',\n",
       " 'knapsack',\n",
       " 'southpaw',\n",
       " 'tangina',\n",
       " 'catalonia',\n",
       " 'homebody',\n",
       " 'refueled',\n",
       " 'batsmen',\n",
       " 'lapierre',\n",
       " 'adorno',\n",
       " 'sangha',\n",
       " 'madames',\n",
       " 'lengthened',\n",
       " 'thrumming',\n",
       " 'corazon',\n",
       " 'swizzle',\n",
       " 'gome',\n",
       " 'baggins',\n",
       " 'casse',\n",
       " 'azusa',\n",
       " 'coccyx',\n",
       " 'colson',\n",
       " 'vanderbilt',\n",
       " 'dusts',\n",
       " 'shamelessness',\n",
       " 'greely',\n",
       " 'cette',\n",
       " 'timbuktu',\n",
       " 'halverson',\n",
       " 'sprouted',\n",
       " 'abnormals',\n",
       " 'stencil',\n",
       " 'brewin',\n",
       " 'giraud',\n",
       " 'tsubaki',\n",
       " 'moseying',\n",
       " 'kimiko',\n",
       " 'moonrise',\n",
       " 'umph',\n",
       " 'swamiji',\n",
       " 'oldham',\n",
       " 'awaywith',\n",
       " 'callendar',\n",
       " 'pavilions',\n",
       " 'perishes',\n",
       " 'dampness',\n",
       " 'nembutal',\n",
       " 'avaunt',\n",
       " 'titanium',\n",
       " 'gregoire',\n",
       " 'genki',\n",
       " 'osage',\n",
       " 'locka',\n",
       " 'mailboxes',\n",
       " 'matzo',\n",
       " 'lookouts',\n",
       " 'locky',\n",
       " 'byung',\n",
       " 'litre',\n",
       " 'oswaldo',\n",
       " 'thermodynamics',\n",
       " 'meddler',\n",
       " 'ilia',\n",
       " 'atac',\n",
       " 'echidna',\n",
       " 'aici',\n",
       " 'mangy',\n",
       " 'remotes',\n",
       " 'forebears',\n",
       " 'wastepaper',\n",
       " 'professore',\n",
       " 'quarrelled',\n",
       " 'tula',\n",
       " 'jie',\n",
       " 'infirmities',\n",
       " 'tull',\n",
       " 'jiu',\n",
       " 'antipathy',\n",
       " 'relapsed',\n",
       " 'milker',\n",
       " 'dingleberry',\n",
       " 'shhh',\n",
       " 'frontage',\n",
       " 'sonal',\n",
       " 'hickory',\n",
       " 'akbar',\n",
       " 'deads',\n",
       " 'dido',\n",
       " 'didi',\n",
       " 'pealing',\n",
       " 'recharged',\n",
       " 'instability',\n",
       " 'withyou',\n",
       " 'savarin',\n",
       " 'commuting',\n",
       " 'prudently',\n",
       " 'keizer',\n",
       " 'vipassana',\n",
       " 'salads',\n",
       " 'rajni',\n",
       " 'isidro',\n",
       " 'hibernate',\n",
       " 'tienes',\n",
       " 'svalbard',\n",
       " 'workmanship',\n",
       " 'brasil',\n",
       " 'marlo',\n",
       " 'thurmond',\n",
       " 'marly',\n",
       " 'draggin',\n",
       " 'hemorrhoids',\n",
       " 'onn',\n",
       " 'onl',\n",
       " 'ony',\n",
       " 'mignon',\n",
       " 'veldt',\n",
       " 'ont',\n",
       " 'penitence',\n",
       " 'tï',\n",
       " 'magnifique',\n",
       " 'herptiles',\n",
       " 'cutty',\n",
       " 'cerebro',\n",
       " 'almond',\n",
       " 'portside',\n",
       " 'hoyle',\n",
       " 'boxin',\n",
       " 'southgate',\n",
       " 'jewelz',\n",
       " 'ishida',\n",
       " 'auroras',\n",
       " 'erudition',\n",
       " 'leeching',\n",
       " 'pimply',\n",
       " 'zoc',\n",
       " 'pekingese',\n",
       " 'zog',\n",
       " 'zon',\n",
       " 'jahn',\n",
       " 'zandra',\n",
       " 'ointments',\n",
       " 'casely',\n",
       " 'printer',\n",
       " 'dinghies',\n",
       " 'brioni',\n",
       " 'amby',\n",
       " 'unpronounceable',\n",
       " 'phin',\n",
       " 'estan',\n",
       " 'messier',\n",
       " 'yucatan',\n",
       " 'whirlybird',\n",
       " 'bellefontaine',\n",
       " 'redirected',\n",
       " 'rihana',\n",
       " 'jewry',\n",
       " 'ooohhh',\n",
       " 'stimulator',\n",
       " 'thoug',\n",
       " 'anshu',\n",
       " 'kools',\n",
       " 'subsidiary',\n",
       " 'qoo',\n",
       " 'transact',\n",
       " 'presse',\n",
       " 'estar',\n",
       " 'ambi',\n",
       " 'hermanos',\n",
       " 'platts',\n",
       " 'balconies',\n",
       " 'rejoices',\n",
       " 'mechanization',\n",
       " 'gerbils',\n",
       " 'timeyou',\n",
       " 'pims',\n",
       " 'bathwater',\n",
       " 'tiko',\n",
       " 'tika',\n",
       " 'homeopathy',\n",
       " 'ramblers',\n",
       " 'sautéed',\n",
       " 'zuzana',\n",
       " 'scalia',\n",
       " 'youself',\n",
       " 'kisaragi',\n",
       " 'vaccines',\n",
       " 'yumi',\n",
       " 'kilometer',\n",
       " 'dunleavy',\n",
       " 'moony',\n",
       " 'punta',\n",
       " 'tsun',\n",
       " 'marietta',\n",
       " 'circumference',\n",
       " 'uou',\n",
       " 'dorkus',\n",
       " 'willets',\n",
       " 'steinmetz',\n",
       " 'drogo',\n",
       " 'kaputt',\n",
       " 'lemoyne',\n",
       " 'baden',\n",
       " 'harumi',\n",
       " 'gaiety',\n",
       " 'buoyancy',\n",
       " 'borgen',\n",
       " 'practises',\n",
       " 'deuteronomy',\n",
       " 'offworld',\n",
       " 'wesa',\n",
       " 'spyglass',\n",
       " 'mekong',\n",
       " 'gget',\n",
       " 'wanta',\n",
       " 'wilco',\n",
       " 'tomer',\n",
       " 'runnin',\n",
       " 'kimura',\n",
       " 'annabeth',\n",
       " 'iver',\n",
       " 'defeatism',\n",
       " 'snuffbox',\n",
       " 'mutineers',\n",
       " 'vuk',\n",
       " 'hermaphrodite',\n",
       " 'hollinger',\n",
       " 'gelding',\n",
       " 'yessir',\n",
       " 'merovingian',\n",
       " 'aitch',\n",
       " 'curi',\n",
       " 'tolar',\n",
       " 'marella',\n",
       " 'profiteer',\n",
       " 'agronomist',\n",
       " 'teepee',\n",
       " 'basie',\n",
       " 'sixer',\n",
       " 'sunnyvale',\n",
       " 'benzi',\n",
       " 'steamed',\n",
       " 'recycles',\n",
       " 'recycler',\n",
       " 'ngos',\n",
       " 'unknowable',\n",
       " 'wank',\n",
       " 'bannisters',\n",
       " 'plunderers',\n",
       " 'plumed',\n",
       " 'kiri',\n",
       " 'pipelines',\n",
       " 'plumes',\n",
       " 'kaveri',\n",
       " 'dicks',\n",
       " 'unerring',\n",
       " 'sportsmanship',\n",
       " 'assemblyman',\n",
       " 'penta',\n",
       " 'ayahuasca',\n",
       " 'rajput',\n",
       " 'sumi',\n",
       " 'fitzsimmons',\n",
       " 'suma',\n",
       " 'babylonian',\n",
       " 'pomoc',\n",
       " 'homeroom',\n",
       " 'futch',\n",
       " 'pleats',\n",
       " 'tuula',\n",
       " 'goosey',\n",
       " 'grub',\n",
       " 'gooses',\n",
       " 'magma',\n",
       " 'ezo',\n",
       " 'horsewhipped',\n",
       " 'rottweilers',\n",
       " 'yesss',\n",
       " 'scuffed',\n",
       " 'pentathlon',\n",
       " 'transmutation',\n",
       " 'timeout',\n",
       " 'jahan',\n",
       " 'fareed',\n",
       " 'palmdale',\n",
       " 'majoring',\n",
       " 'unmannerly',\n",
       " 'whackin',\n",
       " 'leann',\n",
       " 'newsboy',\n",
       " 'sharkboy',\n",
       " 'rumpus',\n",
       " 'locklear',\n",
       " 'rabbinical',\n",
       " 'skippin',\n",
       " 'cowhide',\n",
       " 'pistache',\n",
       " 'hominids',\n",
       " 'rumbled',\n",
       " 'missa',\n",
       " 'morehead',\n",
       " 'detachments',\n",
       " 'harpsichord',\n",
       " 'trusses',\n",
       " 'saris',\n",
       " 'lubin',\n",
       " 'bohemians',\n",
       " 'corvus',\n",
       " 'wite',\n",
       " 'matterwhat',\n",
       " 'witi',\n",
       " 'sarin',\n",
       " 'rusk',\n",
       " 'raga',\n",
       " 'ruso',\n",
       " 'swole',\n",
       " 'pula',\n",
       " 'rago',\n",
       " 'abhimanyu',\n",
       " 'tripp',\n",
       " 'creat',\n",
       " 'freon',\n",
       " 'mundu',\n",
       " 'mundy',\n",
       " 'sayest',\n",
       " 'shakal',\n",
       " 'yogo',\n",
       " 'rarin',\n",
       " 'ashoka',\n",
       " 'faxes',\n",
       " 'monir',\n",
       " 'hydraulics',\n",
       " 'yunus',\n",
       " 'linebackers',\n",
       " 'infirmity',\n",
       " 'lightfoot',\n",
       " 'sidewinders',\n",
       " 'madrigal',\n",
       " 'maliciously',\n",
       " 'piak',\n",
       " 'holman',\n",
       " 'senna',\n",
       " 'ceramic',\n",
       " 'sca',\n",
       " 'internist',\n",
       " 'miggy',\n",
       " 'scr',\n",
       " 'anderton',\n",
       " 'bismarck',\n",
       " 'ángel',\n",
       " 'carters',\n",
       " 'checkin',\n",
       " 'floated',\n",
       " 'bator',\n",
       " 'rm',\n",
       " 'grannie',\n",
       " 'boozed',\n",
       " 'faddle',\n",
       " 'adina',\n",
       " 'sanz',\n",
       " 'sana',\n",
       " 'sano',\n",
       " 'bracha',\n",
       " 'courtesans',\n",
       " 'pasa',\n",
       " 'pash',\n",
       " 'milord',\n",
       " 'correcte',\n",
       " 'quicken',\n",
       " 'neddy',\n",
       " 'menswear',\n",
       " 'accreditation',\n",
       " 'pretrial',\n",
       " 'psychosomatic',\n",
       " 'corked',\n",
       " 'corker',\n",
       " 'jurist',\n",
       " 'leid',\n",
       " 'shinwa',\n",
       " 'sarasota',\n",
       " 'cabled',\n",
       " 'ganda',\n",
       " 'interoffice',\n",
       " 'hast',\n",
       " 'melancholia',\n",
       " 'ohhhhh',\n",
       " 'wogan',\n",
       " 'cessation',\n",
       " 'countin',\n",
       " 'ischia',\n",
       " 'norville',\n",
       " 'retroactive',\n",
       " 'adidas',\n",
       " 'reclaimed',\n",
       " 'aeschylus',\n",
       " 'pyo',\n",
       " 'pliny',\n",
       " 'mory',\n",
       " 'odometer',\n",
       " 'mori',\n",
       " 'moro',\n",
       " 'fragrance',\n",
       " 'beatriz',\n",
       " 'morg',\n",
       " 'beatrix',\n",
       " 'marsch',\n",
       " 'doos',\n",
       " 'testes',\n",
       " 'vesalius',\n",
       " 'doof',\n",
       " 'makeovers',\n",
       " 'séance',\n",
       " 'mí',\n",
       " 'kaminski',\n",
       " 'godown',\n",
       " 'knocker',\n",
       " 'greaseball',\n",
       " 'siku',\n",
       " 'thermite',\n",
       " 'ponte',\n",
       " 'ponti',\n",
       " 'allegro',\n",
       " 'winemaker',\n",
       " 'boga',\n",
       " 'resync',\n",
       " 'bluestone',\n",
       " 'sequins',\n",
       " 'marylebone',\n",
       " 'juve',\n",
       " 'hens',\n",
       " 'gannets',\n",
       " 'footman',\n",
       " 'mender',\n",
       " 'oleander',\n",
       " 'cabrón',\n",
       " 'amparo',\n",
       " 'gdansk',\n",
       " 'ricard',\n",
       " 'jiang',\n",
       " 'tulane',\n",
       " 'pinewood',\n",
       " 'barbarous',\n",
       " 'trellis',\n",
       " 'weirdoes',\n",
       " 'eveyone',\n",
       " 'roote',\n",
       " 'gearhart',\n",
       " 'roshi',\n",
       " 'papen',\n",
       " 'papes',\n",
       " 'rooty',\n",
       " 'refried',\n",
       " 'trivialities',\n",
       " 'deliberating',\n",
       " 'shitless',\n",
       " 'balaban',\n",
       " 'arakawa',\n",
       " 'ellos',\n",
       " 'purdue',\n",
       " 'mandarins',\n",
       " 'betweens',\n",
       " 'repayment',\n",
       " 'corine',\n",
       " 'necrosis',\n",
       " 'spinnaker',\n",
       " 'gilberts',\n",
       " 'numeric',\n",
       " 'brunetti',\n",
       " 'gilberto',\n",
       " 'treasonable',\n",
       " 'inquired',\n",
       " 'atwater',\n",
       " 'cokehead',\n",
       " 'squarepants',\n",
       " 'alrighty',\n",
       " 'windmills',\n",
       " 'colonnade',\n",
       " 'airway',\n",
       " 'ecologically',\n",
       " 'plowing',\n",
       " 'pyne',\n",
       " 'zala',\n",
       " 'ceasefire',\n",
       " 'weatherby',\n",
       " 'faom',\n",
       " 'hyderabad',\n",
       " 'scuffling',\n",
       " 'noma',\n",
       " 'nome',\n",
       " 'tove',\n",
       " 'toluca',\n",
       " 'aaaah',\n",
       " 'peaceably',\n",
       " 'smithers',\n",
       " 'nunchucks',\n",
       " 'talvez',\n",
       " 'elling',\n",
       " 'daytona',\n",
       " 'eiling',\n",
       " 'syringes',\n",
       " 'condolences',\n",
       " 'tummy',\n",
       " 'shlomo',\n",
       " 'navigators',\n",
       " 'ordway',\n",
       " 'mitzvah',\n",
       " 'waft',\n",
       " 'nikolay',\n",
       " 'luzon',\n",
       " 'nikolas',\n",
       " 'odors',\n",
       " 'selo',\n",
       " 'fermented',\n",
       " 'destabilizing',\n",
       " 'extorted',\n",
       " 'mian',\n",
       " 'antagonized',\n",
       " 'unnamable',\n",
       " 'astrea',\n",
       " 'pringles',\n",
       " 'mends',\n",
       " 'departmental',\n",
       " 'mende',\n",
       " 'stoning',\n",
       " 'pré',\n",
       " 'ugo',\n",
       " 'uga',\n",
       " 'patris',\n",
       " 'northwestern',\n",
       " 'clipboard',\n",
       " 'calibers',\n",
       " 'mousetraps',\n",
       " 'ninjutsu',\n",
       " 'sunni',\n",
       " 'planchet',\n",
       " 'boze',\n",
       " 'anniversaries',\n",
       " 'redoubt',\n",
       " 'schmoozing',\n",
       " 'twittering',\n",
       " 'hibbert',\n",
       " 'deactivate',\n",
       " 'richelieu',\n",
       " 'oishi',\n",
       " 'borgias',\n",
       " 'naïve',\n",
       " 'manas',\n",
       " 'condones',\n",
       " 'ocular',\n",
       " 'vickey',\n",
       " 'outlying',\n",
       " 'jawing',\n",
       " 'scatterbrain',\n",
       " 'franny',\n",
       " 'lasy',\n",
       " 'khadija',\n",
       " 'nanotech',\n",
       " 'ludes',\n",
       " 'sergej',\n",
       " 'sadat',\n",
       " 'olli',\n",
       " 'waitressing',\n",
       " 'beli',\n",
       " 'belo',\n",
       " 'olla',\n",
       " 'selden',\n",
       " 'olle',\n",
       " 'screwdrivers',\n",
       " 'olly',\n",
       " 'hetman',\n",
       " 'safet',\n",
       " 'galvatron',\n",
       " 'scherz',\n",
       " 'patrolled',\n",
       " 'thinkthat',\n",
       " 'nrc',\n",
       " 'cretan',\n",
       " 'crenshaw',\n",
       " 'skynyrd',\n",
       " 'nanao',\n",
       " 'nucleotides',\n",
       " 'osan',\n",
       " 'budged',\n",
       " 'cager',\n",
       " 'lade',\n",
       " 'lynette',\n",
       " 'vom',\n",
       " 'mycroft',\n",
       " 'mouche',\n",
       " 'vou',\n",
       " 'vor',\n",
       " 'vos',\n",
       " 'wouid',\n",
       " 'vox',\n",
       " 'voy',\n",
       " 'rifat',\n",
       " 'ddn',\n",
       " 'matchmakers',\n",
       " 'geeta',\n",
       " 'tauntaun',\n",
       " 'ashland',\n",
       " 'mongrels',\n",
       " 'giorno',\n",
       " 'kno',\n",
       " 'merel',\n",
       " 'risqué',\n",
       " 'raters',\n",
       " 'knd',\n",
       " 'thain',\n",
       " 'fuckwit',\n",
       " 'looted',\n",
       " 'nazarene',\n",
       " 'clasped',\n",
       " 'salva',\n",
       " 'salve',\n",
       " 'pesetas',\n",
       " 'joder',\n",
       " 'timur',\n",
       " 'crinkling',\n",
       " 'abstention',\n",
       " 'eero',\n",
       " 'andorra',\n",
       " 'horseradish',\n",
       " 'aimin',\n",
       " 'anklets',\n",
       " 'khun',\n",
       " 'cordon',\n",
       " 'maata',\n",
       " 'hayat',\n",
       " 'fritzy',\n",
       " 'counterintuitive',\n",
       " 'spink',\n",
       " 'wagonload',\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################################################\n",
    "def embeddings_not_in_tokenizer(embeddings_dict, tokenizer):\n",
    "    elems = []\n",
    "    for w in embeddings_dict.keys():\n",
    "        if w not in tokenizer.word_index.keys():\n",
    "            elems += [w]\n",
    "    return elems\n",
    "    \n",
    "embeddings_not_in_tokenizer(embeddings_dict, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40570"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################################################\n",
    "tokenizer.word_index[list(tokenizer.word_index.keys())[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "for w in embeddings_not_in_tokenizer(embeddings_dict, tokenizer):\n",
    "    tokenizer.word_index[w] = len(tokenizer.word_index) + 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65713"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################################################\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_dictionary = tokenizer.word_index\n",
    "\n",
    "inverse_tokens_dictionary = {v : k for (k, v) in tokenizer.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1),\n",
       " ('and', 2),\n",
       " ('a', 3),\n",
       " ('of', 4),\n",
       " ('to', 5),\n",
       " ('is', 6),\n",
       " ('it', 7),\n",
       " ('in', 8),\n",
       " ('i', 9),\n",
       " ('this', 10),\n",
       " ('that', 11),\n",
       " ('s', 12),\n",
       " ('was', 13),\n",
       " ('as', 14),\n",
       " ('with', 15),\n",
       " ('movie', 16),\n",
       " ('for', 17),\n",
       " ('but', 18),\n",
       " ('film', 19),\n",
       " ('on', 20),\n",
       " ('you', 21),\n",
       " ('t', 22),\n",
       " ('not', 23),\n",
       " ('are', 24),\n",
       " ('his', 25),\n",
       " ('he', 26),\n",
       " ('have', 27)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokens_dictionary.items())[:27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'the'), (2, 'and'), (3, 'a')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(inverse_tokens_dictionary.items())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('pickle\\\\tokens_dicts.pickle', 'wb') as f:\n",
    "    pickle.dump([tokens_dictionary,inverse_tokens_dictionary], f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('pickle\\\\tokens_dicts.pickle', 'rb') as f:\n",
    "    tokens_dictionary,inverse_tokens_dictionary = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros(shape = (MAXLEN+1, 300), dtype= 'float32')\n",
    "#embedding_matrix = np.zeros(shape = (len(tokenizer.word_index) + 1, 300), dtype= 'float32')\n",
    "\n",
    "for w, i in tokenizer.word_index.items():\n",
    "    if w in embeddings_dict and i < MAXLEN+1:\n",
    "        embedding_matrix[i,:] = embeddings_dict[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('numpy_files', exist_ok=True)\n",
    "np.save('numpy_files\\embedding_matrix.npy', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.load('numpy_files\\embedding_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import normalize #minmax_scale\n",
    "\n",
    "#embedding_matrix = minmax_scale(embedding_matrix, feature_range=(0, 1), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(embeddings_dict['king'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [ 0.022847, -0.01317 , -0.025261, ..., -0.039248,  0.001481,\n",
       "         0.055489],\n",
       "       [ 0.012515, -0.019482, -0.005424, ..., -0.079507,  0.019481,\n",
       "        -0.01417 ],\n",
       "       ...,\n",
       "       [-0.039857,  0.030305,  0.01695 , ...,  0.038111, -0.062805,\n",
       "        -0.033791],\n",
       "       [-0.012134,  0.049292, -0.029643, ...,  0.094657, -0.053364,\n",
       "         0.007016],\n",
       "       [ 0.08014 , -0.018318,  0.038116, ...,  0.099899,  0.062242,\n",
       "         0.08629 ]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nembedding_matrix = np.zeros((len(embeddings_dict), 300))\\n\\nfor w, emb in embeddings_dict.items():\\n    embedding_matrix[words_glove_dictionary[w],:] = emb\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "embedding_matrix = np.zeros((len(embeddings_dict), 300))\n",
    "\n",
    "for w, emb in embeddings_dict.items():\n",
    "    embedding_matrix[words_glove_dictionary[w],:] = emb\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65714, 300)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from scipy import sparse\n",
    "\n",
    "distance_matrix = cosine_distances(embedding_matrix, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom scipy import spatial\\n\\ndef compute_euclidean_distance(X):\\n    V = spatial.distance.pdist(X.T, 'sqeuclidean')\\n    return spatial.distance.squareform(V)\\n\\ndistance_matrix = compute_euclidean_distance(embedding_matrix)\\n#distance_matrix = spatial.distance_matrix(embedding_matrix, embedding_matrix)\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from scipy import spatial\n",
    "\n",
    "def compute_euclidean_distance(X):\n",
    "    V = spatial.distance.pdist(X.T, 'sqeuclidean')\n",
    "    return spatial.distance.squareform(V)\n",
    "\n",
    "distance_matrix = compute_euclidean_distance(embedding_matrix)\n",
    "#distance_matrix = spatial.distance_matrix(embedding_matrix, embedding_matrix)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        , 1.        , ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [1.        , 0.        , 0.6060601 , ..., 1.1508511 , 1.1063254 ,\n",
       "        0.95153755],\n",
       "       [1.        , 0.6060601 , 0.        , ..., 1.036566  , 1.0173967 ,\n",
       "        0.9427275 ],\n",
       "       ...,\n",
       "       [1.        , 1.1508511 , 1.036566  , ..., 0.        , 0.83871764,\n",
       "        0.94371986],\n",
       "       [1.        , 1.1063254 , 1.0173967 , ..., 0.83871764, 0.        ,\n",
       "        0.94929343],\n",
       "       [1.        , 0.95153755, 0.9427275 , ..., 0.94371986, 0.94929343,\n",
       "        0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('numpy_files', exist_ok=True)\n",
    "np.save('numpy_files\\distance_matrix.npy', distance_matrix)\n",
    "\n",
    "#distance_matrix = np.load('numpy_files\\distance_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = np.load('numpy_files\\distance_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "689"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_dictionary['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max(distance_matrix[:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"with open('pickle\\\\distance_matrix.pickle', 'wb') as f:\\n    pickle.dump(distance_matrix, f)\\nf.close()\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''with open('pickle\\\\distance_matrix.pickle', 'wb') as f:\n",
    "    pickle.dump(distance_matrix, f)\n",
    "f.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distance_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.preprocessing import normalize\\n\\nnormalized_distance_matrix = normalize(distance_matrix, axis = 1, norm = 'l1')\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.preprocessing import normalize\n",
    "\n",
    "normalized_distance_matrix = normalize(distance_matrix, axis = 1, norm = 'l1')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, delta = 0.5, num_words = 20):\n",
    "    \n",
    "    try:\n",
    "        index = tokens_dictionary[word]\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    if (index > distance_matrix.shape[0]):\n",
    "        return []\n",
    "    \n",
    "    dist_order = np.argsort(distance_matrix[index,:])[1:num_words+1]\n",
    "    dist_list = distance_matrix[index][dist_order]\n",
    "    \n",
    "    mask = np.ones_like(dist_list)\n",
    "    mask = np.where(dist_list < delta)\n",
    "    return dist_order[mask]#, dist_list[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def most_similar(word, delta = 0.5, num_words = 20):\\n    \\n    try:\\n        index = tokenizer.word_index[word]\\n    except:\\n        return [], []\\n    \\n    if (index > distance_matrix.shape[0]):\\n        return [], []\\n    \\n    dist_order = np.argsort(distance_matrix[index,:])[1:num_words+1]\\n    dist_list = distance_matrix[index][dist_order]\\n    \\n    print(dist_order)\\n    print(dist_list)\\n    \\n    #return dist_order, dist_list\\n\\n    #if dist_list[-1] == 0:\\n    #    return [], []\\n    \\n    mask = np.ones_like(dist_list)\\n    #print(mask)\\n    mask = np.where(dist_list < delta)\\n    return dist_order[mask], dist_list[mask]'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def most_similar(word, delta = 0.5, num_words = 20):\n",
    "    \n",
    "    try:\n",
    "        index = tokenizer.word_index[word]\n",
    "    except:\n",
    "        return [], []\n",
    "    \n",
    "    if (index > distance_matrix.shape[0]):\n",
    "        return [], []\n",
    "    \n",
    "    dist_order = np.argsort(distance_matrix[index,:])[1:num_words+1]\n",
    "    dist_list = distance_matrix[index][dist_order]\n",
    "    \n",
    "    print(dist_order)\n",
    "    print(dist_list)\n",
    "    \n",
    "    #return dist_order, dist_list\n",
    "\n",
    "    #if dist_list[-1] == 0:\n",
    "    #    return [], []\n",
    "    \n",
    "    mask = np.ones_like(dist_list)\n",
    "    #print(mask)\n",
    "    mask = np.where(dist_list < delta)\n",
    "    return dist_order[mask], dist_list[mask]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = most_similar('great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1898,  3156,  1218,  6437,  1241, 29888,  1921,   379,  4199,\n",
       "         806,  2906,  3300,  7644, 19952, 10235,  9447,   617,  1104,\n",
       "       28723,   303], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grand',\n",
       " 'tremendous',\n",
       " 'awesome',\n",
       " 'phenomenal',\n",
       " 'terrific',\n",
       " 'prodigious',\n",
       " 'magnificent',\n",
       " 'wonderful',\n",
       " 'splendid',\n",
       " 'fantastic',\n",
       " 'marvelous',\n",
       " 'fabulous',\n",
       " 'marvellous',\n",
       " 'whopping',\n",
       " 'wondrous',\n",
       " 'formidable',\n",
       " 'huge',\n",
       " 'super',\n",
       " 'resplendent',\n",
       " 'excellent']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inverse_tokens_dictionary[index] for index in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ALTERNATIVE ###\n",
    "from scipy import spatial\n",
    "\n",
    "def find_nearest_neighbours(word, n = 20, delta = 0.5):\n",
    "    embedding = embeddings_dict[word]\n",
    "    return sorted(embeddings_dict.keys(), key=lambda w: spatial.distance.cosine(embeddings_dict[w], embedding))[1:20+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'find_nearest_neighbours' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-9f93bed5b9a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfind_nearest_neighbours\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'find_nearest_neighbours' is not defined"
     ]
    }
   ],
   "source": [
    "find_nearest_neighbours('fear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GOOGLE 1 BILLION WORDS LANGUAGE MODEL\n",
    "\n",
    "To filter out words that are not in the context of a sentence\n",
    "\n",
    "\n",
    "https://github.com/tensorflow/models/tree/archive/research/lm_1b\n",
    "\n",
    "DOWNLOADS:<br>\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/graph-2016-09-10.pbtxt\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-base\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-char-embedding\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-lstm\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax0\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax1\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax2\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax3\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax4\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax5\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax6\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax7\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax8\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/vocab-2016-09-10.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il codice nella seguente cella è modificato da una base il cui copyright è espresso di seguito:\n",
    "\n",
    "```\n",
    "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import licensed_scripts.lm_1b_eval as google_language_model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM vocab loading done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recovering graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Recovering Graph google_language_model\\graph-2016-09-10.pbtxt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recovering checkpoint google_language_model\\ckpt-*\n"
     ]
    }
   ],
   "source": [
    "google_language_model = google_language_model_utils.LM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest to having are ['gorgeous', 'wonderful', 'splendid', 'handsome', 'resplendent', 'wondrous', 'marvelous', 'marvellous', 'fantastic', 'sumptuous', 'magnificent', 'terrific', 'lovely', 'ravishing', 'sublime', 'magnifique', 'exquisite', 'fabulous', 'delightful', 'superb']\n"
     ]
    }
   ],
   "source": [
    "nearest_indexes = most_similar('beautiful')\n",
    "nearest_words = [inverse_tokens_dictionary[index] for index in nearest_indexes]\n",
    "print('Closest to having are %s' %(nearest_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gorgeous',\n",
       " 'wonderful',\n",
       " 'splendid',\n",
       " 'handsome',\n",
       " 'resplendent',\n",
       " 'wondrous',\n",
       " 'marvelous',\n",
       " 'marvellous',\n",
       " 'fantastic',\n",
       " 'sumptuous',\n",
       " 'magnificent',\n",
       " 'terrific',\n",
       " 'lovely',\n",
       " 'ravishing',\n",
       " 'sublime',\n",
       " 'magnifique',\n",
       " 'exquisite',\n",
       " 'fabulous',\n",
       " 'delightful',\n",
       " 'superb']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Unhinged follows the typical plot of the early 80\\'s slasher trend. Pretty Young Girls In Peril. I have to give it up for the filmmaker who used a helicopter for some of the early road-trip shots, you actually think for a second there\\'s going to be quality in the production. Watching \"Unhinged\" was like seeing an amateur acting class go through it\\'s warm-up. Some of the most awkward, badly lit, overlong scenes are played out with the gusto of a Valium overdose. I wondered why they didn\\'t just put the cue-cards on camera so the actresses wouldn\\'t have to constantly shift their gaze. The two main girls were obviously chosen for their T&A factor rather than talent. Laurel Munson as the main chick Terry is as exciting as watching paint dry. Two nude scenes make for an adolescent thrill. Janet Penner and Virginia Settle as the crazy/creepy daughter and mother the chicks find themselves stranded with compete for Worst Acting Ever. Long pauses, weird expressions, emphasis on the wrong word, it\\'s all there and is a delight for those of us out there who love bad films. The scenes shift suddenly with long black-outs you could drive a Mack truck through. Cartoon lightning crashes across shots without even bothering to show the sky. Eighties eyeshadow assaults the viewer. But ya know, it grew on me. I felt sorry for it. I wanted to hug it, kiss it\\'s boo-boos and make it better. The ending doesn\\'t make up for the damage it\\'s caused but I grinned anyway. I have my own theories regarding the whole \"banned\" hype and hope that anyone who chooses to view this film does so with substantial substance abuse and a sense of humor. Otherwise pass.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unhinged',\n",
       " 'follows',\n",
       " 'the',\n",
       " 'typical',\n",
       " 'plot',\n",
       " 'of',\n",
       " 'the',\n",
       " 'early',\n",
       " \"80's\",\n",
       " 'slasher',\n",
       " 'trend.',\n",
       " 'Pretty',\n",
       " 'Young',\n",
       " 'Girls',\n",
       " 'In',\n",
       " 'Peril.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'to',\n",
       " 'give',\n",
       " 'it',\n",
       " 'up',\n",
       " 'for',\n",
       " 'the',\n",
       " 'filmmaker',\n",
       " 'who',\n",
       " 'used',\n",
       " 'a',\n",
       " 'helicopter',\n",
       " 'for',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'early',\n",
       " 'road-trip',\n",
       " 'shots,',\n",
       " 'you',\n",
       " 'actually',\n",
       " 'think',\n",
       " 'for',\n",
       " 'a',\n",
       " 'second',\n",
       " \"there's\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'be',\n",
       " 'quality',\n",
       " 'in',\n",
       " 'the',\n",
       " 'production.',\n",
       " 'Watching',\n",
       " '\"Unhinged\"',\n",
       " 'was',\n",
       " 'like',\n",
       " 'seeing',\n",
       " 'an',\n",
       " 'amateur',\n",
       " 'acting',\n",
       " 'class',\n",
       " 'go',\n",
       " 'through',\n",
       " \"it's\",\n",
       " 'warm-up.',\n",
       " 'Some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'most',\n",
       " 'awkward,',\n",
       " 'badly',\n",
       " 'lit,',\n",
       " 'overlong',\n",
       " 'scenes',\n",
       " 'are',\n",
       " 'played',\n",
       " 'out',\n",
       " 'with',\n",
       " 'the',\n",
       " 'gusto',\n",
       " 'of',\n",
       " 'a',\n",
       " 'Valium',\n",
       " 'overdose.',\n",
       " 'I',\n",
       " 'wondered',\n",
       " 'why',\n",
       " 'they',\n",
       " \"didn't\",\n",
       " 'just',\n",
       " 'put',\n",
       " 'the',\n",
       " 'cue-cards',\n",
       " 'on',\n",
       " 'camera',\n",
       " 'so',\n",
       " 'the',\n",
       " 'actresses',\n",
       " \"wouldn't\",\n",
       " 'have',\n",
       " 'to',\n",
       " 'constantly',\n",
       " 'shift',\n",
       " 'their',\n",
       " 'gaze.',\n",
       " 'The',\n",
       " 'two',\n",
       " 'main',\n",
       " 'girls',\n",
       " 'were',\n",
       " 'obviously',\n",
       " 'chosen',\n",
       " 'for',\n",
       " 'their',\n",
       " 'T&A',\n",
       " 'factor',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'talent.',\n",
       " 'Laurel',\n",
       " 'Munson',\n",
       " 'as',\n",
       " 'the',\n",
       " 'main',\n",
       " 'chick',\n",
       " 'Terry',\n",
       " 'is',\n",
       " 'as',\n",
       " 'exciting',\n",
       " 'as',\n",
       " 'watching',\n",
       " 'paint',\n",
       " 'dry.',\n",
       " 'Two',\n",
       " 'nude',\n",
       " 'scenes',\n",
       " 'make',\n",
       " 'for',\n",
       " 'an',\n",
       " 'adolescent',\n",
       " 'thrill.',\n",
       " 'Janet',\n",
       " 'Penner',\n",
       " 'and',\n",
       " 'Virginia',\n",
       " 'Settle',\n",
       " 'as',\n",
       " 'the',\n",
       " 'crazy/creepy',\n",
       " 'daughter',\n",
       " 'and',\n",
       " 'mother',\n",
       " 'the',\n",
       " 'chicks',\n",
       " 'find',\n",
       " 'themselves',\n",
       " 'stranded',\n",
       " 'with',\n",
       " 'compete',\n",
       " 'for',\n",
       " 'Worst',\n",
       " 'Acting',\n",
       " 'Ever.',\n",
       " 'Long',\n",
       " 'pauses,',\n",
       " 'weird',\n",
       " 'expressions,',\n",
       " 'emphasis',\n",
       " 'on',\n",
       " 'the',\n",
       " 'wrong',\n",
       " 'word,',\n",
       " \"it's\",\n",
       " 'all',\n",
       " 'there',\n",
       " 'and',\n",
       " 'is',\n",
       " 'a',\n",
       " 'delight',\n",
       " 'for',\n",
       " 'those',\n",
       " 'of',\n",
       " 'us',\n",
       " 'out',\n",
       " 'there',\n",
       " 'who',\n",
       " 'love',\n",
       " 'bad',\n",
       " 'films.',\n",
       " 'The',\n",
       " 'scenes',\n",
       " 'shift',\n",
       " 'suddenly',\n",
       " 'with',\n",
       " 'long',\n",
       " 'black-outs',\n",
       " 'you',\n",
       " 'could',\n",
       " 'drive',\n",
       " 'a',\n",
       " 'Mack',\n",
       " 'truck',\n",
       " 'through.',\n",
       " 'Cartoon',\n",
       " 'lightning',\n",
       " 'crashes',\n",
       " 'across',\n",
       " 'shots',\n",
       " 'without',\n",
       " 'even',\n",
       " 'bothering',\n",
       " 'to',\n",
       " 'show',\n",
       " 'the',\n",
       " 'sky.',\n",
       " 'Eighties',\n",
       " 'eyeshadow',\n",
       " 'assaults',\n",
       " 'the',\n",
       " 'viewer.',\n",
       " 'But',\n",
       " 'ya',\n",
       " 'know,',\n",
       " 'it',\n",
       " 'grew',\n",
       " 'on',\n",
       " 'me.',\n",
       " 'I',\n",
       " 'felt',\n",
       " 'sorry',\n",
       " 'for',\n",
       " 'it.',\n",
       " 'I',\n",
       " 'wanted',\n",
       " 'to',\n",
       " 'hug',\n",
       " 'it,',\n",
       " 'kiss',\n",
       " \"it's\",\n",
       " 'boo-boos',\n",
       " 'and',\n",
       " 'make',\n",
       " 'it',\n",
       " 'better.',\n",
       " 'The',\n",
       " 'ending',\n",
       " \"doesn't\",\n",
       " 'make',\n",
       " 'up',\n",
       " 'for',\n",
       " 'the',\n",
       " 'damage',\n",
       " \"it's\",\n",
       " 'caused',\n",
       " 'but',\n",
       " 'I',\n",
       " 'grinned',\n",
       " 'anyway.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'my',\n",
       " 'own',\n",
       " 'theories',\n",
       " 'regarding',\n",
       " 'the',\n",
       " 'whole',\n",
       " '\"banned\"',\n",
       " 'hype',\n",
       " 'and',\n",
       " 'hope',\n",
       " 'that',\n",
       " 'anyone',\n",
       " 'who',\n",
       " 'chooses',\n",
       " 'to',\n",
       " 'view',\n",
       " 'this',\n",
       " 'film',\n",
       " 'does',\n",
       " 'so',\n",
       " 'with',\n",
       " 'substantial',\n",
       " 'substance',\n",
       " 'abuse',\n",
       " 'and',\n",
       " 'a',\n",
       " 'sense',\n",
       " 'of',\n",
       " 'humor.',\n",
       " 'Otherwise',\n",
       " 'pass.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_text = x_test[0].split()\n",
    "splitted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"', 'Unhinged', '\"']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "pattern = r\"[\\w]+|[^\\s\\w]\"\n",
    "\n",
    "re.findall(pattern, splitted_text[51])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\utente\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from scripts.preprocessing import Preprocesser\n",
    "import nltk.data\n",
    "import nltk\n",
    "\n",
    "\n",
    "def substitute_with_best_word(test, sentence_position = 0, text_position = 3):\n",
    "    \n",
    "    sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = sentence_tokenizer.tokenize(x_test[0])\n",
    "    \n",
    "    splitted_text = sentences[sentence_position].split()\n",
    "    \n",
    "    pattern = r\"[\\w]+|[^\\s\\w]\"\n",
    "\n",
    "    splitted_word = re.findall(pattern, splitted_text[text_position])\n",
    "    \n",
    "    for i,w in enumerate(splitted_word):\n",
    "        preprocessed_w = Preprocesser.test_data_preprocessing_for_tokenization(w, embeddings_dict)\n",
    "        if len(preprocessed_w.split()):\n",
    "            print('substituting: %s' %(preprocessed_w))\n",
    "            nearest_indexes = most_similar(preprocessed_w)\n",
    "            nearest_words = [inverse_tokens_dictionary[index] for index in nearest_indexes]\n",
    "            if len(nearest_words):\n",
    "                prefix = ' '.join(splitted_text[:text_position])\n",
    "                print(prefix)\n",
    "                suffix = ' '.join(splitted_text[text_position + 1:])\n",
    "                print(suffix)\n",
    "                lm_preds = google_language_model.get_words_probs(prefix, nearest_words, suffix)\n",
    "                splitted_word[i] = nearest_words[np.argmax(lm_preds)]\n",
    "    splitted_text[text_position] = ''.join(splitted_word)\n",
    "    sentences[sentence_position] = ' '.join(splitted_text)\n",
    "    return ' '.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport re\\nfrom scripts.preprocessing import Preprocesser\\n\\ndef substitute_with_best_word(test, text_position = 10, offset  = 1):\\n    splitted_text = test.split()\\n    \\n    pattern = r\"[\\\\w]+|[^\\\\s\\\\w]\"\\n\\n    splitted_word = re.findall(pattern, splitted_text[text_position])\\n    \\n    for i,w in enumerate(splitted_word):\\n        preprocessed_w = Preprocesser.test_data_preprocessing_for_tokenization(w)\\n        if len(preprocessed_w.split()):\\n            print(\\'substituting: %s\\' %(preprocessed_w))\\n            nearest_indexes = most_similar(preprocessed_w)\\n            nearest_words = [inverse_tokens_dictionary[index] for index in nearest_indexes]\\n            if len(nearest_words):\\n                minimum = 0 if text_position-offset < 0 else text_position-offset\\n                maximum = len(splitted_text) - 1 if text_position+offset+1 > len(splitted_text) - 1 else text_position+offset+1  \\n                prefix = \\' \\'.join(splitted_text[minimum : text_position])\\n                print(prefix)\\n                suffix = \\' \\'.join(splitted_text[text_position + 1 : maximum])\\n                print(suffix)\\n                lm_preds = google_language_model.get_words_probs(prefix, nearest_words, suffix)\\n                splitted_word[i] = nearest_words[np.argmax(lm_preds)]\\n    splitted_text[text_position] = \\'\\'.join(splitted_word)\\n    return \\' \\'.join(splitted_text)\\n    \\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import re\n",
    "from scripts.preprocessing import Preprocesser\n",
    "\n",
    "def substitute_with_best_word(test, text_position = 10, offset  = 1):\n",
    "    splitted_text = test.split()\n",
    "    \n",
    "    pattern = r\"[\\w]+|[^\\s\\w]\"\n",
    "\n",
    "    splitted_word = re.findall(pattern, splitted_text[text_position])\n",
    "    \n",
    "    for i,w in enumerate(splitted_word):\n",
    "        preprocessed_w = Preprocesser.test_data_preprocessing_for_tokenization(w)\n",
    "        if len(preprocessed_w.split()):\n",
    "            print('substituting: %s' %(preprocessed_w))\n",
    "            nearest_indexes = most_similar(preprocessed_w)\n",
    "            nearest_words = [inverse_tokens_dictionary[index] for index in nearest_indexes]\n",
    "            if len(nearest_words):\n",
    "                minimum = 0 if text_position-offset < 0 else text_position-offset\n",
    "                maximum = len(splitted_text) - 1 if text_position+offset+1 > len(splitted_text) - 1 else text_position+offset+1  \n",
    "                prefix = ' '.join(splitted_text[minimum : text_position])\n",
    "                print(prefix)\n",
    "                suffix = ' '.join(splitted_text[text_position + 1 : maximum])\n",
    "                print(suffix)\n",
    "                lm_preds = google_language_model.get_words_probs(prefix, nearest_words, suffix)\n",
    "                splitted_word[i] = nearest_words[np.argmax(lm_preds)]\n",
    "    splitted_text[text_position] = ''.join(splitted_word)\n",
    "    return ' '.join(splitted_text)\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Unhinged follows the typical plot of the early 80\\'s slasher trend. Pretty Young Girls In Peril. I have to give it up for the filmmaker who used a helicopter for some of the early road-trip shots, you actually think for a second there\\'s going to be quality in the production. Watching \"Unhinged\" was like seeing an amateur acting class go through it\\'s warm-up. Some of the most awkward, badly lit, overlong scenes are played out with the gusto of a Valium overdose. I wondered why they didn\\'t just put the cue-cards on camera so the actresses wouldn\\'t have to constantly shift their gaze. The two main girls were obviously chosen for their T&A factor rather than talent. Laurel Munson as the main chick Terry is as exciting as watching paint dry. Two nude scenes make for an adolescent thrill. Janet Penner and Virginia Settle as the crazy/creepy daughter and mother the chicks find themselves stranded with compete for Worst Acting Ever. Long pauses, weird expressions, emphasis on the wrong word, it\\'s all there and is a delight for those of us out there who love bad films. The scenes shift suddenly with long black-outs you could drive a Mack truck through. Cartoon lightning crashes across shots without even bothering to show the sky. Eighties eyeshadow assaults the viewer. But ya know, it grew on me. I felt sorry for it. I wanted to hug it, kiss it\\'s boo-boos and make it better. The ending doesn\\'t make up for the damage it\\'s caused but I grinned anyway. I have my own theories regarding the whole \"banned\" hype and hope that anyone who chooses to view this film does so with substantial substance abuse and a sense of humor. Otherwise pass.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "substituting: typical\n",
      "Unhinged follows the\n",
      "plot of the early 80's slasher trend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Unhinged follows the usual plot of the early 80\\'s slasher trend. Pretty Young Girls In Peril. I have to give it up for the filmmaker who used a helicopter for some of the early road-trip shots, you actually think for a second there\\'s going to be quality in the production. Watching \"Unhinged\" was like seeing an amateur acting class go through it\\'s warm-up. Some of the most awkward, badly lit, overlong scenes are played out with the gusto of a Valium overdose. I wondered why they didn\\'t just put the cue-cards on camera so the actresses wouldn\\'t have to constantly shift their gaze. The two main girls were obviously chosen for their T&A factor rather than talent. Laurel Munson as the main chick Terry is as exciting as watching paint dry. Two nude scenes make for an adolescent thrill. Janet Penner and Virginia Settle as the crazy/creepy daughter and mother the chicks find themselves stranded with compete for Worst Acting Ever. Long pauses, weird expressions, emphasis on the wrong word, it\\'s all there and is a delight for those of us out there who love bad films. The scenes shift suddenly with long black-outs you could drive a Mack truck through. Cartoon lightning crashes across shots without even bothering to show the sky. Eighties eyeshadow assaults the viewer. But ya know, it grew on me. I felt sorry for it. I wanted to hug it, kiss it\\'s boo-boos and make it better. The ending doesn\\'t make up for the damage it\\'s caused but I grinned anyway. I have my own theories regarding the whole \"banned\" hype and hope that anyone who chooses to view this film does so with substantial substance abuse and a sense of humor. Otherwise pass.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "substitute_with_best_word(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest to having are ['trends', 'tendencies', 'tendency', 'tended', 'tending', 'tend', 'propensity', 'inclination', 'proclivities', 'leanings', 'penchant', 'tends', 'orientation', 'directions', 'sensibilities', 'evolution', 'drift', 'thrusts', 'gravitate', 'minded']\n"
     ]
    }
   ],
   "source": [
    "nearest_indexes = most_similar('trend')\n",
    "nearest_words = [inverse_tokens_dictionary[index] for index in nearest_indexes]\n",
    "print('Closest to having are %s' %(nearest_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "attempt to get argmax of an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-c743620a43a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msuffix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'man'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlm_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgoogle_language_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_words_probs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnearest_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'most probable is '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnearest_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlm_preds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m     \"\"\"\n\u001b[1;32m-> 1188\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: attempt to get argmax of an empty sequence"
     ]
    }
   ],
   "source": [
    "prefix = \"I am a\"\n",
    "suffix = 'man'\n",
    "lm_preds = google_language_model.get_words_probs(prefix, nearest_words, suffix)\n",
    "print('most probable is ', nearest_words[np.argmax(lm_preds)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in np.argsort(lm_preds)[::-1]:\n",
    "    print(nearest_words[index]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
