{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encoding Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(samples)\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'cat': 2,\n",
       " 'sat': 3,\n",
       " 'on': 4,\n",
       " 'mat': 5,\n",
       " 'dog': 6,\n",
       " 'ate': 7,\n",
       " 'my': 8,\n",
       " 'homework': 9}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using word embeddings\n",
    "Another popular and powerful way to associate a vector with a word is the use of dense\n",
    "word vectors, also called word embeddings. Whereas the vectors obtained through one-hot\n",
    "encoding are binary, sparse (mostly made of zeros), and very high-dimensional (same\n",
    "dimensionality as the number of words in the vocabulary), word embeddings are low-\n",
    "dimensional floating-point vectors (that is, dense vectors, as opposed to sparse vec-\n",
    "tors); see figure 6.2. Unlike the word vectors obtained via one-hot encoding, word\n",
    "embeddings are learned from data. It’s common to see word embeddings that are\n",
    "256-dimensional, 512-dimensional, or 1,024-dimensional when dealing with very large\n",
    "vocabularies. On the other hand, one-hot encoding words generally leads to vectors\n",
    "that are 20,000-dimensional or greater (capturing a vocabulary of 20,000 tokens, in\n",
    "this case). So, word embeddings pack more information into far fewer dimensions.\n",
    "\n",
    "There are two ways to obtain word embeddings:\n",
    "* Learn word embeddings jointly with the main task you care about (such as document classification or sentiment prediction). In this setup, you start with random word vectors and then learn word vectors in the same way you learn the weights of a neural network.\n",
    "* Load into your model word embeddings that were precomputed using a different machine-learning task than the one you’re trying to solve. These are called pretrained word embeddings.\n",
    "\n",
    "### Keras Embedding Layer\n",
    "We want to earn an embedding space with every new task. An embedding space is a series of vectors which are able to map words to some other words. Example:\n",
    "\n",
    "* ```cat -> tiger``` is the same vector that maps  ```dog -> wolf``` and we can call it **wild transformator**;\n",
    "* ```king -> queen``` is the same vector that maps  ```actor -> actress``` and we can call it **gender transformator**;\n",
    "\n",
    "Fortunately, backpropagation makes this easy, and Keras makes it even easier. It’s about learning the weights of a layer: the Embedding layer.\n",
    "\n",
    "The Embedding layer is best understood as a dictionary that maps integer indices\n",
    "(which stand for specific words) to dense vectors. It takes integers as input, it looks up\n",
    "these integers in an internal dictionary, and it returns the associated vectors. It’s effectively a dictionary lookup (see figure 6.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments: number of possible tokens and dimensionality of the embeddings\n",
    "\n",
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(1000, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding layer takes as input a 2D tensor of integers, of shape (samples,\n",
    "sequence_length) , where each entry is a sequence of integers. It can embed\n",
    "sequences of variable lengths: for instance, you could feed into the Embedding layer in\n",
    "the previous example batches with shapes (32, 10) (batch of 32 sequences of length\n",
    "10) or (64, 15) (batch of 64 sequences of length 15). All sequences in a batch must\n",
    "have the same length, though (because you need to pack them into a single tensor),\n",
    "so sequences that are shorter than others should be padded with zeros, and sequences\n",
    "that are longer should be truncated.\n",
    "This layer returns a 3D floating-point tensor of shape (samples, sequence_\n",
    "length, embedding_dimensionality) . Such a 3D tensor can then be processed by\n",
    "an RNN layer or a 1D convolution layer (both will be introduced in the following\n",
    "sections).\n",
    "When you instantiate an Embedding layer, its weights (its internal dictionary of\n",
    "token vectors) are initially random, just as with any other layer. During training, these\n",
    "word vectors are gradually adjusted via backpropagation, structuring the space into\n",
    "something the downstream model can exploit. Once fully trained, the embedding\n",
    "space will show a lot of structure—a kind of structure specialized for the specific prob-\n",
    "lem for which you’re training your model.\n",
    "\n",
    "Let’s apply this idea to the IMDB movie-review sentiment-prediction task that\n",
    "you’re already familiar with. First, you’ll quickly prepare the data. You’ll restrict the\n",
    "movie reviews to the top 10,000 most common words (as you did the first time you\n",
    "worked with this dataset) and cut off the reviews after only 20 words. The network will\n",
    "learn 8-dimensional embeddings for each of the 10,000 words, turn the input integer sequences (2D integer tensor) into embedded sequences ( 3D float tensor), flatten the\n",
    "tensor to 2D, and train a single Dense layer on top for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "# Number of words to consider as features\n",
    "max_features = 10000\n",
    "\n",
    "# Cuts off the text after this number of words (among the max_features most common words)\n",
    "maxlen = 20\n",
    "\n",
    "# Loads the data as lists of integers\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(\n",
    "    num_words=max_features)\n",
    "\n",
    "# Turns the lists of integers into a 2D integer tensor of shape (samples, maxlen)\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Specifies the maximum input length to the Embedding layer so you can later flatten the \n",
    "# embedded inputs. After the Embedding layer, the activations have shape (samples, maxlen, 8).\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))\n",
    "# Flattens the 3D tensor of embeddings into a 2D tensor of shape (samples, maxlen * 8)\n",
    "model.add(Flatten())\n",
    "# Adds the classifier on top\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.6692 - acc: 0.6145 - val_loss: 0.6156 - val_acc: 0.6990\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.5375 - acc: 0.7525 - val_loss: 0.5229 - val_acc: 0.7322\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.4604 - acc: 0.7862 - val_loss: 0.5005 - val_acc: 0.7460\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.4246 - acc: 0.8065 - val_loss: 0.4939 - val_acc: 0.7518\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.3998 - acc: 0.8203 - val_loss: 0.4946 - val_acc: 0.7600\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.3796 - acc: 0.8314 - val_loss: 0.4967 - val_acc: 0.7588\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.3609 - acc: 0.8439 - val_loss: 0.5007 - val_acc: 0.7558\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.3430 - acc: 0.8550 - val_loss: 0.5068 - val_acc: 0.7550\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.3259 - acc: 0.8648 - val_loss: 0.5120 - val_acc: 0.7528\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3088 - acc: 0.8749 - val_loss: 0.5206 - val_acc: 0.7524\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, \n",
    "                    y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOWNLOADING THE IMDB DATA AS RAW TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "imdb_dir = \"./Dataset/aclImdb\"\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "labels = []\n",
    "texts = []\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am quite a fan of novelist/screenwriter Michael Chabon. His novel \"Wonder Boys\" became a fantastic movie by Curtis Hanson. His masterful novel \"The Amazing Adventures of Kavalier and Clay\" won the Pulitzer Prize a few years back, and he had a hand in the script of \"Spider Man 2\", arguably the greatest comic book movie of all time.<br /><br />Director Rawson Marshall Thurber has also directed wonderful comedic pieces, such as the gut-busting \"Dodgeball\" and the genius short film series \"Terry Tate: Office Linebacker\". And with a cast including Peter Saarsgard, Sienna Miller, Nick Nolte and Mena Suvari, this seems like a no-brainer.<br /><br />It is. Literally.<br /><br />Jon Foster stars as Art Bechstein, the son of a mobster (Nolte) who recently graduated with a degree in Economics. Jon is in a state of arrested development: he works a minimum wage job at Book Barn, has a vapid relationship with his girlfriend/boss, Phlox (Suvari), which amounts to little more than copious amounts of sex, with no plans other than to chip away at a career for which he has zero passion.<br /><br />One night at a party, an ex-roommate introduces Jon to Jane (Miller), a beautiful, smart violinist. Later that night they go out for pie, and she asks Jon a question that begins to shake him from his catatonic state of existence, \"I want you to tell me something that you have never told a single soul. If you do, it will make this night indelible.\" Jon then tells her a reoccurring dream of his in which he wanders about town looking at the faces of strangers passing him by, yet none of them look him in the eye. \"I imagine it must be what death feels like,\" he says.<br /><br />The next day Jane\\'s wild boyfriend Cleveland (Saarsgard) kidnaps Jon from work and takes him out to a hulking abandoned steel mill, and soon Jon, Cleveland and Jane are spending every waking moment together going to punk rock concerts, doing drugs and drinking lots of alcohol. This doesn\\'t sit well with Phlox, who pushes Jon for a more personal relationship, namely letting her meet his new friends and his father. The film then attempts to take us on Jon\\'s journey as he shakes off the shackles imposed on him by his father, Phlox and his dead-end job as he finds freedom and expression through his relationships with Cleveland and Jane.<br /><br />There is a problem having us follow Jon throughout the film: he\\'s completely uninteresting. He has no ambitions, passions or goals. He walks through life like the invisible wraith he described to Jane the night they met. At the outset this isn\\'t a problem. But he never gets any more interesting. He\\'s a completely passive character. He simply follows along the bohemian Cleveland and Jane, but he never once gives us any inkling as to what he cares about or wants to to do with himself.<br /><br />Consequently, the film and its supporting characters have nowhere to go and little to do other than party, have sex and get in arguments. In other words, much ado about nothing. What we have here is the shallow skin of a good movie without anything on the inside. Sweeping cinematography, ponderous voice-over with characters staring off into the distance, lots of sex scenes both straight and gay, big arguments, more angry sex, a chase scene and a tragic death... but it doesn\\'t seem to matter. Ironically, at one point Jane, confused at a number of Jon\\'s aimless actions, asks him, \"What\\'s going on, Jon? What is this all about?\" Yes, Jon, do tell. We in the audience are dying to know, too.<br /><br />The title \"The Mysteries of Pittsburgh\" must refer to the characters themselves, because that\\'s what they are. They are all facades, one-dimensional stand-ins for actual people. The film never lets us in. We never know what makes any of them tick. We see them do lots of things, but we don\\'t know why. And the absence of \"why\" is one of the worst things a movie can have.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
