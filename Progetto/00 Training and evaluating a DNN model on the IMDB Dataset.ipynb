{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 Training and evaluating a DNN model on the IMDB Dataset\n",
    "## Downloading and data preprocessing\n",
    "\n",
    "Downloaded the dataset at http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "```\n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  month     = {June},\n",
    "  year      = {2011},\n",
    "  address   = {Portland, Oregon, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {142--150},\n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9 µs, sys: 2 µs, total: 11 µs\n",
      "Wall time: 23.8 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns = ['text','sentiment'])\n",
    "\n",
    "imdb_dir = \"./datasets/aclImdb\"\n",
    "\n",
    "for dir_kind in ['train','test']:\n",
    "    for label_type in ['neg', 'pos']:\n",
    "        dir_name = os.path.join(imdb_dir, dir_kind, label_type)\n",
    "        for fname in os.listdir(dir_name):\n",
    "            if fname[-4:] == '.txt':\n",
    "                f = open(os.path.join(dir_name, fname))\n",
    "                df = df.append({'text': f.read(), 'sentiment': ['neg','pos'].index(label_type)}, ignore_index = True)\n",
    "                f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am quite a fan of novelist/screenwriter Mich...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If this book remained faithful to the book the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Eternal Jew (Der Ewige Jude) does not have...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Here are the matches . . . (adv. = advantage)&lt;...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm sorry but I didn't like this doc very much...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  I am quite a fan of novelist/screenwriter Mich...         0\n",
       "1  If this book remained faithful to the book the...         0\n",
       "2  The Eternal Jew (Der Ewige Jude) does not have...         0\n",
       "3  Here are the matches . . . (adv. = advantage)<...         0\n",
       "4  I'm sorry but I didn't like this doc very much...         0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative istances: 25000\n",
      "Number of positive istances: 25000\n",
      "Il dataset risulta essere bilanciato!\n"
     ]
    }
   ],
   "source": [
    "print ('Number of negative istances:', len(df[df['sentiment'] == 0]))\n",
    "print ('Number of positive istances:', len(df[df['sentiment'] == 1]))\n",
    "print ('Il dataset risulta essere bilanciato!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am quite a fan of novelist/screenwriter Michael Chabon. His novel \"Wonder Boys\" became a fantastic movie by Curtis Hanson. His masterful novel \"The Amazing Adventures of Kavalier and Clay\" won the Pulitzer Prize a few years back, and he had a hand in the script of \"Spider Man 2\", arguably the greatest comic book movie of all time.<br /><br />Director Rawson Marshall Thurber has also directed wonderful comedic pieces, such as the gut-busting \"Dodgeball\" and the genius short film series \"Terry Tate: Office Linebacker\". And with a cast including Peter Saarsgard, Sienna Miller, Nick Nolte and Mena Suvari, this seems like a no-brainer.<br /><br />It is. Literally.<br /><br />Jon Foster stars as Art Bechstein, the son of a mobster (Nolte) who recently graduated with a degree in Economics. Jon is in a state of arrested development: he works a minimum wage job at Book Barn, has a vapid relationship with his girlfriend/boss, Phlox (Suvari), which amounts to little more than copious amounts of sex, with no plans other than to chip away at a career for which he has zero passion.<br /><br />One night at a party, an ex-roommate introduces Jon to Jane (Miller), a beautiful, smart violinist. Later that night they go out for pie, and she asks Jon a question that begins to shake him from his catatonic state of existence, \"I want you to tell me something that you have never told a single soul. If you do, it will make this night indelible.\" Jon then tells her a reoccurring dream of his in which he wanders about town looking at the faces of strangers passing him by, yet none of them look him in the eye. \"I imagine it must be what death feels like,\" he says.<br /><br />The next day Jane's wild boyfriend Cleveland (Saarsgard) kidnaps Jon from work and takes him out to a hulking abandoned steel mill, and soon Jon, Cleveland and Jane are spending every waking moment together going to punk rock concerts, doing drugs and drinking lots of alcohol. This doesn't sit well with Phlox, who pushes Jon for a more personal relationship, namely letting her meet his new friends and his father. The film then attempts to take us on Jon's journey as he shakes off the shackles imposed on him by his father, Phlox and his dead-end job as he finds freedom and expression through his relationships with Cleveland and Jane.<br /><br />There is a problem having us follow Jon throughout the film: he's completely uninteresting. He has no ambitions, passions or goals. He walks through life like the invisible wraith he described to Jane the night they met. At the outset this isn't a problem. But he never gets any more interesting. He's a completely passive character. He simply follows along the bohemian Cleveland and Jane, but he never once gives us any inkling as to what he cares about or wants to to do with himself.<br /><br />Consequently, the film and its supporting characters have nowhere to go and little to do other than party, have sex and get in arguments. In other words, much ado about nothing. What we have here is the shallow skin of a good movie without anything on the inside. Sweeping cinematography, ponderous voice-over with characters staring off into the distance, lots of sex scenes both straight and gay, big arguments, more angry sex, a chase scene and a tragic death... but it doesn't seem to matter. Ironically, at one point Jane, confused at a number of Jon's aimless actions, asks him, \"What's going on, Jon? What is this all about?\" Yes, Jon, do tell. We in the audience are dying to know, too.<br /><br />The title \"The Mysteries of Pittsburgh\" must refer to the characters themselves, because that's what they are. They are all facades, one-dimensional stand-ins for actual people. The film never lets us in. We never know what makes any of them tick. We see them do lots of things, but we don't know why. And the absence of \"why\" is one of the worst things a movie can have.\n"
     ]
    }
   ],
   "source": [
    "print(df['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/spola/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopword_list = stopwords.words('english')\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    return BeautifulSoup(text, 'lxml').text\n",
    "\n",
    "def remove_special_characters (text):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    return re.sub(pattern,' ',text)\n",
    "\n",
    "def stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = ToktokTokenizer().tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    text = remove_html_tags(text)\n",
    "    text = remove_special_characters (text)\n",
    "    #text = stemmer(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quite fan novelist screenwriter Michael Chabon novel Wonder Boys became fantastic movie Curtis Hanson masterful novel Amazing Adventures Kavalier Clay Pulitzer Prize years back hand script Spider Man 2 arguably greatest comic book movie time Director Rawson Marshall Thurber also directed wonderful comedic pieces gut busting Dodgeball genius short film series Terry Tate Office Linebacker cast including Peter Saarsgard Sienna Miller Nick Nolte Mena Suvari seems like brainer Literally Jon Foster stars Art Bechstein son mobster Nolte recently graduated degree Economics Jon state arrested development works minimum wage job Book Barn vapid relationship girlfriend boss Phlox Suvari amounts little copious amounts sex plans chip away career zero passion One night party ex roommate introduces Jon Jane Miller beautiful smart violinist Later night go pie asks Jon question begins shake catatonic state existence want tell something never told single soul make night indelible Jon tells reoccurring dream wanders town looking faces strangers passing yet none look eye imagine must death feels like says next day Jane wild boyfriend Cleveland Saarsgard kidnaps Jon work takes hulking abandoned steel mill soon Jon Cleveland Jane spending every waking moment together going punk rock concerts drugs drinking lots alcohol sit well Phlox pushes Jon personal relationship namely letting meet new friends father film attempts take us Jon journey shakes shackles imposed father Phlox dead end job finds freedom expression relationships Cleveland Jane problem us follow Jon throughout film completely uninteresting ambitions passions goals walks life like invisible wraith described Jane night met outset problem never gets interesting completely passive character simply follows along bohemian Cleveland Jane never gives us inkling cares wants Consequently film supporting characters nowhere go little party sex get arguments words much ado nothing shallow skin good movie without anything inside Sweeping cinematography ponderous voice characters staring distance lots sex scenes straight gay big arguments angry sex chase scene tragic death seem matter Ironically one point Jane confused number Jon aimless actions asks going Jon Yes Jon tell audience dying know title Mysteries Pittsburgh must refer characters facades one dimensional stand ins actual people film never lets us never know makes tick see lots things know absence one worst things movie\n"
     ]
    }
   ],
   "source": [
    "print(text_preprocessing(df['text'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: text_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quite fan novelist screenwriter Michael Chabon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>book remained faithful book assume author igno...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eternal Jew Der Ewige Jude today would call ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>matches adv advantage Warriors Ultimate Warrio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sorry like doc much think million ways could b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  quite fan novelist screenwriter Michael Chabon...         0\n",
       "1  book remained faithful book assume author igno...         0\n",
       "2  Eternal Jew Der Ewige Jude today would call ma...         0\n",
       "3  matches adv advantage Warriors Ultimate Warrio...         0\n",
       "4  sorry like doc much think million ways could b...         0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'df' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to restore variable 'model', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n"
     ]
    }
   ],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(df['text'], df['sentiment'], test_size = 0.33, shuffle = True)\n",
    "\n",
    "x_train = list(x_train)\n",
    "x_val = list(x_val)\n",
    "\n",
    "y_train = list(y_train)\n",
    "y_val = list(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 103331 unique tokens.\n",
      "Shape of train data tensor: (33500, 1429)\n",
      "Shape of train label tensor: (33500,)\n",
      "Shape of validation data tensor: (16500, 1429)\n",
      "Shape of validation label tensor: (16500,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "texts = x_train + x_val\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "maxlen = max([len(t.split()) for t in texts])\n",
    "\n",
    "words_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "val_sequences = tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "print('Found %s unique tokens.' % len(tokenizer.word_index))\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen = maxlen)\n",
    "val_data = pad_sequences(val_sequences, maxlen = maxlen)\n",
    "\n",
    "y_train = np.asarray(y_train)\n",
    "y_val = np.asarray(y_val)\n",
    "print('Shape of train data tensor:', train_data.shape)\n",
    "print('Shape of train label tensor:', y_train.shape)\n",
    "\n",
    "print('Shape of validation data tensor:', val_data.shape)\n",
    "print('Shape of validation label tensor:', y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,  7589,   329,     6],\n",
       "       [    0,     0,     0, ..., 61040,   149,   291],\n",
       "       [    0,     0,     0, ...,  1804, 18227,  1847],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,  1745,    94,   866],\n",
       "       [    0,     0,     0, ...,  1565,  1481,  1802],\n",
       "       [    0,     0,     0, ...,   254,   546,     2]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras import regularizers\n",
    "from keras import layers\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='acc',\n",
    "        patience=5\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='my_model.h5',\n",
    "        monitor='val_acc',\n",
    "        save_best_only=True\n",
    "    ),\n",
    "        \n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.1,\n",
    "        patience=5,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_fitted_model(dropout = 0.5, layer_num = 1, init_mode='uniform',\n",
    "                       regularizer = None, batch_size = 128):\n",
    "    \n",
    "    seed = 7\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    str_kernel_regularizer = None\n",
    "    if (regularizer is not None):\n",
    "        str_kernel_regularizer = str(regularizer)\n",
    "    else:\n",
    "        str_kernel_regularizer = 'None'\n",
    "    print('\\n', f'Training Model with:', '\\n',\n",
    "    f'* dropout = {dropout};', '\\n',\n",
    "    f'* number of hidden layers = {layer_num};', '\\n',\n",
    "    f'* init mode = {init_mode};', '\\n',\n",
    "    f'* l2 kernel regularizer value = {str_kernel_regularizer};', '\\n',\n",
    "    f'* batch size = {batch_size}')\n",
    "    \n",
    "    def add_layers():\n",
    "        for i in range (0, layer_num):\n",
    "            if (regularizer):\n",
    "                model.add(Dense(64, kernel_initializer=init_mode, activation='relu',\n",
    "                               kernel_regularizer = regularizers.l2(regularizer)))\n",
    "            else:\n",
    "                model.add(Dense(64, kernel_initializer=init_mode, activation='relu'))\n",
    "            model.add(layers.Dropout(dropout))\n",
    "    \n",
    "    EMBEDDING_DIM = 100\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(words_size, EMBEDDING_DIM, input_length=maxlen))\n",
    "    model.add(Flatten())\n",
    "    add_layers()\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "    history = model.fit(train_data, y_train,\n",
    "                        epochs=10,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_data=(val_data, y_val),\n",
    "                        verbose = 2)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict(dropout = [0.2, 0.5, 0.65, 0.8],\n",
    "                       layer_num = [1,2,3],\n",
    "                       regularizer = [0.1, 0.01, 0.001],\n",
    "                       batch_size =[128,512],\n",
    "                       init_mode = ['uniform', 'lecun_uniform', 'normal', \n",
    "                                    'glorot_normal', 'glorot_uniform']\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model with: \n",
      " * dropout = 0.2; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = uniform; \n",
      " * l2 kernel regularizer value = None; \n",
      " * batch size = 128\n",
      "Epoch 1/10\n",
      "262/262 - 74s - loss: 0.5783 - acc: 0.7562 - val_loss: 0.2486 - val_acc: 0.8998 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "262/262 - 69s - loss: 0.1507 - acc: 0.9442 - val_loss: 0.2446 - val_acc: 0.9026 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "262/262 - 67s - loss: 0.0379 - acc: 0.9883 - val_loss: 0.3281 - val_acc: 0.8942 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "262/262 - 66s - loss: 0.0072 - acc: 0.9981 - val_loss: 0.4558 - val_acc: 0.8884 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "262/262 - 67s - loss: 0.0036 - acc: 0.9988 - val_loss: 0.5584 - val_acc: 0.8938 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "262/262 - 66s - loss: 7.9427e-04 - acc: 0.9996 - val_loss: 0.6350 - val_acc: 0.8898 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "262/262 - 70s - loss: 5.8435e-04 - acc: 0.9999 - val_loss: 0.6777 - val_acc: 0.8908 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "262/262 - 69s - loss: 1.6233e-06 - acc: 1.0000 - val_loss: 0.7774 - val_acc: 0.8890 - lr: 1.0000e-04\n",
      "Epoch 9/10\n",
      "262/262 - 68s - loss: 5.2064e-07 - acc: 1.0000 - val_loss: 0.7747 - val_acc: 0.8911 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "262/262 - 67s - loss: 3.3146e-07 - acc: 1.0000 - val_loss: 0.7896 - val_acc: 0.8911 - lr: 1.0000e-04\n",
      "Training Model with: \n",
      " * dropout = 0.5; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = uniform; \n",
      " * l2 kernel regularizer value = None; \n",
      " * batch size = 128\n",
      "Epoch 1/10\n",
      "262/262 - 70s - loss: 0.7545 - acc: 0.5034 - val_loss: 0.6933 - val_acc: 0.4945 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "262/262 - 70s - loss: 0.5490 - acc: 0.6727 - val_loss: 0.2643 - val_acc: 0.8917 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "262/262 - 70s - loss: 0.2011 - acc: 0.9221 - val_loss: 0.2694 - val_acc: 0.8949 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "262/262 - 71s - loss: 0.0712 - acc: 0.9764 - val_loss: 0.3060 - val_acc: 0.8968 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "262/262 - 69s - loss: 0.0163 - acc: 0.9954 - val_loss: 0.4188 - val_acc: 0.8934 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "262/262 - 68s - loss: 0.0042 - acc: 0.9985 - val_loss: 0.4993 - val_acc: 0.8926 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "262/262 - 68s - loss: 4.3784e-04 - acc: 0.9999 - val_loss: 0.6871 - val_acc: 0.8906 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "262/262 - 68s - loss: 3.1946e-05 - acc: 1.0000 - val_loss: 0.7037 - val_acc: 0.8935 - lr: 1.0000e-04\n",
      "Epoch 9/10\n",
      "262/262 - 69s - loss: 1.5606e-05 - acc: 1.0000 - val_loss: 0.7156 - val_acc: 0.8939 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "262/262 - 68s - loss: 2.7754e-05 - acc: 1.0000 - val_loss: 0.7401 - val_acc: 0.8945 - lr: 1.0000e-04\n",
      "Training Model with: \n",
      " * dropout = 0.65; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = uniform; \n",
      " * l2 kernel regularizer value = None; \n",
      " * batch size = 128\n",
      "Epoch 1/10\n",
      "262/262 - 69s - loss: 0.8061 - acc: 0.5016 - val_loss: 0.6932 - val_acc: 0.4945 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "262/262 - 69s - loss: 0.6965 - acc: 0.5098 - val_loss: 0.6706 - val_acc: 0.5172 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "262/262 - 68s - loss: 0.3600 - acc: 0.8421 - val_loss: 0.2567 - val_acc: 0.8952 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "262/262 - 67s - loss: 0.1497 - acc: 0.9453 - val_loss: 0.2480 - val_acc: 0.9022 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "262/262 - 68s - loss: 0.0509 - acc: 0.9828 - val_loss: 0.3551 - val_acc: 0.8967 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "262/262 - 68s - loss: 0.0132 - acc: 0.9965 - val_loss: 0.4492 - val_acc: 0.8971 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "262/262 - 67s - loss: 0.0041 - acc: 0.9988 - val_loss: 0.5392 - val_acc: 0.8741 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "262/262 - 71s - loss: 0.0018 - acc: 0.9996 - val_loss: 0.6134 - val_acc: 0.8975 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "262/262 - 70s - loss: 7.3303e-04 - acc: 0.9998 - val_loss: 0.7647 - val_acc: 0.8963 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "262/262 - 67s - loss: 1.2432e-04 - acc: 1.0000 - val_loss: 0.8779 - val_acc: 0.8964 - lr: 1.0000e-04\n",
      "Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = uniform; \n",
      " * l2 kernel regularizer value = None; \n",
      " * batch size = 128\n",
      "Epoch 1/10\n",
      "262/262 - 69s - loss: 0.7392 - acc: 0.4976 - val_loss: 0.6932 - val_acc: 0.4946 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "262/262 - 68s - loss: 0.6945 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4945 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "262/262 - 66s - loss: 0.6948 - acc: 0.5164 - val_loss: 0.6521 - val_acc: 0.7465 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "262/262 - 65s - loss: 0.3807 - acc: 0.8342 - val_loss: 0.2586 - val_acc: 0.8983 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "262/262 - 68s - loss: 0.1873 - acc: 0.9315 - val_loss: 0.2473 - val_acc: 0.9064 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "262/262 - 68s - loss: 0.1024 - acc: 0.9660 - val_loss: 0.3138 - val_acc: 0.8971 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "262/262 - 68s - loss: 0.0462 - acc: 0.9852 - val_loss: 0.3709 - val_acc: 0.8988 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "262/262 - 66s - loss: 0.0196 - acc: 0.9941 - val_loss: 0.5494 - val_acc: 0.8984 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "262/262 - 66s - loss: 0.0095 - acc: 0.9967 - val_loss: 0.6817 - val_acc: 0.8973 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "262/262 - 66s - loss: 0.0052 - acc: 0.9982 - val_loss: 0.7909 - val_acc: 0.8991 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "dict_dropout_histories = {}\n",
    "best_dropout = 0.5\n",
    "best_dropout_acc = 0\n",
    "for i in hyperparameters['dropout']:\n",
    "    history = get_fitted_model(dropout = i)\n",
    "    if max(history.history['val_acc']) > best_dropout_acc:\n",
    "        best_dropout = i\n",
    "        best_dropout_acc = max(history.history['val_acc'])\n",
    "    dict_dropout_histories[str(i)] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(max(dict_dropout_histories[str(best_dropout)].history['val_acc']))\n",
    "#print(best_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dropout = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = uniform; \n",
      " * l2 kernel regularizer value = None; \n",
      " * batch size = 128\n",
      "Epoch 1/10\n",
      "262/262 - 67s - loss: 0.7365 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.4945 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "262/262 - 67s - loss: 0.6949 - acc: 0.5012 - val_loss: 0.6932 - val_acc: 0.4945 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "262/262 - 66s - loss: 0.6962 - acc: 0.5033 - val_loss: 0.6928 - val_acc: 0.5077 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "262/262 - 67s - loss: 0.5884 - acc: 0.6457 - val_loss: 0.3182 - val_acc: 0.8843 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "262/262 - 68s - loss: 0.2597 - acc: 0.9007 - val_loss: 0.2396 - val_acc: 0.9042 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "262/262 - 67s - loss: 0.1478 - acc: 0.9476 - val_loss: 0.2685 - val_acc: 0.9042 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "262/262 - 70s - loss: 0.0771 - acc: 0.9746 - val_loss: 0.3225 - val_acc: 0.9015 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "262/262 - 69s - loss: 0.0318 - acc: 0.9901 - val_loss: 0.5237 - val_acc: 0.8853 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "262/262 - 70s - loss: 0.0153 - acc: 0.9955 - val_loss: 0.5769 - val_acc: 0.8988 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "262/262 - 71s - loss: 0.0080 - acc: 0.9975 - val_loss: 0.7065 - val_acc: 0.8962 - lr: 0.0010\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 2; \n",
      " * init mode = uniform; \n",
      " * l2 kernel regularizer value = None; \n",
      " * batch size = 128\n",
      "Epoch 1/10\n",
      "262/262 - 68s - loss: 0.7052 - acc: 0.4981 - val_loss: 0.6932 - val_acc: 0.4945 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "262/262 - 68s - loss: 0.6957 - acc: 0.5025 - val_loss: 0.6933 - val_acc: 0.4945 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "262/262 - 68s - loss: 0.6941 - acc: 0.5162 - val_loss: 0.6828 - val_acc: 0.5812 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "262/262 - 68s - loss: 0.4509 - acc: 0.7900 - val_loss: 0.2578 - val_acc: 0.8940 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "262/262 - 69s - loss: 0.2476 - acc: 0.9157 - val_loss: 0.2733 - val_acc: 0.8958 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "262/262 - 69s - loss: 0.1656 - acc: 0.9466 - val_loss: 0.3275 - val_acc: 0.9005 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "262/262 - 69s - loss: 0.1004 - acc: 0.9694 - val_loss: 0.3914 - val_acc: 0.8985 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "262/262 - 68s - loss: 0.0577 - acc: 0.9824 - val_loss: 0.5833 - val_acc: 0.8925 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "262/262 - 70s - loss: 0.0359 - acc: 0.9903 - val_loss: 0.7592 - val_acc: 0.8940 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "262/262 - 70s - loss: 0.0132 - acc: 0.9971 - val_loss: 0.9161 - val_acc: 0.8978 - lr: 1.0000e-04\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 3; \n",
      " * init mode = uniform; \n",
      " * l2 kernel regularizer value = None; \n",
      " * batch size = 128\n",
      "Epoch 1/10\n",
      "262/262 - 72s - loss: 0.6972 - acc: 0.5014 - val_loss: 0.6933 - val_acc: 0.4945 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "262/262 - 71s - loss: 0.6933 - acc: 0.4985 - val_loss: 0.6932 - val_acc: 0.4945 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "262/262 - 71s - loss: 0.6943 - acc: 0.5009 - val_loss: 0.6932 - val_acc: 0.4944 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "262/262 - 71s - loss: 0.6941 - acc: 0.4974 - val_loss: 0.6932 - val_acc: 0.4945 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "262/262 - 70s - loss: 0.6939 - acc: 0.5002 - val_loss: 0.6932 - val_acc: 0.4945 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "262/262 - 71s - loss: 0.6940 - acc: 0.4999 - val_loss: 0.6931 - val_acc: 0.5064 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "dict_layers_num_histories = {}\n",
    "best_layer_num = 1\n",
    "best_layer_num_acc = 0\n",
    "for i in hyperparameters['layer_num']:\n",
    "    history = get_fitted_model(dropout = best_dropout, layer_num = i)\n",
    "    if max(history.history['val_acc']) > best_layer_num_acc:\n",
    "        best_layer_num = i\n",
    "        best_layer_num_acc = max(history.history['val_acc'])\n",
    "    dict_layers_num_histories[str(i)] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9041818380355835\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(max(dict_layers_num_histories[str(best_layer_num)].history['val_acc']))\n",
    "print(best_layer_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = uniform; \n",
      " * l2 kernel regularizer value = None; \n",
      " * batch size = 128\n",
      "Epoch 1/10\n",
      "262/262 - 73s - loss: 0.7706 - acc: 0.5003 - val_loss: 0.6932 - val_acc: 0.4945 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "262/262 - 74s - loss: 0.6749 - acc: 0.5578 - val_loss: 0.4227 - val_acc: 0.8475 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "262/262 - 73s - loss: 0.3166 - acc: 0.8763 - val_loss: 0.2416 - val_acc: 0.9024 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "262/262 - 73s - loss: 0.1737 - acc: 0.9390 - val_loss: 0.2539 - val_acc: 0.9040 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "262/262 - 74s - loss: 0.0931 - acc: 0.9696 - val_loss: 0.3177 - val_acc: 0.9022 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "262/262 - 74s - loss: 0.0403 - acc: 0.9875 - val_loss: 0.4204 - val_acc: 0.8990 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "262/262 - 73s - loss: 0.0188 - acc: 0.9944 - val_loss: 0.5389 - val_acc: 0.8969 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "262/262 - 74s - loss: 0.0097 - acc: 0.9972 - val_loss: 0.6026 - val_acc: 0.8970 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "262/262 - 73s - loss: 0.0034 - acc: 0.9994 - val_loss: 0.7086 - val_acc: 0.8959 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "262/262 - 73s - loss: 0.0029 - acc: 0.9995 - val_loss: 0.7579 - val_acc: 0.8965 - lr: 1.0000e-04\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = lecun_uniform; \n",
      " * l2 kernel regularizer value = None; \n",
      " * batch size = 128\n",
      "Epoch 1/10\n",
      "262/262 - 74s - loss: 0.7764 - acc: 0.5012 - val_loss: 0.6932 - val_acc: 0.4945 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "262/262 - 73s - loss: 0.6957 - acc: 0.5030 - val_loss: 0.6932 - val_acc: 0.4945 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "262/262 - 73s - loss: 0.4880 - acc: 0.7323 - val_loss: 0.2703 - val_acc: 0.8907 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "262/262 - 73s - loss: 0.2157 - acc: 0.9203 - val_loss: 0.2470 - val_acc: 0.9040 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "262/262 - 72s - loss: 0.1188 - acc: 0.9605 - val_loss: 0.2906 - val_acc: 0.9015 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "262/262 - 73s - loss: 0.0544 - acc: 0.9827 - val_loss: 0.3771 - val_acc: 0.8998 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "262/262 - 72s - loss: 0.0220 - acc: 0.9930 - val_loss: 0.5052 - val_acc: 0.8963 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "262/262 - 70s - loss: 0.0109 - acc: 0.9967 - val_loss: 0.6279 - val_acc: 0.8944 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "262/262 - 69s - loss: 0.0070 - acc: 0.9977 - val_loss: 0.6931 - val_acc: 0.8932 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "262/262 - 69s - loss: 0.0020 - acc: 0.9997 - val_loss: 0.8412 - val_acc: 0.8981 - lr: 1.0000e-04\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = normal; \n",
      " * l2 kernel regularizer value = None; \n",
      " * batch size = 128\n",
      "Epoch 1/10\n",
      "262/262 - 70s - loss: 0.7635 - acc: 0.4955 - val_loss: 0.6932 - val_acc: 0.4944 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "262/262 - 70s - loss: 0.6972 - acc: 0.5024 - val_loss: 0.6925 - val_acc: 0.5232 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "262/262 - 71s - loss: 0.6157 - acc: 0.6233 - val_loss: 0.3191 - val_acc: 0.8741 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "262/262 - 71s - loss: 0.2762 - acc: 0.8942 - val_loss: 0.2577 - val_acc: 0.8935 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "262/262 - 72s - loss: 0.1577 - acc: 0.9436 - val_loss: 0.2882 - val_acc: 0.8911 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "262/262 - 72s - loss: 0.0852 - acc: 0.9721 - val_loss: 0.2885 - val_acc: 0.8991 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "262/262 - 71s - loss: 0.0380 - acc: 0.9880 - val_loss: 0.3786 - val_acc: 0.8997 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "262/262 - 73s - loss: 0.0161 - acc: 0.9955 - val_loss: 0.5202 - val_acc: 0.8891 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "262/262 - 69s - loss: 0.0093 - acc: 0.9973 - val_loss: 0.6174 - val_acc: 0.8967 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "262/262 - 69s - loss: 0.0032 - acc: 0.9993 - val_loss: 0.7393 - val_acc: 0.8972 - lr: 1.0000e-04\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = glorot_normal; \n",
      " * l2 kernel regularizer value = None; \n",
      " * batch size = 128\n",
      "Epoch 1/10\n",
      "262/262 - 70s - loss: 0.8084 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4945 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "262/262 - 70s - loss: 0.6992 - acc: 0.5130 - val_loss: 0.6631 - val_acc: 0.6468 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "262/262 - 71s - loss: 0.3627 - acc: 0.8465 - val_loss: 0.2416 - val_acc: 0.9039 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "262/262 - 70s - loss: 0.1843 - acc: 0.9333 - val_loss: 0.2465 - val_acc: 0.9055 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "262/262 - 70s - loss: 0.0985 - acc: 0.9669 - val_loss: 0.3302 - val_acc: 0.9022 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "262/262 - 70s - loss: 0.0418 - acc: 0.9866 - val_loss: 0.3800 - val_acc: 0.8995 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "262/262 - 70s - loss: 0.0194 - acc: 0.9941 - val_loss: 0.5452 - val_acc: 0.8954 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "262/262 - 70s - loss: 0.0101 - acc: 0.9971 - val_loss: 0.6758 - val_acc: 0.8995 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "262/262 - 70s - loss: 0.0029 - acc: 0.9992 - val_loss: 0.7388 - val_acc: 0.8992 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "262/262 - 73s - loss: 0.0030 - acc: 0.9989 - val_loss: 0.7585 - val_acc: 0.8998 - lr: 1.0000e-04\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = glorot_uniform; \n",
      " * l2 kernel regularizer value = None; \n",
      " * batch size = 128\n",
      "Epoch 1/10\n",
      "262/262 - 74s - loss: 0.7496 - acc: 0.5024 - val_loss: 0.6932 - val_acc: 0.4945 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "262/262 - 74s - loss: 0.6789 - acc: 0.5333 - val_loss: 0.4852 - val_acc: 0.7972 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "262/262 - 74s - loss: 0.3062 - acc: 0.8798 - val_loss: 0.2430 - val_acc: 0.9027 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "262/262 - 74s - loss: 0.1713 - acc: 0.9394 - val_loss: 0.2656 - val_acc: 0.8963 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "262/262 - 73s - loss: 0.0975 - acc: 0.9674 - val_loss: 0.3186 - val_acc: 0.9021 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "262/262 - 71s - loss: 0.0457 - acc: 0.9858 - val_loss: 0.4246 - val_acc: 0.8970 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "262/262 - 72s - loss: 0.0192 - acc: 0.9939 - val_loss: 0.5721 - val_acc: 0.8975 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "262/262 - 72s - loss: 0.0090 - acc: 0.9973 - val_loss: 0.7135 - val_acc: 0.8976 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "262/262 - 71s - loss: 0.0026 - acc: 0.9992 - val_loss: 0.7789 - val_acc: 0.8981 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "262/262 - 71s - loss: 0.0027 - acc: 0.9991 - val_loss: 0.7978 - val_acc: 0.8981 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "dict_init_mode_histories = {}\n",
    "best_init_mode = 'uniform'\n",
    "best_init_mode_acc = 0\n",
    "for i in hyperparameters['init_mode']:\n",
    "    history = get_fitted_model(dropout = best_dropout, layer_num = best_layer_num, init_mode = i)\n",
    "    if max(history.history['val_acc']) > best_init_mode_acc:\n",
    "        best_init_mode = i\n",
    "        best_init_mode_acc = max(history.history['val_acc'])\n",
    "    dict_init_mode_histories[str(i)] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9055151343345642\n",
      "glorot_normal\n"
     ]
    }
   ],
   "source": [
    "print(max(dict_init_mode_histories[str(best_init_mode)].history['val_acc']))\n",
    "print(best_init_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = glorot_normal; \n",
      " * l2 kernel regularizer value = None; \n",
      " * batch size = 128\n",
      "Epoch 1/10\n",
      "262/262 - 73s - loss: 0.8211 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.4945 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "262/262 - 74s - loss: 0.6916 - acc: 0.5206 - val_loss: 0.5398 - val_acc: 0.7776 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "262/262 - 74s - loss: 0.3332 - acc: 0.8646 - val_loss: 0.2405 - val_acc: 0.9038 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "262/262 - 75s - loss: 0.1769 - acc: 0.9381 - val_loss: 0.2641 - val_acc: 0.9030 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "262/262 - 75s - loss: 0.0934 - acc: 0.9701 - val_loss: 0.3183 - val_acc: 0.8984 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "262/262 - 73s - loss: 0.0398 - acc: 0.9883 - val_loss: 0.4168 - val_acc: 0.9018 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "262/262 - 71s - loss: 0.0202 - acc: 0.9942 - val_loss: 0.5338 - val_acc: 0.8979 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "262/262 - 71s - loss: 0.0104 - acc: 0.9969 - val_loss: 0.6205 - val_acc: 0.8964 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "262/262 - 71s - loss: 0.0041 - acc: 0.9993 - val_loss: 0.7622 - val_acc: 0.8956 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "262/262 - 71s - loss: 0.0039 - acc: 0.9993 - val_loss: 0.8033 - val_acc: 0.8913 - lr: 1.0000e-04\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = glorot_normal; \n",
      " * l2 kernel regularizer value = None; \n",
      " * batch size = 512\n",
      "Epoch 1/10\n",
      "66/66 - 42s - loss: 0.8257 - acc: 0.4990 - val_loss: 0.7267 - val_acc: 0.4945 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "66/66 - 41s - loss: 0.6996 - acc: 0.5020 - val_loss: 0.6932 - val_acc: 0.4945 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "66/66 - 41s - loss: 0.6935 - acc: 0.5030 - val_loss: 0.6932 - val_acc: 0.4945 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "66/66 - 41s - loss: 0.6966 - acc: 0.5135 - val_loss: 0.6430 - val_acc: 0.6608 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "66/66 - 41s - loss: 0.4043 - acc: 0.8242 - val_loss: 0.2504 - val_acc: 0.8965 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "66/66 - 42s - loss: 0.2095 - acc: 0.9275 - val_loss: 0.2428 - val_acc: 0.9047 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "66/66 - 41s - loss: 0.1372 - acc: 0.9567 - val_loss: 0.2645 - val_acc: 0.8947 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "66/66 - 43s - loss: 0.0843 - acc: 0.9749 - val_loss: 0.2891 - val_acc: 0.9022 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "66/66 - 43s - loss: 0.0451 - acc: 0.9883 - val_loss: 0.3183 - val_acc: 0.8988 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "66/66 - 45s - loss: 0.0233 - acc: 0.9947 - val_loss: 0.4311 - val_acc: 0.8975 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "dict_batch_size_histories = {}\n",
    "best_batch_size = 128\n",
    "best_batch_size_acc = 0\n",
    "for i in hyperparameters['batch_size']:\n",
    "    history = get_fitted_model(dropout = best_dropout, layer_num = best_layer_num, \n",
    "                              init_mode = best_init_mode, batch_size = i)\n",
    "    if max(history.history['val_acc']) > best_batch_size_acc:\n",
    "        best_batch_size = i\n",
    "        best_batch_size_acc = max(history.history['val_acc'])\n",
    "    dict_batch_size_histories[str(i)] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9047272801399231\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(max(dict_batch_size_histories[str(best_batch_size)].history['val_acc']))\n",
    "print(best_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = glorot_normal; \n",
      " * l2 kernel regularizer value = 0.1; \n",
      " * batch size = 512\n",
      "Epoch 1/10\n",
      "66/66 - 43s - loss: 1.3671 - acc: 0.5021 - val_loss: 0.9157 - val_acc: 0.4945 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "66/66 - 46s - loss: 0.9335 - acc: 0.6421 - val_loss: 1.0309 - val_acc: 0.7313 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "66/66 - 47s - loss: 0.8637 - acc: 0.8147 - val_loss: 0.8550 - val_acc: 0.8531 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "66/66 - 46s - loss: 0.7398 - acc: 0.8678 - val_loss: 0.8150 - val_acc: 0.8856 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "66/66 - 46s - loss: 0.7182 - acc: 0.8844 - val_loss: 0.6791 - val_acc: 0.8897 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "66/66 - 47s - loss: 0.6763 - acc: 0.9003 - val_loss: 0.8295 - val_acc: 0.8952 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "66/66 - 46s - loss: 0.6990 - acc: 0.9034 - val_loss: 0.7381 - val_acc: 0.8944 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "66/66 - 47s - loss: 0.6375 - acc: 0.9144 - val_loss: 0.7777 - val_acc: 0.8964 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "66/66 - 47s - loss: 0.6385 - acc: 0.9192 - val_loss: 0.7481 - val_acc: 0.8870 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "66/66 - 46s - loss: 0.6572 - acc: 0.9182 - val_loss: 0.7134 - val_acc: 0.8964 - lr: 0.0010\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = glorot_normal; \n",
      " * l2 kernel regularizer value = 0.01; \n",
      " * batch size = 512\n",
      "Epoch 1/10\n",
      "66/66 - 47s - loss: 0.9901 - acc: 0.5035 - val_loss: 0.7329 - val_acc: 0.4947 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "66/66 - 46s - loss: 0.7232 - acc: 0.5789 - val_loss: 0.5893 - val_acc: 0.8468 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "66/66 - 45s - loss: 0.5821 - acc: 0.8446 - val_loss: 0.5015 - val_acc: 0.8906 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "66/66 - 44s - loss: 0.4620 - acc: 0.8958 - val_loss: 0.4358 - val_acc: 0.8991 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "66/66 - 45s - loss: 0.3995 - acc: 0.9173 - val_loss: 0.3750 - val_acc: 0.9028 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "66/66 - 44s - loss: 0.3852 - acc: 0.9263 - val_loss: 0.3939 - val_acc: 0.9029 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "66/66 - 44s - loss: 0.3679 - acc: 0.9355 - val_loss: 0.3844 - val_acc: 0.9036 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "66/66 - 44s - loss: 0.3294 - acc: 0.9474 - val_loss: 0.3687 - val_acc: 0.9036 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "66/66 - 44s - loss: 0.3127 - acc: 0.9532 - val_loss: 0.3712 - val_acc: 0.9015 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "66/66 - 45s - loss: 0.2932 - acc: 0.9584 - val_loss: 0.4064 - val_acc: 0.8987 - lr: 0.0010\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = glorot_normal; \n",
      " * l2 kernel regularizer value = 0.001; \n",
      " * batch size = 512\n",
      "Epoch 1/10\n",
      "66/66 - 44s - loss: 0.9429 - acc: 0.4993 - val_loss: 0.6988 - val_acc: 0.4948 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "66/66 - 44s - loss: 0.7285 - acc: 0.5001 - val_loss: 0.7074 - val_acc: 0.6224 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "66/66 - 45s - loss: 0.5309 - acc: 0.7749 - val_loss: 0.3557 - val_acc: 0.8940 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "66/66 - 44s - loss: 0.3442 - acc: 0.9088 - val_loss: 0.3499 - val_acc: 0.8939 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "66/66 - 44s - loss: 0.2932 - acc: 0.9349 - val_loss: 0.3210 - val_acc: 0.9042 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "66/66 - 44s - loss: 0.2441 - acc: 0.9540 - val_loss: 0.3223 - val_acc: 0.9030 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "66/66 - 44s - loss: 0.1950 - acc: 0.9702 - val_loss: 0.3370 - val_acc: 0.8946 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "66/66 - 44s - loss: 0.2009 - acc: 0.9748 - val_loss: 0.3358 - val_acc: 0.8979 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "66/66 - 44s - loss: 0.1478 - acc: 0.9810 - val_loss: 0.3581 - val_acc: 0.9005 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "66/66 - 44s - loss: 0.1461 - acc: 0.9833 - val_loss: 0.3871 - val_acc: 0.8979 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "dict_regularizers_histories = {}\n",
    "best_regularizer = 0.01\n",
    "best_regularizer_acc = 0\n",
    "for i in hyperparameters['regularizer']:\n",
    "    history = get_fitted_model(dropout = best_dropout, layer_num = best_layer_num, \n",
    "                              init_mode = best_init_mode, batch_size = best_batch_size, regularizer = i)\n",
    "    if max(history.history['val_acc']) > best_regularizer_acc:\n",
    "        best_regularizer = i\n",
    "        best_regularizer_acc = max(history.history['val_acc'])\n",
    "    dict_regularizers_histories[str(i)] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9042423963546753\n",
      "0.001\n"
     ]
    }
   ],
   "source": [
    "print(max(dict_regularizers_histories[str(best_regularizer)].history['val_acc']))\n",
    "print(best_regularizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "66/66 - 59s - loss: 1.2264 - acc: 0.5003 - val_loss: 0.7014 - val_acc: 0.5055 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "66/66 - 45s - loss: 0.7280 - acc: 0.5050 - val_loss: 0.6802 - val_acc: 0.6028 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "66/66 - 45s - loss: 0.4785 - acc: 0.8256 - val_loss: 0.3541 - val_acc: 0.8989 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "66/66 - 45s - loss: 0.3214 - acc: 0.9195 - val_loss: 0.3406 - val_acc: 0.8988 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "66/66 - 45s - loss: 0.2830 - acc: 0.9381 - val_loss: 0.3245 - val_acc: 0.9020 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "66/66 - 45s - loss: 0.2384 - acc: 0.9545 - val_loss: 0.3320 - val_acc: 0.9046 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "66/66 - 45s - loss: 0.2001 - acc: 0.9670 - val_loss: 0.3446 - val_acc: 0.9031 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "66/66 - 45s - loss: 0.2043 - acc: 0.9750 - val_loss: 0.3628 - val_acc: 0.9003 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "66/66 - 46s - loss: 0.1195 - acc: 0.9906 - val_loss: 0.3385 - val_acc: 0.8982 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "66/66 - 46s - loss: 0.1770 - acc: 0.9799 - val_loss: 0.3489 - val_acc: 0.8965 - lr: 0.0010\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'text_preprocessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-b41d23e7bf2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m                \"it was good\"]\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mtest_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-153-b41d23e7bf2c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     55\u001b[0m                \"it was good\"]\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mtest_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_preprocessing' is not defined"
     ]
    }
   ],
   "source": [
    "def get_best_model(dropout = 0.5, layer_num = 1, init_mode='uniform',\n",
    "                       regularizer = None, batch_size = 128):\n",
    "    \n",
    "    seed = 7\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    def add_layers():\n",
    "        for i in range (0, layer_num):\n",
    "            if (regularizer):\n",
    "                model.add(Dense(64, kernel_initializer=init_mode, activation='relu',\n",
    "                               kernel_regularizer = regularizers.l2(regularizer)))\n",
    "            else:\n",
    "                model.add(Dense(64, kernel_initializer=init_mode, activation='relu'))\n",
    "            model.add(layers.Dropout(dropout))\n",
    "    \n",
    "    EMBEDDING_DIM = 100\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(words_size, EMBEDDING_DIM, input_length=maxlen))\n",
    "    model.add(Flatten())\n",
    "    add_layers()\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(words_size, EMBEDDING_DIM, input_length=maxlen))\n",
    "    model.add(Flatten())\n",
    "    add_layers()\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "    history = model.fit(train_data, y_train,\n",
    "                        epochs=10,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_data=(val_data, y_val),\n",
    "                        verbose = 2)\n",
    "    model.load_weights('my_model.h5')\n",
    "    \n",
    "    return model\n",
    "\n",
    "best_model = get_best_model(dropout = best_dropout, layer_num = best_layer_num, \n",
    "                            init_mode = best_init_mode, batch_size = best_batch_size, \n",
    "                            regularizer = best_regularizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liked film', 'film terrible', 'enjoyed watching', 'worst experience ever', 'want see', 'really enjoyed', 'amazing', 'good']\n",
      "0.79799\n",
      "0.18767\n",
      "0.71185\n",
      "0.17779\n",
      "0.67965\n",
      "0.76598\n",
      "0.82159\n",
      "0.55706\n"
     ]
    }
   ],
   "source": [
    "test_samples = [\"I liked this film\",\n",
    "               \"the film was terrible...\",\n",
    "               \"I enjoyed watching it!\",\n",
    "               \"It was the worst experience I've ever had!\",\n",
    "               \"I want to see it again!\",\n",
    "               \"I really enjoyed all of it\",\n",
    "               \"It was amazing!\",\n",
    "               \"it was good\"]\n",
    "\n",
    "test_samples = [text_preprocessing(i) for i in test_samples]\n",
    "\n",
    "print(test_samples)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_samples)\n",
    "test_data = pad_sequences(test_sequences, maxlen = maxlen)\n",
    "\n",
    "for i in best_model.predict(x = test_data):\n",
    "    print(\"{:.5f}\".format(float(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel.compile(optimizer='rmsprop',\\n              loss='binary_crossentropy',\\n              metrics=['acc'])\\nhistory = model.fit(train_data, y_train,\\n                    epochs=10,\\n                    batch_size=128,\\n                    callbacks=callbacks_list,\\n                    validation_data=(val_data, y_val),\\n                    verbose = 2)\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(train_data, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=(val_data, y_val),\n",
    "                    verbose = 2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel.compile(optimizer='rmsprop',\\n              loss='binary_crossentropy',\\n              metrics=['acc'])\\nhistory = model.fit(train_data, y_train,\\n                    epochs=10,\\n                    batch_size=128,\\n                    callbacks=callbacks_list,\\n                    validation_data=(val_data, y_val),\\n                    verbose = 2)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(train_data, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=(val_data, y_val),\n",
    "                    verbose = 2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport matplotlib.pyplot as plt\\n\\nacc = history.history['acc']\\nval_acc = history.history['val_acc']\\nloss = history.history['loss']\\nval_loss = history.history['val_loss']\\n\\nepochs = range(1, len(acc) + 1)\\n\\nplt.plot(epochs, acc, 'bo', label='Training acc')\\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\\nplt.title('Training and validation accuracy')\\nplt.legend()\\n\\nplt.figure()\\n\\nplt.plot(epochs, loss, 'bo', label='Training loss')\\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\\nplt.title('Training and validation loss')\\nplt.legend()\\n\\nplt.show()\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model.load_weights(\\'my_model.h5\\')\\n\\ntest_samples = [\"I liked this film\",\\n               \"the film was terrible...\",\\n               \"I enjoyed watching it!\",\\n               \"It was the worst experience I\\'ve ever had!\",\\n               \"I want to see it again!\",\\n               \"I really enjoyed all of it\",\\n               \"It was amazing!\",\\n               \"it was good\"]\\n\\ntest_samples = [text_preprocessing(i) for i in test_samples]\\n\\nprint(test_samples)\\n\\ntest_sequences = tokenizer.texts_to_sequences(test_samples)\\ntest_data = pad_sequences(test_sequences, maxlen = maxlen)\\n\\nfor i in model.predict(x = test_data):\\n    print(\"{:.5f}\".format(float(i)))\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model.load_weights('my_model.h5')\n",
    "\n",
    "test_samples = [\"I liked this film\",\n",
    "               \"the film was terrible...\",\n",
    "               \"I enjoyed watching it!\",\n",
    "               \"It was the worst experience I've ever had!\",\n",
    "               \"I want to see it again!\",\n",
    "               \"I really enjoyed all of it\",\n",
    "               \"It was amazing!\",\n",
    "               \"it was good\"]\n",
    "\n",
    "test_samples = [text_preprocessing(i) for i in test_samples]\n",
    "\n",
    "print(test_samples)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_samples)\n",
    "test_data = pad_sequences(test_sequences, maxlen = maxlen)\n",
    "\n",
    "for i in model.predict(x = test_data):\n",
    "    print(\"{:.5f}\".format(float(i)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation\n",
    "\n",
    "**Risulta essere troppo dispendiosa per quanto riguarda i tempi di computazione**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras import regularizers\n",
    "from keras import layers\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n%time\\nimport keras, numpy as np\\n\\ncallbacks_list = [\\n    keras.callbacks.EarlyStopping(\\n        monitor='acc',\\n        patience=5\\n    ),\\n    keras.callbacks.ModelCheckpoint(\\n        filepath='my_model.h5',\\n        monitor='val_loss',\\n        save_best_only=True\\n    ),\\n        \\n    keras.callbacks.ReduceLROnPlateau(\\n        monitor='val_loss',\\n        factor=0.1,\\n        patience=5,\\n    )\\n]\\n\\nbatch_size = 128\\nepochs = 3\\n\\nmodel_CV = KerasClassifier(build_fn=create_model, epochs=epochs, \\n                           batch_size=batch_size, shuffle = True, verbose=1) #callbacks = callbacks_list)\\n# define the grid search parameters\\ninit_mode = ['uniform', 'lecun_uniform', 'normal', \\n             'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\\n\\nparam_grid = dict(init_mode=init_mode)\\ngrid = GridSearchCV(estimator=model_CV, param_grid=param_grid, cv=3)\\ngrid_result = grid.fit(train_data, y_train)\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "%time\n",
    "import keras, numpy as np\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='acc',\n",
    "        patience=5\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='my_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    ),\n",
    "        \n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.1,\n",
    "        patience=5,\n",
    "    )\n",
    "]\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 3\n",
    "\n",
    "model_CV = KerasClassifier(build_fn=create_model, epochs=epochs, \n",
    "                           batch_size=batch_size, shuffle = True, verbose=1) #callbacks = callbacks_list)\n",
    "# define the grid search parameters\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', \n",
    "             'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "\n",
    "param_grid = dict(init_mode=init_mode)\n",
    "grid = GridSearchCV(estimator=model_CV, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(train_data, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np\\nseed = 7\\nnp.random.seed(seed)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import numpy as np\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# let's create a function that creates the model (required for KerasClassifier) \\n# while accepting the hyperparameters we want to tune \\n# we also pass some default values such as optimizer='rmsprop'\\ndef create_model(init_mode='uniform', dropout = 0.5, layer_num = 1, regularizer = 0.01):\\n\\n    def add_layers():\\n        for i in range (0, layer_num):\\n            model.add(Dense(64, kernel_initializer=init_mode, activation='relu',\\n                           kernel_regularizer = regularizers.l2(regularizer)))\\n            model.add(layers.Dropout(dropout))\\n    \\n    EMBEDDING_DIM = 100\\n    \\n    model = Sequential()\\n    model.add(Embedding(words_size, EMBEDDING_DIM, input_length=maxlen))\\n    model.add(Flatten())\\n    add_layers()\\n    model.add(Dense(1, kernel_initializer=init_mode, activation='sigmoid', \\n                    kernel_regularizer = regularizers.l2(regularizer)))\\n    \\n    # compile model\\n    model.compile(optimizer='rmsprop',\\n              loss='binary_crossentropy',\\n              metrics=['acc'])\\n    return model\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# let's create a function that creates the model (required for KerasClassifier) \n",
    "# while accepting the hyperparameters we want to tune \n",
    "# we also pass some default values such as optimizer='rmsprop'\n",
    "def create_model(init_mode='uniform', dropout = 0.5, layer_num = 1, regularizer = 0.01):\n",
    "\n",
    "    def add_layers():\n",
    "        for i in range (0, layer_num):\n",
    "            model.add(Dense(64, kernel_initializer=init_mode, activation='relu',\n",
    "                           kernel_regularizer = regularizers.l2(regularizer)))\n",
    "            model.add(layers.Dropout(dropout))\n",
    "    \n",
    "    EMBEDDING_DIM = 100\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(words_size, EMBEDDING_DIM, input_length=maxlen))\n",
    "    model.add(Flatten())\n",
    "    add_layers()\n",
    "    model.add(Dense(1, kernel_initializer=init_mode, activation='sigmoid', \n",
    "                    kernel_regularizer = regularizers.l2(regularizer)))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "    return model\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nhyperparameters = dict(dropout = [0.2, 0.5, 0.65, 0.8],\\n                       layer_num = [1,2,3],\\n                       regularizer = [0.1, 0.01, 0.001],\\n                       batch_size =[128,512],\\n                       init_mode = ['uniform', 'lecun_uniform', 'normal', \\n                                    'glorot_normal', 'glorot_uniform']\\n                      )\\n\\ngrid_results = {}\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "hyperparameters = dict(dropout = [0.2, 0.5, 0.65, 0.8],\n",
    "                       layer_num = [1,2,3],\n",
    "                       regularizer = [0.1, 0.01, 0.001],\n",
    "                       batch_size =[128,512],\n",
    "                       init_mode = ['uniform', 'lecun_uniform', 'normal', \n",
    "                                    'glorot_normal', 'glorot_uniform']\n",
    "                      )\n",
    "\n",
    "grid_results = {}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef get_fold_3_validate_result (dropout = [0.5], layer_num = [1], regularizer = [0.01], \\n                                batch_size = [128], init_mode = ['uniform']):\\n    epochs = 10\\n\\n    model_CV = KerasClassifier(build_fn=create_model, epochs=epochs,\\n                               shuffle = True, verbose=2)\\n    # Define the grid search parameters\\n    param_grid = dict(dropout = dropout,\\n                     layer_num = layer_num,\\n                     batch_size = batch_size,\\n                     init_mode = init_mode,\\n                     regularizer = regularizer)\\n\\n    grid = GridSearchCV(estimator=model_CV, param_grid=param_grid, cv=3)\\n    return grid.fit(train_data, y_train)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def get_fold_3_validate_result (dropout = [0.5], layer_num = [1], regularizer = [0.01], \n",
    "                                batch_size = [128], init_mode = ['uniform']):\n",
    "    epochs = 10\n",
    "\n",
    "    model_CV = KerasClassifier(build_fn=create_model, epochs=epochs,\n",
    "                               shuffle = True, verbose=2)\n",
    "    # Define the grid search parameters\n",
    "    param_grid = dict(dropout = dropout,\n",
    "                     layer_num = layer_num,\n",
    "                     batch_size = batch_size,\n",
    "                     init_mode = init_mode,\n",
    "                     regularizer = regularizer)\n",
    "\n",
    "    grid = GridSearchCV(estimator=model_CV, param_grid=param_grid, cv=3)\n",
    "    return grid.fit(train_data, y_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint('Tuning dropout')\\ngrid_results['dropout'] = get_fold_3_validate_result (dropout = hyperparameters['dropout'])\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print('Tuning dropout')\n",
    "grid_results['dropout'] = get_fold_3_validate_result (dropout = hyperparameters['dropout'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# print results\\ngrid_dropout = grid_results['dropout']\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# print results\n",
    "grid_dropout = grid_results['dropout']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(f\\' Best Accuracy for {grid_dropout.best_score_} using dropout: { grid_dropout.best_params_[\"dropout\"]}\\')\\nmeans = grid_dropout.cv_results_[\\'loss_test_score\\']\\nstds = grid_dropout.cv_results_[\\'std_test_score\\']\\nparams = grid_dropout.cv_results_[\\'params\\']\\nfor mean, stdev, param in zip(means, stds, params):\\n    print(f\\' mean={mean:.4}, std={stdev:.4} using {param}\\')\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(f' Best Accuracy for {grid_dropout.best_score_} using dropout: { grid_dropout.best_params_[\"dropout\"]}')\n",
    "means = grid_dropout.cv_results_['loss_test_score']\n",
    "stds = grid_dropout.cv_results_['std_test_score']\n",
    "params = grid_dropout.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f' mean={mean:.4}, std={stdev:.4} using {param}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
