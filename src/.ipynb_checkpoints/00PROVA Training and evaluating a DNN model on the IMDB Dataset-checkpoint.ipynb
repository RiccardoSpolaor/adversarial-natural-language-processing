{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "\n",
    "SEED = 123\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 Training and evaluating a DNN model on the IMDB Dataset\n",
    "## Downloading and data preprocessing\n",
    "\n",
    "Downloaded the dataset at http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "```\n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  month     = {June},\n",
    "  year      = {2011},\n",
    "  address   = {Portland, Oregon, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {142--150},\n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns = ['text','sentiment'])\n",
    "\n",
    "imdb_dir = \"./datasets/aclImdb\"\n",
    "\n",
    "for dir_kind in ['train','test']:\n",
    "    for label_type in ['neg', 'pos']:\n",
    "        dir_name = os.path.join(imdb_dir, dir_kind, label_type)\n",
    "        for fname in os.listdir(dir_name):\n",
    "            if fname[-4:] == '.txt':\n",
    "                f = open(os.path.join(dir_name, fname), encoding = \"utf8\")\n",
    "                df = df.append({'text': f.read(), 'sentiment': ['neg','pos'].index(label_type)}, ignore_index = True)\n",
    "                f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Story of a man who has unnatural feelings for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Airport '77 starts as a brand new luxury 747 p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This film lacked something I couldn't put my f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sorry everyone,,, I know this is supposed to b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When I was little my parents took me along to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  Story of a man who has unnatural feelings for ...         0\n",
       "1  Airport '77 starts as a brand new luxury 747 p...         0\n",
       "2  This film lacked something I couldn't put my f...         0\n",
       "3  Sorry everyone,,, I know this is supposed to b...         0\n",
       "4  When I was little my parents took me along to ...         0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative istances: 25000\n",
      "Number of positive istances: 25000\n",
      "Il dataset risulta essere bilanciato!\n"
     ]
    }
   ],
   "source": [
    "print ('Number of negative istances:', len(df[df['sentiment'] == 0]))\n",
    "print ('Number of positive istances:', len(df[df['sentiment'] == 1]))\n",
    "print ('Il dataset risulta essere bilanciato!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\n"
     ]
    }
   ],
   "source": [
    "print(df['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.preprocessing import Preprocesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Text Example:\n",
      "stori man unnatur feel pig start open scene terrif exampl absurd comedi formal orchestra audienc turn insan violent mob crazi chant singer unfortun stay absurd whole time gener narr eventu make put even era turn cryptic dialogu would make shakespear seem easi third grader technic level better might think good cinematographi futur great vilmo zsigmond futur star salli kirkland freder forrest see briefli\n"
     ]
    }
   ],
   "source": [
    "print('Preprocessed Text Example:')\n",
    "print(Preprocesser.text_preprocessing(df['text'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing Train and Test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size = 0.33, shuffle = True)\n",
    "\n",
    "x_train = list(x_train)\n",
    "x_test = list(x_test)\n",
    "\n",
    "y_train = list(y_train)\n",
    "y_test = list(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_preprocessed = [preprocesser.text_preprocessing(sentence) for sentence in x_train]\n",
    "x_test_preprocessed = [preprocesser.text_preprocessing(sentence) for sentence in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed texts\n",
      "['get stupid excus child play rip man think first mess rumpelstiltskin horror movi make crap like fariy tale hater well honest see kid scar bite lot simpli age assumpt pinocchio wah wah wah grow come think child play rip fairi tale bash nonsens lame tale crypt episod tri one least lame end stupid mani plot hole still understand come life work evil geppetto evil deed becom real boy becom america want person think concept evil geppetto sound better build armi wooden killer start crime wave funni aw aw aw aw aw aw stinki like shoe aw suck suck want killer puppet settl killer doll specif child play instead string attach want fairi tale figur turn upsid watch leprechaun want pinocchio watch anim disnet version live version jonathan taylor thoma martin landau instead neg', 'delight movi tell stori bud incred laugh smile laugh realli laugh jon bon jovi funni movi heck movi nuff say go watch', 'movi lot up down storylin strong tell saga barker grow misadventur boy fbi theresa russel talent beauti even shine barker public enemi direct mark l lester good commando still interest eric robert play short live part secur guard turn thug lover alyssa milano play prostitut hang gang frank stallon play thug help gang one exploit get one boy troubl get final way speak perplex intrigu captiv throughout movi make wonder movi other vote low watch wonder umm fbi actual bad begin tommi gun like outlaw thu quit disadvantag whenev get shootout gang era sinc everyth saw repres felt realist mani movi make portray era may way like train wreck happen want watch enjoy']\n",
      "['unhing follow typic plot earli slasher trend pretti young girl peril give filmmak use helicopt earli road trip shoot actual think second go qualiti product watch unhing like see amateur act class go warm awkward badli light overlong scene play gusto valium overdos wonder put cue card camera actress constantli shift gaze two main girl obvious choos factor rather talent laurel munson main chick terri excit watch paint dri two nude scene make adolesc thrill janet penner virginia settl crazi creepi daughter mother chick find strand compet worst act ever long paus weird express emphasi wrong word delight u love bad film scene shift suddenli long black out could drive mack truck cartoon lightn crash across shoot without even bother show sky eighti eyeshadow assault viewer ya know grow felt sorri want hug kiss boo boo make better end make damag caus grin anyway theori regard whole ban hype hope anyon choos view film substanti substanc abus sens humor otherwis pa', 'wow saw movi day within hour differ theater saw mr bug first total disappoint follow beauti touch film movi u nowaday irk melodramat act dialog anim melodrama groan humor wonder soft organ line draw music put nice comfort mood enjoy show littl charact ladybug grasshopp bee snail stinkbug fli mosquito beetl cricket cute littl overbear idiosyncrasi interact human world nemesi cigar smoker high heel wearer innoc kick play kid kind heart unknown destroy realist fascin care bug dick mari protagonist hoppiti perfect superman come set thing right starri eye optimist lead everyon garden path liter everi time think go end happili style along come anoth roadblock edg seat much walk movi theater grin chuckl someth happen long long long long time', 'accept horror scari today standard hell see titl like expect see blood blood thirsti beast instead get blood beast either want take world live peac earth yeah peopl want overal stori fine astronaut come back life one beast titl realli kill movi night beast would make fan happi realli blood speak like movi end leav room sequel wise never make one movi worst ever see almost']\n"
     ]
    }
   ],
   "source": [
    "print('Preprocessed texts')\n",
    "print(x_train_preprocessed[:3])\n",
    "print(x_test_preprocessed[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['text'] = df['text'].apply(lambda x: preprocesser.text_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-db5d3628e76e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pickle\\\\data.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "os.makedirs('pickle', exist_ok=True)\n",
    "\n",
    "with open('pickle\\\\data.pickle', 'wb') as f:\n",
    "    pickle.dump([x_test, y_test], f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.utils import shuffle\\n\\nx_train, x_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size = 0.33, shuffle = True)\\n\\nx_train = list(x_train)\\nx_test = list(x_test)\\n\\ny_train = list(y_train)\\ny_test = list(y_test)\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size = 0.33, shuffle = True)\n",
    "\n",
    "x_train = list(x_train)\n",
    "x_test = list(x_test)\n",
    "\n",
    "y_train = list(y_train)\n",
    "y_test = list(y_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 58721 unique tokens.\n",
      "Shape of train data tensor: (33500, 1154)\n",
      "Shape of train label tensor: (33500,)\n",
      "Shape of test data tensor: (16500, 1154)\n",
      "Shape of test label tensor: (16500,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train_preprocessed)\n",
    "\n",
    "maxlen = max([len(t.split()) for t in x_train_preprocessed])\n",
    "\n",
    "words_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train_preprocessed)\n",
    "test_sequences = tokenizer.texts_to_sequences(x_test_preprocessed)\n",
    "\n",
    "print('Found %s unique tokens.' % len(tokenizer.word_index))\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen = maxlen)\n",
    "test_data = pad_sequences(test_sequences, maxlen = maxlen)\n",
    "\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n",
    "print('Shape of train data tensor:', train_data.shape)\n",
    "print('Shape of train label tensor:', y_train.shape)\n",
    "\n",
    "print('Shape of test data tensor:', test_data.shape)\n",
    "print('Shape of test label tensor:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "os.makedirs('pickle', exist_ok=True)\n",
    "\n",
    "with open('pickle\\\\tokenizer.pickle', 'wb') as f:\n",
    "    pickle.dump([tokenizer, maxlen], f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  328,  778, 1057],\n",
       "       [   0,    0,    0, ...,  109,  109,    8],\n",
       "       [   0,    0,    0, ...,   54,    6,  136],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,   52,   23,   22],\n",
       "       [   0,    0,    0, ...,    7,    6,  105],\n",
       "       [   0,    0,    0, ..., 5761,  291,  235]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%store test_data\\n%store x_test\\n%store y_test\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%store test_data\n",
    "%store x_test\n",
    "%store y_test\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=3\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.1,\n",
    "        patience=3,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fitted_model(dropout = 0.5, layer_num = 1, init_mode='uniform', batch_size = 128):\n",
    "    \n",
    "    print('\\n', f'Training Model with:', '\\n',\n",
    "    f'* dropout = {dropout};', '\\n',\n",
    "    f'* number of hidden layers = {layer_num};', '\\n',\n",
    "    f'* init mode = {init_mode};', '\\n',\n",
    "    f'* batch size = {batch_size}')\n",
    "    \n",
    "    def add_layers():\n",
    "        for i in range (0, layer_num):\n",
    "            model.add(Dense(64, kernel_initializer=init_mode, activation='relu'))\n",
    "            model.add(Dropout(rate=dropout))\n",
    "    \n",
    "    EMBEDDING_DIM = 100\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(words_size, EMBEDDING_DIM, input_length=maxlen))\n",
    "    model.add(Flatten())\n",
    "    add_layers()\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "    history = model.fit(train_data, y_train,\n",
    "                        epochs=10,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_split=0.2,\n",
    "                        verbose = 2)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict(dropout = [0.2, 0.5, 0.65, 0.8],\n",
    "                       layer_num = [1,2,3],\n",
    "                       batch_size =[128,512],\n",
    "                       init_mode = ['uniform', 'lecun_uniform', 'normal', \n",
    "                                    'glorot_normal', 'glorot_uniform']\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Model with: \n",
      " * dropout = 0.2; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = uniform; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 12s - loss: 0.6166 - acc: 0.7269 - val_loss: 0.2927 - val_acc: 0.8761\n",
      "Epoch 2/10\n",
      "26800/26800 - 12s - loss: 0.1948 - acc: 0.9253 - val_loss: 0.3495 - val_acc: 0.8634\n",
      "Epoch 3/10\n",
      "26800/26800 - 12s - loss: 0.0679 - acc: 0.9775 - val_loss: 0.3658 - val_acc: 0.8787\n",
      "Epoch 4/10\n",
      "26800/26800 - 12s - loss: 0.0155 - acc: 0.9961 - val_loss: 0.4651 - val_acc: 0.8806\n",
      "Epoch 5/10\n",
      "26800/26800 - 12s - loss: 0.0015 - acc: 0.9997 - val_loss: 0.4912 - val_acc: 0.8828\n",
      "Epoch 6/10\n",
      "26800/26800 - 12s - loss: 0.0011 - acc: 0.9997 - val_loss: 0.5133 - val_acc: 0.8830\n",
      "Epoch 7/10\n",
      "26800/26800 - 12s - loss: 6.9732e-04 - acc: 0.9999 - val_loss: 0.5366 - val_acc: 0.8824\n",
      "Epoch 8/10\n",
      "26800/26800 - 12s - loss: 5.9996e-04 - acc: 0.9999 - val_loss: 0.5360 - val_acc: 0.8827\n",
      "Epoch 9/10\n",
      "26800/26800 - 12s - loss: 5.4129e-04 - acc: 0.9999 - val_loss: 0.5390 - val_acc: 0.8827\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.5; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = uniform; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 12s - loss: 0.8310 - acc: 0.5019 - val_loss: 0.6931 - val_acc: 0.5224\n",
      "Epoch 2/10\n",
      "26800/26800 - 12s - loss: 0.5123 - acc: 0.7057 - val_loss: 0.3050 - val_acc: 0.8706\n",
      "Epoch 3/10\n",
      "26800/26800 - 11s - loss: 0.2032 - acc: 0.9220 - val_loss: 0.2729 - val_acc: 0.8869\n",
      "Epoch 4/10\n",
      "26800/26800 - 12s - loss: 0.0761 - acc: 0.9753 - val_loss: 0.3695 - val_acc: 0.8770\n",
      "Epoch 5/10\n",
      "26800/26800 - 12s - loss: 0.0183 - acc: 0.9953 - val_loss: 0.5441 - val_acc: 0.8569\n",
      "Epoch 6/10\n",
      "26800/26800 - 12s - loss: 0.0059 - acc: 0.9984 - val_loss: 0.5576 - val_acc: 0.8833\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.65; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = uniform; \n",
      " * batch size = 128\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "26800/26800 - 12s - loss: 0.7292 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5079\n",
      "Epoch 2/10\n",
      "26800/26800 - 12s - loss: 0.6908 - acc: 0.5197 - val_loss: 0.6027 - val_acc: 0.7491\n",
      "Epoch 3/10\n",
      "26800/26800 - 12s - loss: 0.3462 - acc: 0.8591 - val_loss: 0.2782 - val_acc: 0.8855\n",
      "Epoch 4/10\n",
      "26800/26800 - 12s - loss: 0.1659 - acc: 0.9397 - val_loss: 0.3431 - val_acc: 0.8631\n",
      "Epoch 5/10\n",
      "26800/26800 - 12s - loss: 0.0670 - acc: 0.9791 - val_loss: 0.3484 - val_acc: 0.8839\n",
      "Epoch 6/10\n",
      "26800/26800 - 12s - loss: 0.0202 - acc: 0.9945 - val_loss: 0.4304 - val_acc: 0.8807\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = uniform; \n",
      " * batch size = 128\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "26800/26800 - 12s - loss: 0.7368 - acc: 0.4954 - val_loss: 0.6931 - val_acc: 0.4933\n",
      "Epoch 2/10\n",
      "26800/26800 - 11s - loss: 0.6971 - acc: 0.5013 - val_loss: 0.6940 - val_acc: 0.4930\n",
      "Epoch 3/10\n",
      "26800/26800 - 11s - loss: 0.6123 - acc: 0.6307 - val_loss: 0.3420 - val_acc: 0.8542\n",
      "Epoch 4/10\n",
      "26800/26800 - 12s - loss: 0.2930 - acc: 0.8843 - val_loss: 0.3047 - val_acc: 0.8733\n",
      "Epoch 5/10\n",
      "26800/26800 - 12s - loss: 0.1799 - acc: 0.9353 - val_loss: 0.2771 - val_acc: 0.8885\n",
      "Epoch 6/10\n",
      "26800/26800 - 11s - loss: 0.0994 - acc: 0.9668 - val_loss: 0.3457 - val_acc: 0.8885\n",
      "Epoch 7/10\n",
      "26800/26800 - 11s - loss: 0.0451 - acc: 0.9869 - val_loss: 0.4779 - val_acc: 0.8821\n",
      "Epoch 8/10\n",
      "26800/26800 - 11s - loss: 0.0195 - acc: 0.9939 - val_loss: 0.5814 - val_acc: 0.8793\n"
     ]
    }
   ],
   "source": [
    "dict_dropout_histories = {}\n",
    "best_dropout = 0.5\n",
    "best_dropout_acc = 0\n",
    "for i in hyperparameters['dropout']:\n",
    "    history = get_fitted_model(dropout = i)\n",
    "    if max(history.history['val_acc']) > best_dropout_acc:\n",
    "        best_dropout = i\n",
    "        best_dropout_acc = max(history.history['val_acc'])\n",
    "    dict_dropout_histories[str(i)] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8885075\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "print(max(dict_dropout_histories[str(best_dropout)].history['val_acc']))\n",
    "print(best_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = uniform; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 12s - loss: 0.7812 - acc: 0.4993 - val_loss: 0.6931 - val_acc: 0.5072\n",
      "Epoch 2/10\n",
      "26800/26800 - 12s - loss: 0.6885 - acc: 0.5291 - val_loss: 0.5984 - val_acc: 0.6433\n",
      "Epoch 3/10\n",
      "26800/26800 - 11s - loss: 0.3695 - acc: 0.8456 - val_loss: 0.3151 - val_acc: 0.8646\n",
      "Epoch 4/10\n",
      "26800/26800 - 11s - loss: 0.2027 - acc: 0.9263 - val_loss: 0.2842 - val_acc: 0.8906\n",
      "Epoch 5/10\n",
      "26800/26800 - 11s - loss: 0.1155 - acc: 0.9604 - val_loss: 0.2972 - val_acc: 0.8896\n",
      "Epoch 6/10\n",
      "26800/26800 - 11s - loss: 0.0540 - acc: 0.9840 - val_loss: 0.3999 - val_acc: 0.8875\n",
      "Epoch 7/10\n",
      "26800/26800 - 11s - loss: 0.0239 - acc: 0.9928 - val_loss: 0.5089 - val_acc: 0.8863\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 2; \n",
      " * init mode = uniform; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 12s - loss: 0.7111 - acc: 0.4993 - val_loss: 0.6931 - val_acc: 0.5067\n",
      "Epoch 2/10\n",
      "26800/26800 - 11s - loss: 0.6949 - acc: 0.4974 - val_loss: 0.6935 - val_acc: 0.4933\n",
      "Epoch 3/10\n",
      "26800/26800 - 12s - loss: 0.6948 - acc: 0.5124 - val_loss: 0.6882 - val_acc: 0.6007\n",
      "Epoch 4/10\n",
      "26800/26800 - 11s - loss: 0.5385 - acc: 0.7340 - val_loss: 0.3146 - val_acc: 0.8709\n",
      "Epoch 5/10\n",
      "26800/26800 - 11s - loss: 0.3029 - acc: 0.8911 - val_loss: 0.2881 - val_acc: 0.8876\n",
      "Epoch 6/10\n",
      "26800/26800 - 12s - loss: 0.2148 - acc: 0.9294 - val_loss: 0.3510 - val_acc: 0.8899\n",
      "Epoch 7/10\n",
      "26800/26800 - 11s - loss: 0.1454 - acc: 0.9535 - val_loss: 0.4479 - val_acc: 0.8869\n",
      "Epoch 8/10\n",
      "26800/26800 - 11s - loss: 0.0969 - acc: 0.9710 - val_loss: 0.5112 - val_acc: 0.8864\n",
      "Epoch 9/10\n",
      "26800/26800 - 11s - loss: 0.0465 - acc: 0.9897 - val_loss: 0.5659 - val_acc: 0.8875\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 3; \n",
      " * init mode = uniform; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 12s - loss: 0.6957 - acc: 0.4934 - val_loss: 0.6931 - val_acc: 0.5067\n",
      "Epoch 2/10\n",
      "26800/26800 - 12s - loss: 0.6933 - acc: 0.4943 - val_loss: 0.6932 - val_acc: 0.4933\n",
      "Epoch 3/10\n",
      "26800/26800 - 11s - loss: 0.6934 - acc: 0.5051 - val_loss: 0.6932 - val_acc: 0.4934\n",
      "Epoch 4/10\n",
      "26800/26800 - 12s - loss: 0.6943 - acc: 0.5038 - val_loss: 0.6932 - val_acc: 0.4933\n"
     ]
    }
   ],
   "source": [
    "dict_layers_num_histories = {}\n",
    "best_layer_num = 1\n",
    "best_layer_num_acc = 0\n",
    "for i in hyperparameters['layer_num']:\n",
    "    history = get_fitted_model(dropout = best_dropout, layer_num = i)\n",
    "    if max(history.history['val_acc']) > best_layer_num_acc:\n",
    "        best_layer_num = i\n",
    "        best_layer_num_acc = max(history.history['val_acc'])\n",
    "    dict_layers_num_histories[str(i)] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.890597\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(max(dict_layers_num_histories[str(best_layer_num)].history['val_acc']))\n",
    "print(best_layer_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = uniform; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 12s - loss: 0.7865 - acc: 0.4981 - val_loss: 0.6931 - val_acc: 0.5069\n",
      "Epoch 2/10\n",
      "26800/26800 - 12s - loss: 0.6951 - acc: 0.4975 - val_loss: 0.6914 - val_acc: 0.5400\n",
      "Epoch 3/10\n",
      "26800/26800 - 11s - loss: 0.5516 - acc: 0.6881 - val_loss: 0.3167 - val_acc: 0.8675\n",
      "Epoch 4/10\n",
      "26800/26800 - 11s - loss: 0.2675 - acc: 0.8966 - val_loss: 0.2887 - val_acc: 0.8830\n",
      "Epoch 5/10\n",
      "26800/26800 - 12s - loss: 0.1623 - acc: 0.9426 - val_loss: 0.2927 - val_acc: 0.8815\n",
      "Epoch 6/10\n",
      "26800/26800 - 11s - loss: 0.0863 - acc: 0.9723 - val_loss: 0.3908 - val_acc: 0.8864\n",
      "Epoch 7/10\n",
      "26800/26800 - 11s - loss: 0.0412 - acc: 0.9875 - val_loss: 0.4310 - val_acc: 0.8863\n",
      "Epoch 8/10\n",
      "26800/26800 - 11s - loss: 0.0136 - acc: 0.9968 - val_loss: 0.5195 - val_acc: 0.8848\n",
      "Epoch 9/10\n",
      "26800/26800 - 11s - loss: 0.0109 - acc: 0.9976 - val_loss: 0.5408 - val_acc: 0.8842\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = lecun_uniform; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 12s - loss: 0.8147 - acc: 0.5024 - val_loss: 0.7046 - val_acc: 0.4933\n",
      "Epoch 2/10\n",
      "26800/26800 - 12s - loss: 0.4141 - acc: 0.8129 - val_loss: 0.3235 - val_acc: 0.8636\n",
      "Epoch 3/10\n",
      "26800/26800 - 11s - loss: 0.2214 - acc: 0.9175 - val_loss: 0.2684 - val_acc: 0.8916\n",
      "Epoch 4/10\n",
      "26800/26800 - 12s - loss: 0.1406 - acc: 0.9510 - val_loss: 0.3176 - val_acc: 0.8907\n",
      "Epoch 5/10\n",
      "26800/26800 - 11s - loss: 0.0757 - acc: 0.9757 - val_loss: 0.3592 - val_acc: 0.8839\n",
      "Epoch 6/10\n",
      "26800/26800 - 12s - loss: 0.0336 - acc: 0.9905 - val_loss: 0.4715 - val_acc: 0.8872\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = normal; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 14s - loss: 0.7489 - acc: 0.4988 - val_loss: 0.6931 - val_acc: 0.5069\n",
      "Epoch 2/10\n",
      "26800/26800 - 13s - loss: 0.6944 - acc: 0.4989 - val_loss: 0.6932 - val_acc: 0.4936\n",
      "Epoch 3/10\n",
      "26800/26800 - 12s - loss: 0.6969 - acc: 0.5101 - val_loss: 0.6906 - val_acc: 0.5100\n",
      "Epoch 4/10\n",
      "26800/26800 - 12s - loss: 0.4961 - acc: 0.7479 - val_loss: 0.3085 - val_acc: 0.8727\n",
      "Epoch 5/10\n",
      "26800/26800 - 12s - loss: 0.2546 - acc: 0.9013 - val_loss: 0.2735 - val_acc: 0.8857\n",
      "Epoch 6/10\n",
      "26800/26800 - 12s - loss: 0.1630 - acc: 0.9420 - val_loss: 0.2890 - val_acc: 0.8901\n",
      "Epoch 7/10\n",
      "26800/26800 - 12s - loss: 0.0921 - acc: 0.9699 - val_loss: 0.3588 - val_acc: 0.8851\n",
      "Epoch 8/10\n",
      "26800/26800 - 12s - loss: 0.0456 - acc: 0.9855 - val_loss: 0.3666 - val_acc: 0.8863\n",
      "Epoch 9/10\n",
      "26800/26800 - 12s - loss: 0.0155 - acc: 0.9958 - val_loss: 0.4772 - val_acc: 0.8881\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = glorot_normal; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 13s - loss: 0.7722 - acc: 0.5011 - val_loss: 0.6928 - val_acc: 0.5139\n",
      "Epoch 2/10\n",
      "26800/26800 - 12s - loss: 0.4841 - acc: 0.7503 - val_loss: 0.3208 - val_acc: 0.8648\n",
      "Epoch 3/10\n",
      "26800/26800 - 12s - loss: 0.2393 - acc: 0.9090 - val_loss: 0.2805 - val_acc: 0.8885\n",
      "Epoch 4/10\n",
      "26800/26800 - 12s - loss: 0.1509 - acc: 0.9471 - val_loss: 0.3264 - val_acc: 0.8907\n",
      "Epoch 5/10\n",
      "26800/26800 - 12s - loss: 0.0820 - acc: 0.9744 - val_loss: 0.3502 - val_acc: 0.8894\n",
      "Epoch 6/10\n",
      "26800/26800 - 12s - loss: 0.0359 - acc: 0.9897 - val_loss: 0.5117 - val_acc: 0.8875\n",
      "Epoch 7/10\n",
      "26800/26800 - 13s - loss: 0.0119 - acc: 0.9972 - val_loss: 0.5484 - val_acc: 0.8875\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = glorot_uniform; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 12s - loss: 0.7369 - acc: 0.4961 - val_loss: 0.6931 - val_acc: 0.5067\n",
      "Epoch 2/10\n",
      "26800/26800 - 12s - loss: 0.6779 - acc: 0.5430 - val_loss: 0.4868 - val_acc: 0.8179\n",
      "Epoch 3/10\n",
      "26800/26800 - 12s - loss: 0.3304 - acc: 0.8694 - val_loss: 0.3122 - val_acc: 0.8672\n",
      "Epoch 4/10\n",
      "26800/26800 - 12s - loss: 0.1936 - acc: 0.9294 - val_loss: 0.2789 - val_acc: 0.8933\n",
      "Epoch 5/10\n",
      "26800/26800 - 12s - loss: 0.1138 - acc: 0.9622 - val_loss: 0.3299 - val_acc: 0.8904\n",
      "Epoch 6/10\n",
      "26800/26800 - 12s - loss: 0.0527 - acc: 0.9839 - val_loss: 0.4525 - val_acc: 0.8912\n",
      "Epoch 7/10\n",
      "26800/26800 - 13s - loss: 0.0251 - acc: 0.9928 - val_loss: 0.5437 - val_acc: 0.8891\n"
     ]
    }
   ],
   "source": [
    "dict_init_mode_histories = {}\n",
    "best_init_mode = 'uniform'\n",
    "best_init_mode_acc = 0\n",
    "for i in hyperparameters['init_mode']:\n",
    "    history = get_fitted_model(dropout = best_dropout, layer_num = best_layer_num, init_mode = i)\n",
    "    if max(history.history['val_acc']) > best_init_mode_acc:\n",
    "        best_init_mode = i\n",
    "        best_init_mode_acc = max(history.history['val_acc'])\n",
    "    dict_init_mode_histories[str(i)] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8932836\n",
      "glorot_uniform\n"
     ]
    }
   ],
   "source": [
    "print(max(dict_init_mode_histories[str(best_init_mode)].history['val_acc']))\n",
    "print(best_init_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = glorot_uniform; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 13s - loss: 0.7380 - acc: 0.4971 - val_loss: 0.6932 - val_acc: 0.4933\n",
      "Epoch 2/10\n",
      "26800/26800 - 12s - loss: 0.7005 - acc: 0.4963 - val_loss: 0.6930 - val_acc: 0.5067\n",
      "Epoch 3/10\n",
      "26800/26800 - 12s - loss: 0.5916 - acc: 0.6375 - val_loss: 0.3022 - val_acc: 0.8749\n",
      "Epoch 4/10\n",
      "26800/26800 - 12s - loss: 0.2844 - acc: 0.8910 - val_loss: 0.2779 - val_acc: 0.8860\n",
      "Epoch 5/10\n",
      "26800/26800 - 12s - loss: 0.1833 - acc: 0.9368 - val_loss: 0.2819 - val_acc: 0.8910\n",
      "Epoch 6/10\n",
      "26800/26800 - 12s - loss: 0.1055 - acc: 0.9648 - val_loss: 0.3475 - val_acc: 0.8858\n",
      "Epoch 7/10\n",
      "26800/26800 - 12s - loss: 0.0508 - acc: 0.9831 - val_loss: 0.4577 - val_acc: 0.8860\n",
      "Epoch 8/10\n",
      "26800/26800 - 12s - loss: 0.0199 - acc: 0.9955 - val_loss: 0.4974 - val_acc: 0.8866\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = glorot_uniform; \n",
      " * batch size = 512\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 9s - loss: 0.9722 - acc: 0.4975 - val_loss: 0.6931 - val_acc: 0.4934\n",
      "Epoch 2/10\n",
      "26800/26800 - 8s - loss: 0.7024 - acc: 0.4992 - val_loss: 0.6931 - val_acc: 0.5067\n",
      "Epoch 3/10\n",
      "26800/26800 - 8s - loss: 0.6963 - acc: 0.5007 - val_loss: 0.6932 - val_acc: 0.4933\n",
      "Epoch 4/10\n",
      "26800/26800 - 8s - loss: 0.6927 - acc: 0.5039 - val_loss: 0.6889 - val_acc: 0.5251\n",
      "Epoch 5/10\n",
      "26800/26800 - 8s - loss: 0.5703 - acc: 0.6697 - val_loss: 0.3267 - val_acc: 0.8673\n",
      "Epoch 6/10\n",
      "26800/26800 - 8s - loss: 0.2867 - acc: 0.8921 - val_loss: 0.2718 - val_acc: 0.8909\n",
      "Epoch 7/10\n",
      "26800/26800 - 9s - loss: 0.2019 - acc: 0.9307 - val_loss: 0.2673 - val_acc: 0.8937\n",
      "Epoch 8/10\n",
      "26800/26800 - 8s - loss: 0.1350 - acc: 0.9565 - val_loss: 0.3467 - val_acc: 0.8743\n",
      "Epoch 9/10\n",
      "26800/26800 - 8s - loss: 0.0873 - acc: 0.9742 - val_loss: 0.3248 - val_acc: 0.8918\n",
      "Epoch 10/10\n",
      "26800/26800 - 8s - loss: 0.0528 - acc: 0.9856 - val_loss: 0.3751 - val_acc: 0.8882\n"
     ]
    }
   ],
   "source": [
    "dict_batch_size_histories = {}\n",
    "best_batch_size = 128\n",
    "best_batch_size_acc = 0\n",
    "for i in hyperparameters['batch_size']:\n",
    "    history = get_fitted_model(dropout = best_dropout, layer_num = best_layer_num, \n",
    "                              init_mode = best_init_mode, batch_size = i)\n",
    "    if max(history.history['val_acc']) > best_batch_size_acc:\n",
    "        best_batch_size = i\n",
    "        best_batch_size_acc = max(history.history['val_acc'])\n",
    "    dict_batch_size_histories[str(i)] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89373136\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(max(dict_batch_size_histories[str(best_batch_size)].history['val_acc']))\n",
    "print(best_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "callbacks_list.append(\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath= 'models\\\\best_model.h5',\n",
    "        save_weights_only=False,\n",
    "        monitor='val_acc',\n",
    "        save_best_only=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 9s - loss: 0.9870 - acc: 0.4988 - val_loss: 0.6932 - val_acc: 0.4921\n",
      "Epoch 2/10\n",
      "26800/26800 - 9s - loss: 0.7009 - acc: 0.5015 - val_loss: 0.6932 - val_acc: 0.4933\n",
      "Epoch 3/10\n",
      "26800/26800 - 9s - loss: 0.6932 - acc: 0.4989 - val_loss: 0.6931 - val_acc: 0.4933\n",
      "Epoch 4/10\n",
      "26800/26800 - 9s - loss: 0.6960 - acc: 0.5013 - val_loss: 0.6909 - val_acc: 0.5213\n",
      "Epoch 5/10\n",
      "26800/26800 - 8s - loss: 0.5688 - acc: 0.6825 - val_loss: 0.3364 - val_acc: 0.8681\n",
      "Epoch 6/10\n",
      "26800/26800 - 8s - loss: 0.2852 - acc: 0.8929 - val_loss: 0.2681 - val_acc: 0.8922\n",
      "Epoch 7/10\n",
      "26800/26800 - 8s - loss: 0.1983 - acc: 0.9306 - val_loss: 0.2722 - val_acc: 0.8894\n",
      "Epoch 8/10\n",
      "26800/26800 - 8s - loss: 0.1353 - acc: 0.9560 - val_loss: 0.4181 - val_acc: 0.8594\n",
      "Epoch 9/10\n",
      "26800/26800 - 8s - loss: 0.0876 - acc: 0.9726 - val_loss: 0.3204 - val_acc: 0.8878\n"
     ]
    }
   ],
   "source": [
    "def get_best_model(dropout = 0.5, layer_num = 1, init_mode='uniform', batch_size = 128):\n",
    "\n",
    "    def add_layers():\n",
    "        for i in range (0, layer_num):\n",
    "            model.add(Dense(64, kernel_initializer=init_mode, activation='relu'))\n",
    "            model.add(Dropout(rate=dropout))\n",
    "    \n",
    "    EMBEDDING_DIM = 100\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(words_size, EMBEDDING_DIM, input_length=maxlen))\n",
    "    model.add(Flatten())\n",
    "    add_layers()\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(words_size, EMBEDDING_DIM, input_length=maxlen))\n",
    "    model.add(Flatten())\n",
    "    add_layers()\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "    history = model.fit(train_data, y_train,\n",
    "                        epochs=10,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_split=0.2,\n",
    "                        verbose=2)\n",
    "    #model.load_weights('./models/best_model.h5')\n",
    "    \n",
    "    #return model\n",
    "    return tf.keras.models.load_model(\"models\\\\best_model.h5\" )\n",
    "\n",
    "best_model = get_best_model(dropout = best_dropout, layer_num = best_layer_num, \n",
    "                            init_mode = best_init_mode, batch_size = best_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500/16500 [==============================] - 2s 144us/sample - loss: 0.2679 - acc: 0.8918\n",
      "accuracy: 0.89175755%\n"
     ]
    }
   ],
   "source": [
    "#Testing the accuracy of the model\n",
    "\n",
    "test_result = best_model.evaluate(test_data, y_test)\n",
    "\n",
    "print ('accuracy: ' + str(test_result[1]) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16500, 1154)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tf.keras.models.load_model(\"models\\\\best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-6ec283fe2414>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    928\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    931\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m   def predict(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, model, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m         use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m   def predict(self, model, x, batch_size=None, verbose=0, steps=None,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    634\u001b[0m                     use_multiprocessing=False):\n\u001b[0;32m    635\u001b[0m   \u001b[1;34m\"\"\"Process the inputs for fit/eval/predict().\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 636\u001b[1;33m   \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    637\u001b[0m   standardize = functools.partial(\n\u001b[0;32m    638\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    996\u001b[0m         \u001b[1;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    997\u001b[0m         \"input: {}, {}\".format(\n\u001b[1;32m--> 998\u001b[1;33m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[0;32m    999\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1000\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})"
     ]
    }
   ],
   "source": [
    "best_model.evaluate(test_data, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the black box algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('scripts', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/blackBox.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/blackBox.py\n",
    "\n",
    "import tensorflow as tf\n",
    "from scripts.preprocessing import Preprocesser\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "class BlackBox:\n",
    "    \n",
    "    def __init__(self):\n",
    "        with open('pickle\\\\tokenizer.pickle', 'rb') as f:\n",
    "            tokenizer, maxlen = pickle.load(f)\n",
    "            self.__tokenizer = tokenizer\n",
    "            self.__maxlen = maxlen\n",
    "        f.close()\n",
    "        self.__model = tf.keras.models.load_model(\"models\\\\best_model.h5\")\n",
    "        \n",
    "    def __text_preprocessing(self, text):\n",
    "        return Preprocesser.text_preprocessing(text)      \n",
    "        \n",
    "    def __tokenize(self, text):\n",
    "        sequences = self.__tokenizer.texts_to_sequences(text)\n",
    "        return pad_sequences(sequences, maxlen = self.__maxlen)\n",
    "        \n",
    "    def predict_sentiment(self, text):\n",
    "        text = self.__text_preprocessing(text)\n",
    "        seq = self.__tokenize([text])\n",
    "        return self.__model.predict(seq).take(0)\n",
    "    \n",
    "    def evaluate(self, test, label):\n",
    "        self.__model.evaluate(test,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.blackBox import BlackBox\n",
    "\n",
    "#import scripts.blackBox as blackbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "black_box = BlackBox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('pickle\\\\data.pickle', 'rb') as f:\n",
    "    x_test, y_test = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#black_box.evaluate(test_data, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[y_test[7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8256238"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "black_box.predict_sentiment(x_test[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
