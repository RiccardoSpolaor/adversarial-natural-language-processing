{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Creating a LSTM DNN Model for IMDB Sentiment Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the seeds for deterministic results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.seed_setter import set_seed\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating The Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os\n",
    "\n",
    "with open(os.path.join('./pickle_data/glove_utils/embeddings_dictionary.pickle'), 'rb') as f:\n",
    "    embeddings_dict = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "with open(os.path.join('./pickle_data/train_test_data/train_data.pickle'), 'rb') as f:\n",
    "    x_train, y_train = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.black_box_preprocessing import BlackBoxPreprocesser\n",
    "\n",
    "black_box_preprocesser = BlackBoxPreprocesser()\n",
    "x_train = [black_box_preprocesser.preprocess_text(text) for text in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average review length:\n",
      "271.5328358208955\n"
     ]
    }
   ],
   "source": [
    "print('Average review length:')\n",
    "print( sum([len(t.split()) for t in x_train])/len(x_train) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the train dataset:\n",
      "88117\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique words in the train dataset:')\n",
    "print( len(set(w for t in x_train for w in t.split())) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scelta parametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS_SIZE = 50_000\n",
    "MAXLEN = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88087 unique tokens.\n",
      "Shape of train data tensor: (33500, 500)\n",
      "Shape of train label tensor: (33500,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(WORDS_SIZE)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "print('Found %s unique tokens.' % len(tokenizer.word_index))\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen = MAXLEN)\n",
    "\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "print('Shape of train data tensor:', train_data.shape)\n",
    "print('Shape of train label tensor:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "os.makedirs(os.path.join('./pickle_data/preprocesser_utils'), exist_ok=True)\n",
    "\n",
    "with open(os.path.join('./pickle_data/preprocesser_utils/tokenizer.pickle'), 'wb') as f:\n",
    "    pickle.dump([tokenizer, MAXLEN], f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo la mia matrice per ogni parola del mio dizionario e metto la riga della matrice a tutti 0 se non\n",
    "# esiste una certa parola\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "embedding_matrix = np.zeros((WORDS_SIZE + 1, EMBEDDING_DIM))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < WORDS_SIZE + 1:\n",
    "        embedding_vector = embeddings_dict.get(word)\n",
    "        # Words not found in the embedding index will be all zeros.\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout, LSTM, Bidirectional, GlobalMaxPool1D\n",
    "#from tensorflow.keras import regularizers\n",
    "#from tensorflow.keras import layers\n",
    "#import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"callbacks_list = [\\n    keras.callbacks.EarlyStopping(\\n        monitor='val_acc',\\n        patience=2\\n    ),\\n    keras.callbacks.ReduceLROnPlateau(\\n        monitor='val_loss',\\n        factor=0.1,\\n        patience=2,\\n    ),\\n    keras.callbacks.ModelCheckpoint(\\n        filepath= os.path.join('./models/best_model_redone_2.h5'),\\n        save_weights_only=False,\\n        monitor='val_acc',\\n        save_best_only=True\\n    )\\n]\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=2\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.1,\n",
    "        patience=2,\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath= os.path.join('./models/best_model_redone_2.h5'),\n",
    "        save_weights_only=False,\n",
    "        monitor='val_acc',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def get_fitted_model():\\n\\n    model = Sequential()\\n    model.add(Embedding(WORDS_SIZE+1,\\n                        EMBEDDING_DIM,\\n                        weights=[embedding_matrix],\\n                        trainable=False,\\n                        input_length = MAXLEN))\\n    model.add(Bidirectional(LSTM(100, return_sequences = True)))\\n    model.add(GlobalMaxPool1D())\\n    model.add(Dense(64, activation='relu'))\\n    model.add(Dropout(rate=0.2))\\n    model.add(Dense(1, activation='sigmoid'))\\n    \\n    model.compile(optimizer='adam',\\n              loss='binary_crossentropy',\\n              metrics=['acc'])\\n    \\n    history = model.fit(train_data, y_train,\\n                        epochs=15,\\n                        batch_size=128,\\n                        callbacks=callbacks_list,\\n                        validation_split=0.2,\\n                        verbose = 2)\\n    return history\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def get_fitted_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(WORDS_SIZE+1,\n",
    "                        EMBEDDING_DIM,\n",
    "                        weights=[embedding_matrix],\n",
    "                        trainable=False,\n",
    "                        input_length = MAXLEN))\n",
    "    model.add(Bidirectional(LSTM(100, return_sequences = True)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "    \n",
    "    history = model.fit(train_data, y_train,\n",
    "                        epochs=15,\n",
    "                        batch_size=128,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_split=0.2,\n",
    "                        verbose = 2)\n",
    "    return history'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = get_fitted_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow\n",
    "\n",
    "#best_model = tensorflow.keras.models.load_model(os.path.join('./models/best_model_redone_2.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle, os\n",
    "\n",
    "#with open(os.path.join('./pickle_data/train_test_data/test_data.pickle'), 'rb') as f:\n",
    "#    x_test, y_test = pickle.load(f)\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test = [black_box_preprocesser.preprocess_text(text) for text in x_test]\n",
    "\n",
    "#test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "#test_data = pad_sequences(test_sequences, maxlen = MAXLEN)\n",
    "\n",
    "#y_test = np.asarray(y_test)\n",
    "\n",
    "#print('Shape of test data tensor:', test_data.shape)\n",
    "#print('Shape of test label tensor:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model.evaluate(test_data, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM DNN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, LSTM, Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=3\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.1,\n",
    "        patience=3,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fitted_model(optimizer='rmsprop', dropout = 0.1, init_mode='uniform'):\n",
    "\n",
    "    print('\\n', f'Training Model with:', '\\n',\n",
    "          f'* optimizer = {optimizer};', '\\n',\n",
    "          f'* dropout = {dropout};', '\\n',\n",
    "          f'* init mode = {init_mode};', '\\n')\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(WORDS_SIZE+1,\n",
    "                        EMBEDDING_DIM,\n",
    "                        weights=[embedding_matrix],\n",
    "                        trainable=False,\n",
    "                        input_length = MAXLEN))\n",
    "    model.add(Bidirectional(LSTM(100, return_sequences = True)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dropout(rate=dropout))\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer=init_mode))\n",
    "    model.add(Dropout(rate=dropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "    \n",
    "    history = model.fit(train_data, y_train,\n",
    "                        epochs=10,\n",
    "                        batch_size=128,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_split=0.2,\n",
    "                        verbose = 2)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparameter:\n",
    "    OPTIMIZER = 'optimizer'\n",
    "    DROPOUT = 'dropout'\n",
    "    INIT_MODE = 'init_mode'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_dict = {\n",
    "    'optimizer': ['rmsprop', 'adam'],\n",
    "    'dropout': [0.1, 0.2, 0.5],\n",
    "    'init_mode': ['uniform', 'lecun_uniform', 'normal', 'glorot_normal', 'glorot_uniform']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_result_dict = {key: None for key in hyperparameters_dict.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameter(hyperparameter):\n",
    "    for i in hyperparameters_dict[hyperparameter]:\n",
    "        history = get_fitted_model(\n",
    "            **{k : v['Value'] if k != hyperparameter else i for k,v in tuning_result_dict.items() if v is not None}\n",
    "        )\n",
    "        if tuning_result_dict[hyperparameter] is None or max(history.history['val_acc']) > tuning_result_dict[hyperparameter]['Accuracy']:\n",
    "            tuning_result_dict[hyperparameter] = {}\n",
    "            tuning_result_dict[hyperparameter]['Accuracy'] = max(history.history['val_acc'])\n",
    "            tuning_result_dict[hyperparameter]['Value'] = i\n",
    "            \n",
    "def print_tuning_result(hyperparameter):\n",
    "    print('Best {}: {}, Accuracy: {}'.format(\n",
    "        hyperparameter, \n",
    "        tuning_result_dict[hyperparameter]['Value'], \n",
    "        tuning_result_dict[hyperparameter]['Accuracy']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Model with: \n",
      " * optimizer = rmsprop; \n",
      " * dropout = 0.1; \n",
      " * init mode = uniform; \n",
      "\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 21s - loss: 0.4376 - acc: 0.7870 - val_loss: 0.3087 - val_acc: 0.8722\n",
      "Epoch 2/10\n",
      "26800/26800 - 17s - loss: 0.3049 - acc: 0.8712 - val_loss: 0.3827 - val_acc: 0.8322\n",
      "Epoch 3/10\n",
      "26800/26800 - 17s - loss: 0.2551 - acc: 0.8949 - val_loss: 0.3456 - val_acc: 0.8563\n",
      "Epoch 4/10\n",
      "26800/26800 - 17s - loss: 0.2193 - acc: 0.9118 - val_loss: 0.2365 - val_acc: 0.9042\n",
      "Epoch 5/10\n",
      "26800/26800 - 17s - loss: 0.1860 - acc: 0.9263 - val_loss: 0.2299 - val_acc: 0.9091\n",
      "Epoch 6/10\n",
      "26800/26800 - 17s - loss: 0.1602 - acc: 0.9397 - val_loss: 0.2652 - val_acc: 0.8973\n",
      "Epoch 7/10\n",
      "26800/26800 - 17s - loss: 0.1332 - acc: 0.9499 - val_loss: 0.2449 - val_acc: 0.9036\n",
      "Epoch 8/10\n",
      "26800/26800 - 17s - loss: 0.1127 - acc: 0.9586 - val_loss: 0.3432 - val_acc: 0.8782\n",
      "\n",
      " Training Model with: \n",
      " * optimizer = adam; \n",
      " * dropout = 0.1; \n",
      " * init mode = uniform; \n",
      "\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 20s - loss: 0.4208 - acc: 0.7953 - val_loss: 0.2914 - val_acc: 0.8807\n",
      "Epoch 2/10\n",
      "26800/26800 - 17s - loss: 0.2687 - acc: 0.8890 - val_loss: 0.2427 - val_acc: 0.8993\n",
      "Epoch 3/10\n",
      "26800/26800 - 18s - loss: 0.2230 - acc: 0.9102 - val_loss: 0.2633 - val_acc: 0.8876\n",
      "Epoch 4/10\n",
      "26800/26800 - 17s - loss: 0.1900 - acc: 0.9249 - val_loss: 0.2522 - val_acc: 0.8957\n",
      "Epoch 5/10\n",
      "26800/26800 - 17s - loss: 0.1503 - acc: 0.9432 - val_loss: 0.2415 - val_acc: 0.9025\n",
      "Epoch 6/10\n",
      "26800/26800 - 17s - loss: 0.1191 - acc: 0.9572 - val_loss: 0.2281 - val_acc: 0.9140\n",
      "Epoch 7/10\n",
      "26800/26800 - 18s - loss: 0.0899 - acc: 0.9707 - val_loss: 0.2604 - val_acc: 0.9064\n",
      "Epoch 8/10\n",
      "26800/26800 - 17s - loss: 0.0676 - acc: 0.9787 - val_loss: 0.2531 - val_acc: 0.9099\n",
      "Epoch 9/10\n",
      "26800/26800 - 17s - loss: 0.0513 - acc: 0.9841 - val_loss: 0.2642 - val_acc: 0.9112\n"
     ]
    }
   ],
   "source": [
    "tune_hyperparameter(Hyperparameter.OPTIMIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best optimizer: adam, Accuracy: 0.9140298366546631\n"
     ]
    }
   ],
   "source": [
    "print_tuning_result(Hyperparameter.OPTIMIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Model with: \n",
      " * optimizer = adam; \n",
      " * dropout = 0.1; \n",
      " * init mode = uniform; \n",
      "\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 20s - loss: 0.4224 - acc: 0.7971 - val_loss: 0.2962 - val_acc: 0.8775\n",
      "Epoch 2/10\n",
      "26800/26800 - 17s - loss: 0.2749 - acc: 0.8853 - val_loss: 0.2569 - val_acc: 0.8937\n",
      "Epoch 3/10\n",
      "26800/26800 - 17s - loss: 0.2282 - acc: 0.9088 - val_loss: 0.2507 - val_acc: 0.8936\n",
      "Epoch 4/10\n",
      "26800/26800 - 17s - loss: 0.1960 - acc: 0.9225 - val_loss: 0.2521 - val_acc: 0.8948\n",
      "Epoch 5/10\n",
      "26800/26800 - 17s - loss: 0.1586 - acc: 0.9401 - val_loss: 0.2242 - val_acc: 0.9101\n",
      "Epoch 6/10\n",
      "26800/26800 - 17s - loss: 0.1292 - acc: 0.9534 - val_loss: 0.2397 - val_acc: 0.9073\n",
      "Epoch 7/10\n",
      "26800/26800 - 17s - loss: 0.0975 - acc: 0.9671 - val_loss: 0.2328 - val_acc: 0.9091\n",
      "Epoch 8/10\n",
      "26800/26800 - 18s - loss: 0.0721 - acc: 0.9773 - val_loss: 0.2754 - val_acc: 0.9063\n",
      "\n",
      " Training Model with: \n",
      " * optimizer = adam; \n",
      " * dropout = 0.2; \n",
      " * init mode = uniform; \n",
      "\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 20s - loss: 0.4357 - acc: 0.7857 - val_loss: 0.2899 - val_acc: 0.8788\n",
      "Epoch 2/10\n",
      "26800/26800 - 17s - loss: 0.2784 - acc: 0.8835 - val_loss: 0.2499 - val_acc: 0.8973\n",
      "Epoch 3/10\n",
      "26800/26800 - 17s - loss: 0.2439 - acc: 0.8993 - val_loss: 0.2437 - val_acc: 0.8994\n",
      "Epoch 4/10\n",
      "26800/26800 - 17s - loss: 0.2103 - acc: 0.9165 - val_loss: 0.2297 - val_acc: 0.9051\n",
      "Epoch 5/10\n",
      "26800/26800 - 17s - loss: 0.1795 - acc: 0.9293 - val_loss: 0.2200 - val_acc: 0.9085\n",
      "Epoch 6/10\n",
      "26800/26800 - 17s - loss: 0.1504 - acc: 0.9438 - val_loss: 0.2190 - val_acc: 0.9140\n",
      "Epoch 7/10\n",
      "26800/26800 - 17s - loss: 0.1251 - acc: 0.9547 - val_loss: 0.2214 - val_acc: 0.9172\n",
      "Epoch 8/10\n",
      "26800/26800 - 17s - loss: 0.1011 - acc: 0.9640 - val_loss: 0.2389 - val_acc: 0.9149\n",
      "Epoch 9/10\n",
      "26800/26800 - 17s - loss: 0.0827 - acc: 0.9719 - val_loss: 0.2453 - val_acc: 0.9151\n",
      "Epoch 10/10\n",
      "26800/26800 - 17s - loss: 0.0532 - acc: 0.9851 - val_loss: 0.2527 - val_acc: 0.9151\n",
      "\n",
      " Training Model with: \n",
      " * optimizer = adam; \n",
      " * dropout = 0.5; \n",
      " * init mode = uniform; \n",
      "\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 19s - loss: 0.4864 - acc: 0.7506 - val_loss: 0.3296 - val_acc: 0.8645\n",
      "Epoch 2/10\n",
      "26800/26800 - 17s - loss: 0.3201 - acc: 0.8664 - val_loss: 0.2728 - val_acc: 0.8843\n",
      "Epoch 3/10\n",
      "26800/26800 - 17s - loss: 0.2850 - acc: 0.8835 - val_loss: 0.2884 - val_acc: 0.8819\n",
      "Epoch 4/10\n",
      "26800/26800 - 18s - loss: 0.2615 - acc: 0.8941 - val_loss: 0.2454 - val_acc: 0.8972\n",
      "Epoch 5/10\n",
      "26800/26800 - 17s - loss: 0.2304 - acc: 0.9093 - val_loss: 0.2384 - val_acc: 0.9025\n",
      "Epoch 6/10\n",
      "26800/26800 - 17s - loss: 0.2177 - acc: 0.9144 - val_loss: 0.2296 - val_acc: 0.9097\n",
      "Epoch 7/10\n",
      "26800/26800 - 17s - loss: 0.1980 - acc: 0.9239 - val_loss: 0.2235 - val_acc: 0.9113\n",
      "Epoch 8/10\n",
      "26800/26800 - 17s - loss: 0.1782 - acc: 0.9328 - val_loss: 0.2234 - val_acc: 0.9119\n",
      "Epoch 9/10\n",
      "26800/26800 - 17s - loss: 0.1605 - acc: 0.9406 - val_loss: 0.2400 - val_acc: 0.9113\n",
      "Epoch 10/10\n",
      "26800/26800 - 17s - loss: 0.1466 - acc: 0.9440 - val_loss: 0.2506 - val_acc: 0.9003\n"
     ]
    }
   ],
   "source": [
    "tune_hyperparameter(Hyperparameter.DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best dropout: 0.2, Accuracy: 0.9171642065048218\n"
     ]
    }
   ],
   "source": [
    "print_tuning_result(Hyperparameter.DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Model with: \n",
      " * optimizer = adam; \n",
      " * dropout = 0.2; \n",
      " * init mode = uniform; \n",
      "\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 20s - loss: 0.4365 - acc: 0.7884 - val_loss: 0.3015 - val_acc: 0.8752\n",
      "Epoch 2/10\n",
      "26800/26800 - 17s - loss: 0.2874 - acc: 0.8809 - val_loss: 0.2542 - val_acc: 0.8952\n",
      "Epoch 3/10\n",
      "26800/26800 - 17s - loss: 0.2440 - acc: 0.9016 - val_loss: 0.2885 - val_acc: 0.8800\n",
      "Epoch 4/10\n",
      "26800/26800 - 17s - loss: 0.2188 - acc: 0.9122 - val_loss: 0.2263 - val_acc: 0.9078\n",
      "Epoch 5/10\n",
      "26800/26800 - 17s - loss: 0.1822 - acc: 0.9305 - val_loss: 0.2279 - val_acc: 0.9063\n",
      "Epoch 6/10\n",
      "26800/26800 - 17s - loss: 0.1566 - acc: 0.9415 - val_loss: 0.2428 - val_acc: 0.9018\n",
      "Epoch 7/10\n",
      "26800/26800 - 17s - loss: 0.1341 - acc: 0.9511 - val_loss: 0.2446 - val_acc: 0.9037\n",
      "\n",
      " Training Model with: \n",
      " * optimizer = adam; \n",
      " * dropout = 0.2; \n",
      " * init mode = lecun_uniform; \n",
      "\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 20s - loss: 0.4289 - acc: 0.7897 - val_loss: 0.3243 - val_acc: 0.8563\n",
      "Epoch 2/10\n",
      "26800/26800 - 17s - loss: 0.2883 - acc: 0.8814 - val_loss: 0.2551 - val_acc: 0.8958\n",
      "Epoch 3/10\n",
      "26800/26800 - 17s - loss: 0.2471 - acc: 0.9001 - val_loss: 0.2749 - val_acc: 0.8870\n",
      "Epoch 4/10\n",
      "26800/26800 - 17s - loss: 0.2185 - acc: 0.9133 - val_loss: 0.2384 - val_acc: 0.9013\n",
      "Epoch 5/10\n",
      "26800/26800 - 17s - loss: 0.1832 - acc: 0.9296 - val_loss: 0.2301 - val_acc: 0.9042\n",
      "Epoch 6/10\n",
      "26800/26800 - 17s - loss: 0.1549 - acc: 0.9404 - val_loss: 0.2276 - val_acc: 0.9091\n",
      "Epoch 7/10\n",
      "26800/26800 - 17s - loss: 0.1329 - acc: 0.9495 - val_loss: 0.2470 - val_acc: 0.9075\n",
      "Epoch 8/10\n",
      "26800/26800 - 17s - loss: 0.1081 - acc: 0.9597 - val_loss: 0.2353 - val_acc: 0.9137\n",
      "Epoch 9/10\n",
      "26800/26800 - 17s - loss: 0.0872 - acc: 0.9698 - val_loss: 0.2497 - val_acc: 0.9136\n",
      "Epoch 10/10\n",
      "26800/26800 - 17s - loss: 0.0564 - acc: 0.9829 - val_loss: 0.2555 - val_acc: 0.9131\n",
      "\n",
      " Training Model with: \n",
      " * optimizer = adam; \n",
      " * dropout = 0.2; \n",
      " * init mode = normal; \n",
      "\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 20s - loss: 0.4349 - acc: 0.7830 - val_loss: 0.2894 - val_acc: 0.8800\n",
      "Epoch 2/10\n",
      "26800/26800 - 17s - loss: 0.2821 - acc: 0.8849 - val_loss: 0.2517 - val_acc: 0.8940\n",
      "Epoch 3/10\n",
      "26800/26800 - 17s - loss: 0.2419 - acc: 0.9017 - val_loss: 0.2501 - val_acc: 0.8967\n",
      "Epoch 4/10\n",
      "26800/26800 - 17s - loss: 0.2157 - acc: 0.9131 - val_loss: 0.2321 - val_acc: 0.9055\n",
      "Epoch 5/10\n",
      "26800/26800 - 17s - loss: 0.1774 - acc: 0.9314 - val_loss: 0.2208 - val_acc: 0.9101\n",
      "Epoch 6/10\n",
      "26800/26800 - 17s - loss: 0.1514 - acc: 0.9432 - val_loss: 0.2208 - val_acc: 0.9133\n",
      "Epoch 7/10\n",
      "26800/26800 - 17s - loss: 0.1303 - acc: 0.9518 - val_loss: 0.2273 - val_acc: 0.9149\n",
      "Epoch 8/10\n",
      "26800/26800 - 17s - loss: 0.1010 - acc: 0.9638 - val_loss: 0.2471 - val_acc: 0.9119\n",
      "Epoch 9/10\n",
      "26800/26800 - 17s - loss: 0.0647 - acc: 0.9810 - val_loss: 0.2362 - val_acc: 0.9142\n",
      "Epoch 10/10\n",
      "26800/26800 - 17s - loss: 0.0579 - acc: 0.9835 - val_loss: 0.2417 - val_acc: 0.9157\n",
      "\n",
      " Training Model with: \n",
      " * optimizer = adam; \n",
      " * dropout = 0.2; \n",
      " * init mode = glorot_normal; \n",
      "\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 20s - loss: 0.4155 - acc: 0.8032 - val_loss: 0.3198 - val_acc: 0.8607\n",
      "Epoch 2/10\n",
      "26800/26800 - 18s - loss: 0.2843 - acc: 0.8805 - val_loss: 0.2580 - val_acc: 0.8946\n",
      "Epoch 3/10\n",
      "26800/26800 - 17s - loss: 0.2483 - acc: 0.8967 - val_loss: 0.2468 - val_acc: 0.9009\n",
      "Epoch 4/10\n",
      "26800/26800 - 17s - loss: 0.2195 - acc: 0.9129 - val_loss: 0.2280 - val_acc: 0.9097\n",
      "Epoch 5/10\n",
      "26800/26800 - 17s - loss: 0.1821 - acc: 0.9292 - val_loss: 0.2246 - val_acc: 0.9121\n",
      "Epoch 6/10\n",
      "26800/26800 - 18s - loss: 0.1608 - acc: 0.9395 - val_loss: 0.2224 - val_acc: 0.9163\n",
      "Epoch 7/10\n",
      "26800/26800 - 18s - loss: 0.1305 - acc: 0.9531 - val_loss: 0.2306 - val_acc: 0.9148\n",
      "Epoch 8/10\n",
      "26800/26800 - 17s - loss: 0.1100 - acc: 0.9607 - val_loss: 0.2294 - val_acc: 0.9157\n",
      "Epoch 9/10\n",
      "26800/26800 - 17s - loss: 0.0895 - acc: 0.9704 - val_loss: 0.2475 - val_acc: 0.9139\n",
      "\n",
      " Training Model with: \n",
      " * optimizer = adam; \n",
      " * dropout = 0.2; \n",
      " * init mode = glorot_uniform; \n",
      "\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 20s - loss: 0.4393 - acc: 0.7865 - val_loss: 0.3031 - val_acc: 0.8718\n",
      "Epoch 2/10\n",
      "26800/26800 - 17s - loss: 0.2881 - acc: 0.8820 - val_loss: 0.2574 - val_acc: 0.8915\n",
      "Epoch 3/10\n",
      "26800/26800 - 17s - loss: 0.2467 - acc: 0.9003 - val_loss: 0.2596 - val_acc: 0.8921\n",
      "Epoch 4/10\n",
      "26800/26800 - 17s - loss: 0.2151 - acc: 0.9134 - val_loss: 0.2289 - val_acc: 0.9045\n",
      "Epoch 5/10\n",
      "26800/26800 - 17s - loss: 0.1807 - acc: 0.9308 - val_loss: 0.2237 - val_acc: 0.9100\n",
      "Epoch 6/10\n",
      "26800/26800 - 17s - loss: 0.1532 - acc: 0.9439 - val_loss: 0.2247 - val_acc: 0.9121\n",
      "Epoch 7/10\n",
      "26800/26800 - 17s - loss: 0.1330 - acc: 0.9510 - val_loss: 0.2272 - val_acc: 0.9076\n",
      "Epoch 8/10\n",
      "26800/26800 - 17s - loss: 0.1068 - acc: 0.9631 - val_loss: 0.2301 - val_acc: 0.9134\n",
      "Epoch 9/10\n",
      "26800/26800 - 17s - loss: 0.0686 - acc: 0.9783 - val_loss: 0.2394 - val_acc: 0.9121\n",
      "Epoch 10/10\n",
      "26800/26800 - 17s - loss: 0.0624 - acc: 0.9811 - val_loss: 0.2553 - val_acc: 0.9103\n"
     ]
    }
   ],
   "source": [
    "tune_hyperparameter(Hyperparameter.INIT_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best init_mode: glorot_normal, Accuracy: 0.9162686467170715\n"
     ]
    }
   ],
   "source": [
    "print_tuning_result(Hyperparameter.INIT_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(os.path.join('./models/imdb'), exist_ok=True)\n",
    "\n",
    "callbacks_list.append(\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath= os.path.join('./models/imdb/best_model.h5'),\n",
    "        save_weights_only=False,\n",
    "        monitor='val_acc',\n",
    "        save_best_only=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 20s - loss: 0.4219 - acc: 0.7965 - val_loss: 0.2945 - val_acc: 0.8787\n",
      "Epoch 2/10\n",
      "26800/26800 - 17s - loss: 0.2860 - acc: 0.8800 - val_loss: 0.2624 - val_acc: 0.8894\n",
      "Epoch 3/10\n",
      "26800/26800 - 17s - loss: 0.2485 - acc: 0.9000 - val_loss: 0.2413 - val_acc: 0.9024\n",
      "Epoch 4/10\n",
      "26800/26800 - 17s - loss: 0.2182 - acc: 0.9138 - val_loss: 0.2309 - val_acc: 0.9025\n",
      "Epoch 5/10\n",
      "26800/26800 - 18s - loss: 0.1835 - acc: 0.9284 - val_loss: 0.2226 - val_acc: 0.9100\n",
      "Epoch 6/10\n",
      "26800/26800 - 18s - loss: 0.1555 - acc: 0.9407 - val_loss: 0.2238 - val_acc: 0.9103\n",
      "Epoch 7/10\n",
      "26800/26800 - 17s - loss: 0.1289 - acc: 0.9520 - val_loss: 0.2259 - val_acc: 0.9103\n",
      "Epoch 8/10\n",
      "26800/26800 - 17s - loss: 0.1064 - acc: 0.9621 - val_loss: 0.2254 - val_acc: 0.9166\n",
      "Epoch 9/10\n",
      "26800/26800 - 17s - loss: 0.0702 - acc: 0.9780 - val_loss: 0.2319 - val_acc: 0.9163\n",
      "Epoch 10/10\n",
      "26800/26800 - 17s - loss: 0.0626 - acc: 0.9810 - val_loss: 0.2377 - val_acc: 0.9154\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_best_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(WORDS_SIZE+1,\n",
    "                        EMBEDDING_DIM,\n",
    "                        weights=[embedding_matrix],\n",
    "                        trainable=False,\n",
    "                        input_length = MAXLEN))\n",
    "    model.add(Bidirectional(LSTM(100, return_sequences = True)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dropout(rate=tuning_result_dict['dropout']['Value']))\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer=tuning_result_dict['init_mode']['Value']))\n",
    "    model.add(Dropout(rate=tuning_result_dict['dropout']['Value']))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=tuning_result_dict['optimizer']['Value'],\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "    \n",
    "    history = model.fit(train_data, y_train,\n",
    "                        epochs=10,\n",
    "                        batch_size=128,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_split=0.2,\n",
    "                        verbose = 2)\n",
    "    return tf.keras.models.load_model(os.path.join('./models/imdb/best_model.h5'))\n",
    "\n",
    "best_model = get_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os\n",
    "\n",
    "with open(os.path.join('./pickle_data/train_test_data/test_data.pickle'), 'rb') as f:\n",
    "    x_test, y_test = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test data tensor: (16500, 500)\n",
      "Shape of test label tensor: (16500,)\n"
     ]
    }
   ],
   "source": [
    "x_test = [black_box_preprocesser.preprocess_text(text) for text in x_test]\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen = MAXLEN)\n",
    "\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "print('Shape of test data tensor:', test_data.shape)\n",
    "print('Shape of test label tensor:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500/16500 [==============================] - 41s 3ms/sample - loss: 0.2292 - acc: 0.9176\n",
      "accuracy: 0.9175758%\n"
     ]
    }
   ],
   "source": [
    "#Testing the accuracy of the model\n",
    "\n",
    "test_result = best_model.evaluate(test_data, y_test)\n",
    "\n",
    "print ('accuracy: ' + str(test_result[1]) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tf.keras.models.load_model(os.path.join('./models/imdb/best_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500/16500 [==============================] - 41s 2ms/sample - loss: 0.2292 - acc: 0.9176\n"
     ]
    }
   ],
   "source": [
    "test_result = best_model.evaluate(test_data, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the black box algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join('./utils'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./utils/black_box.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./utils/black_box.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from utils.black_box_preprocessing import BlackBoxPreprocesser\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "class BlackBox(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        with open(os.path.join('./pickle_data/preprocesser_utils/tokenizer.pickle'), 'rb') as f:\n",
    "            tokenizer, MAXLEN = pickle.load(f)\n",
    "            self.__tokenizer = tokenizer\n",
    "            self.__MAXLEN = MAXLEN\n",
    "        f.close()\n",
    "        self.__preprocesser = BlackBoxPreprocesser()\n",
    "        self.__model = tf.keras.models.load_model(os.path.join('./models/imdb/best_model.h5'))\n",
    "        \n",
    "    def __text_preprocessing(self, text):\n",
    "        return self.__preprocesser.preprocess_text(text)      \n",
    "        \n",
    "    def __tokenize(self, text):\n",
    "        sequences = self.__tokenizer.texts_to_sequences(text)\n",
    "        return pad_sequences(sequences, maxlen = self.__MAXLEN)\n",
    "        \n",
    "    def __get_pad_sequences(self, test):\n",
    "        test = [self.__text_preprocessing(text) for text in test]\n",
    "        test_sequences = self.__tokenizer.texts_to_sequences(test)\n",
    "        return pad_sequences(test_sequences, maxlen = self.__MAXLEN)\n",
    "        \n",
    "    def predict_sentiment(self, text):\n",
    "        text = self.__text_preprocessing(text)\n",
    "        seq = self.__tokenize([text])\n",
    "        return self.__model.predict(seq).take(0)\n",
    "    \n",
    "    def predict_all(self, test):\n",
    "        test_data = self.__get_pad_sequences(test)\n",
    "        return list(pred[0] for pred in self.__model.predict(test_data).tolist())\n",
    "    \n",
    "    def evaluate(self, test, label):\n",
    "        test_data = self.__get_pad_sequences(test)\n",
    "        label = np.asarray(label)\n",
    "        return self.__model.evaluate(test_data,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.black_box import BlackBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_box = BlackBox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#black_box.evaluate(test_data, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[y_test[456]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9924508"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "black_box.predict_sentiment(x_test[456])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500/16500 [==============================] - 41s 3ms/sample - loss: 0.2292 - acc: 0.9176\n"
     ]
    }
   ],
   "source": [
    "black_box.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
