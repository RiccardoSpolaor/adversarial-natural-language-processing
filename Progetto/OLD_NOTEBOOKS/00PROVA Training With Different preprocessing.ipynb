{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "\n",
    "SEED = 123\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 Training and evaluating a DNN model on the IMDB Dataset\n",
    "## Downloading and data preprocessing\n",
    "\n",
    "Downloaded the dataset at http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "```\n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  month     = {June},\n",
    "  year      = {2011},\n",
    "  address   = {Portland, Oregon, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {142--150},\n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns = ['text','sentiment'])\n",
    "\n",
    "imdb_dir = \"./datasets/aclImdb\"\n",
    "\n",
    "for dir_kind in ['train','test']:\n",
    "    for label_type in ['neg', 'pos']:\n",
    "        dir_name = os.path.join(imdb_dir, dir_kind, label_type)\n",
    "        for fname in os.listdir(dir_name):\n",
    "            if fname[-4:] == '.txt':\n",
    "                f = open(os.path.join(dir_name, fname), encoding = \"utf8\")\n",
    "                df = df.append({'text': f.read(), 'sentiment': ['neg','pos'].index(label_type)}, ignore_index = True)\n",
    "                f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Story of a man who has unnatural feelings for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Airport '77 starts as a brand new luxury 747 p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This film lacked something I couldn't put my f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sorry everyone,,, I know this is supposed to b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When I was little my parents took me along to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  Story of a man who has unnatural feelings for ...         0\n",
       "1  Airport '77 starts as a brand new luxury 747 p...         0\n",
       "2  This film lacked something I couldn't put my f...         0\n",
       "3  Sorry everyone,,, I know this is supposed to b...         0\n",
       "4  When I was little my parents took me along to ...         0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative istances: 25000\n",
      "Number of positive istances: 25000\n",
      "Il dataset risulta essere bilanciato!\n"
     ]
    }
   ],
   "source": [
    "print ('Number of negative istances:', len(df[df['sentiment'] == 0]))\n",
    "print ('Number of positive istances:', len(df[df['sentiment'] == 1]))\n",
    "print ('Il dataset risulta essere bilanciato!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\n"
     ]
    }
   ],
   "source": [
    "print(df['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing Train and Test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size = 0.33, shuffle = True)\n",
    "\n",
    "x_train = list(x_train)\n",
    "x_test = list(x_test)\n",
    "\n",
    "y_train = list(y_train)\n",
    "y_test = list(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove preprocessing\n",
    "\n",
    "https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "\n",
    "with open(\"glove\\\\glove.42B.300d.txt\", \"r\",errors ='ignore', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_dict[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.5632e-01,  7.0167e-02, -1.0856e-01,  6.3920e-02,  4.4188e-01,\n",
       "        1.6448e-01, -2.2552e+00,  4.1941e-01, -3.1636e-01, -2.8735e-01,\n",
       "       -1.0089e-01,  2.8728e-01, -1.9072e-01,  1.9813e-01,  1.4305e-01,\n",
       "       -1.9234e-02,  7.8137e-03, -2.7725e-01, -1.7461e-01, -2.7296e-02,\n",
       "        2.0745e-01, -3.8855e-02, -6.2267e-01,  2.0114e-01,  1.8017e-01,\n",
       "       -1.4309e-01,  7.3436e-03,  4.5914e-02,  1.2701e-01,  1.9567e-01,\n",
       "       -3.3800e-01, -5.2403e-02,  3.8635e-01,  3.2452e-01,  4.3314e-02,\n",
       "        5.5894e-02, -2.7400e-01,  2.3822e-01,  3.5066e-01,  9.3277e-02,\n",
       "       -2.3778e-01, -2.3854e-01, -1.3535e-01,  1.5447e-01,  9.6359e-02,\n",
       "        9.1433e-02,  2.2692e-01, -7.4975e-02, -5.9885e-01,  1.0320e-01,\n",
       "        3.8681e-01, -3.0790e-01, -9.9559e-02, -2.6215e-02, -2.2730e-01,\n",
       "       -4.7876e-01, -7.3886e-02,  1.3225e-01, -3.0348e-01,  5.2221e-01,\n",
       "        4.4130e-02, -5.5885e-02, -3.4364e-01,  2.9747e-01, -1.1198e-01,\n",
       "       -6.0315e-01, -2.7066e-01,  1.9420e-01,  1.5879e-01, -1.2067e-01,\n",
       "       -3.9149e-01, -2.2446e-01,  5.7599e-03,  1.0279e-02, -1.6890e-01,\n",
       "       -2.1680e-01, -2.0914e-01,  4.8150e-01, -3.9147e-01, -2.8953e-01,\n",
       "        2.5419e-01, -4.6174e-01,  4.4380e-01, -2.3713e-01,  7.2150e-02,\n",
       "        4.8336e-01,  9.3756e-02, -3.7705e-02,  1.4864e-01,  2.9109e-01,\n",
       "       -6.0434e-02, -5.9944e-01, -1.0500e-01, -7.4636e-02,  1.8786e-01,\n",
       "        6.8264e-01, -2.0351e+00, -4.9578e-02,  1.5642e-01, -3.9180e-02,\n",
       "       -1.4909e-01, -7.3187e-02,  5.0762e-01,  2.6983e-02, -5.5783e-01,\n",
       "        4.7270e-01,  1.0095e-01,  4.5862e-01, -6.2622e-02, -2.1382e-01,\n",
       "       -5.4910e-03,  5.2229e-02, -1.0026e-01,  1.4687e-01,  1.1650e-01,\n",
       "       -6.3272e-02,  9.9580e-02,  5.0915e-01,  1.0498e-01, -9.4182e-02,\n",
       "       -2.3452e-01, -1.8576e-02,  2.0310e-02, -1.3528e-02,  1.8450e-01,\n",
       "       -4.1091e-01,  3.9790e-02,  2.5298e-01, -5.5495e-02,  3.3062e-01,\n",
       "       -1.6355e-01,  3.5122e-01, -6.7532e-02, -1.5499e-01, -4.0283e-01,\n",
       "       -4.1311e-01, -1.0714e-01, -6.2246e-01,  7.9395e-02,  8.4307e-01,\n",
       "        3.3514e-01, -1.1906e-01, -4.9424e-01, -4.3044e-01, -1.3389e-01,\n",
       "       -5.1032e-01,  6.8153e-01, -1.2873e-01, -1.8020e-01, -1.0992e-02,\n",
       "       -3.6983e-02, -2.3680e-01, -2.1248e-01,  3.2912e-01,  1.0232e-01,\n",
       "        3.4121e-02, -2.4824e-01, -5.4069e-02,  3.1243e-01, -3.4853e-01,\n",
       "        5.6615e-02,  2.5936e-01, -1.7554e-01, -1.7332e-01,  2.2569e-03,\n",
       "       -4.3400e-01,  1.0858e-01, -1.4214e-01,  5.6738e-01, -3.7382e-01,\n",
       "       -2.7801e-01,  3.7660e-02, -8.6311e-02, -1.7032e-02,  3.4599e-01,\n",
       "        3.8407e-02,  3.9709e-01,  3.7436e-02,  2.0677e-02, -1.2647e-01,\n",
       "        9.9674e-02,  3.9934e-01,  2.5696e-01,  1.8549e-01, -1.2690e-01,\n",
       "        1.7089e-01, -1.6896e-01,  1.3489e-01,  1.8779e-02, -2.7880e-02,\n",
       "        1.3588e-01, -4.5849e-02,  3.0583e-01,  8.1116e-02,  4.4987e-02,\n",
       "       -3.3128e-01,  5.3009e-01,  4.1595e-01, -3.3131e-01, -4.0157e-02,\n",
       "        3.2195e-01,  3.7154e-01,  3.8202e-01,  1.1299e-01, -1.5610e-01,\n",
       "       -8.5718e-02,  3.3083e-01,  3.1913e-01, -3.4404e-01, -2.1760e-01,\n",
       "       -1.6266e-01, -1.1864e-01,  6.5893e-02, -3.1143e-02, -1.2251e-02,\n",
       "       -1.0847e-01, -1.4923e-01, -7.1291e-01,  5.5287e-02,  6.7633e-02,\n",
       "       -1.0804e-01, -1.2727e-03, -3.8229e-01, -6.0203e-02, -4.2665e+00,\n",
       "        4.2173e-01,  1.3024e-01,  9.4837e-02, -1.9178e-03,  1.9432e-01,\n",
       "       -2.4945e-01,  1.4912e-01,  7.1274e-02, -1.9299e-01,  3.1941e-01,\n",
       "        6.0742e-02,  9.5889e-02, -1.7302e-01, -4.8453e-01, -4.7914e-01,\n",
       "       -3.3519e-01, -3.8776e-01,  1.3326e-01,  3.5923e-01, -9.1952e-02,\n",
       "       -2.8467e-01, -2.0948e-01,  2.3090e-01, -3.5915e-01,  2.1352e-01,\n",
       "       -2.4381e-01,  6.1679e-02, -2.4739e-01, -1.2886e-01, -2.5595e-01,\n",
       "        4.4218e-01, -2.1272e-01, -1.4728e-02, -1.9043e-01,  3.4073e-01,\n",
       "       -2.8298e-01,  2.4248e-01,  8.2348e-02, -3.8178e-01,  5.7402e-01,\n",
       "       -3.1505e-01,  4.2794e-02,  1.1085e-01,  4.6410e-01, -3.4755e-01,\n",
       "       -4.3290e-01,  3.4554e-01,  2.8313e-02,  8.4112e-02,  1.5865e-01,\n",
       "        9.6038e-02, -9.5528e-02,  7.9629e-02, -1.8217e-01,  9.5811e-02,\n",
       "        2.1544e-01, -5.4085e-02,  4.0857e-01,  2.4174e-01, -8.1513e-02,\n",
       "       -2.0428e-02,  1.9264e-01, -3.6382e-01, -2.8863e-02,  1.0432e-01,\n",
       "        1.9712e-01, -3.3538e-02,  2.5351e-01, -3.0916e-01, -1.0009e-01,\n",
       "        2.8267e-01, -1.8034e-01, -5.9228e-02,  3.3472e-01, -2.1873e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dict['10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def remove_html_tags(text):\n",
    "    return BeautifulSoup(text, 'lxml').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [remove_html_tags(sentence) for sentence in x_train]\n",
    "x_test = [remove_html_tags(sentence) for sentence in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = [sentence.replace(\"\\x85\", \"\") for sentence in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in embeddings_dict.keys() if w[0].isupper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [sentence.lower() for sentence in x_train]\n",
    "x_test = [sentence.lower() for sentence in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "latin_similar = \"’'‘ÆÐƎƏƐƔĲŊŒẞÞǷȜæðǝəɛɣĳŋœĸſßþƿȝĄƁÇĐƊĘĦĮƘŁØƠŞȘŢȚŦŲƯY̨Ƴąɓçđɗęħįƙłøơşșţțŧųưy̨ƴÁÀÂÄǍĂĀÃÅǺĄÆǼǢƁĆĊĈČÇĎḌĐƊÐÉÈĖÊËĚĔĒĘẸƎƏƐĠĜǦĞĢƔáàâäǎăāãåǻąæǽǣɓćċĉčçďḍđɗðéèėêëěĕēęẹǝəɛġĝǧğģɣĤḤĦIÍÌİÎÏǏĬĪĨĮỊĲĴĶƘĹĻŁĽĿʼNŃN̈ŇÑŅŊÓÒÔÖǑŎŌÕŐỌØǾƠŒĥḥħıíìiîïǐĭīĩįịĳĵķƙĸĺļłľŀŉńn̈ňñņŋóòôöǒŏōõőọøǿơœŔŘŖŚŜŠŞȘṢẞŤŢṬŦÞÚÙÛÜǓŬŪŨŰŮŲỤƯẂẀŴẄǷÝỲŶŸȲỸƳŹŻŽẒŕřŗſśŝšşșṣßťţṭŧþúùûüǔŭūũűůųụưẃẁŵẅƿýỳŷÿȳỹƴźżžẓ\"\n",
    "safe_characters = string.ascii_letters + string.digits + latin_similar + ' '\n",
    "safe_characters += \"'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " '.',\n",
       " '\"',\n",
       " ':',\n",
       " ')',\n",
       " '(',\n",
       " '-',\n",
       " '!',\n",
       " '?',\n",
       " '|',\n",
       " ';',\n",
       " '$',\n",
       " '&',\n",
       " '/',\n",
       " '[',\n",
       " ']',\n",
       " '>',\n",
       " '%',\n",
       " '=',\n",
       " '#',\n",
       " '+',\n",
       " '@',\n",
       " '~',\n",
       " '£',\n",
       " '\\\\',\n",
       " '_',\n",
       " '{',\n",
       " '}',\n",
       " '^',\n",
       " '`',\n",
       " '<',\n",
       " '€',\n",
       " '›',\n",
       " '½',\n",
       " '…',\n",
       " '“',\n",
       " '”',\n",
       " '–',\n",
       " '¢',\n",
       " '¡',\n",
       " '¿',\n",
       " '―',\n",
       " '¥',\n",
       " '—',\n",
       " '‹',\n",
       " '¼',\n",
       " '¤',\n",
       " '¾',\n",
       " '、',\n",
       " '»',\n",
       " '。',\n",
       " '‟',\n",
       " '￥',\n",
       " '«',\n",
       " '฿',\n",
       " 'ª',\n",
       " '˚',\n",
       " 'ƒ',\n",
       " 'ˈ',\n",
       " 'ˑ',\n",
       " '⅓',\n",
       " '˜',\n",
       " '₤',\n",
       " 'ˆ',\n",
       " '￡',\n",
       " '₂',\n",
       " '˙',\n",
       " '؟',\n",
       " '˝',\n",
       " '⅛',\n",
       " '„',\n",
       " 'ɡ',\n",
       " '۞',\n",
       " '๑',\n",
       " '⅔',\n",
       " 'ˌ',\n",
       " 'ﾟ',\n",
       " '⅜',\n",
       " '‛',\n",
       " '܂',\n",
       " '⁰',\n",
       " 'ở',\n",
       " '⅝',\n",
       " 'ﬁ',\n",
       " '͡',\n",
       " '̅',\n",
       " '۩',\n",
       " 'α',\n",
       " 'ʈ',\n",
       " '⅞',\n",
       " 'ɪ',\n",
       " '￦',\n",
       " ';',\n",
       " '̣',\n",
       " '˛',\n",
       " '٠',\n",
       " '₃',\n",
       " 'ȃ',\n",
       " '‚',\n",
       " 'ν',\n",
       " '۶',\n",
       " 'ǡ',\n",
       " 'ʿ',\n",
       " 'ʃ',\n",
       " '₁',\n",
       " 'β',\n",
       " 'ʤ',\n",
       " '˘',\n",
       " '٩',\n",
       " '̵',\n",
       " '￠',\n",
       " 'в',\n",
       " '̶',\n",
       " 'ǥ',\n",
       " 'λ',\n",
       " '２',\n",
       " 'δ',\n",
       " '٤',\n",
       " '۵',\n",
       " 'ˇ',\n",
       " '۲',\n",
       " '́',\n",
       " '１',\n",
       " 'ー',\n",
       " '۰',\n",
       " 'ƃ',\n",
       " 'ɔ',\n",
       " 'ɑ',\n",
       " '̂',\n",
       " 'ǀ',\n",
       " 'ω',\n",
       " '۱',\n",
       " 'ʡ',\n",
       " 'ʊ',\n",
       " '̃',\n",
       " '日',\n",
       " '⁴',\n",
       " 'ʒ',\n",
       " '̳',\n",
       " '３',\n",
       " '։',\n",
       " 'μ',\n",
       " 'ɂ',\n",
       " '₄',\n",
       " 'θ',\n",
       " 'ɨ',\n",
       " 'ｏ',\n",
       " 'ͧ',\n",
       " '年',\n",
       " 'ǰ',\n",
       " 'φ',\n",
       " 'ȥ',\n",
       " '７',\n",
       " 'ɿ',\n",
       " 'ـ',\n",
       " 'γ',\n",
       " 'ʌ',\n",
       " 'ǂ',\n",
       " 'ʻ',\n",
       " 'ɐ',\n",
       " 'ﬂ',\n",
       " 'ǹ',\n",
       " '̿',\n",
       " '̊',\n",
       " 'ƥ',\n",
       " 'ɒ',\n",
       " 'и',\n",
       " 'π',\n",
       " '４',\n",
       " 'ɹ',\n",
       " 'а',\n",
       " 'ｓ',\n",
       " '̏',\n",
       " 'ʔ',\n",
       " 'σ',\n",
       " 'ａ',\n",
       " 'ｉ',\n",
       " 'ȡ',\n",
       " 'ǵ',\n",
       " 'は',\n",
       " 'ǩ',\n",
       " '⁵',\n",
       " '̀',\n",
       " 'ʹ',\n",
       " '５',\n",
       " 'ᴥ',\n",
       " '߂',\n",
       " '˃',\n",
       " '˹',\n",
       " 'ȣ',\n",
       " '͂',\n",
       " 'ｗ',\n",
       " '̄',\n",
       " 'ȭ',\n",
       " 'ȿ',\n",
       " 'ｍ',\n",
       " 'ɤ',\n",
       " 'ȸ',\n",
       " 'с',\n",
       " 'ƽ',\n",
       " '₀',\n",
       " 'ｃ',\n",
       " 'ạ',\n",
       " 'ε',\n",
       " '⁶',\n",
       " 'の',\n",
       " '๐',\n",
       " '月',\n",
       " '̌',\n",
       " 'ɾ',\n",
       " 'ﾞ',\n",
       " '̸',\n",
       " 'ʘ',\n",
       " 'ɸ',\n",
       " 'ȫ',\n",
       " '⁸',\n",
       " '⅕',\n",
       " '̾',\n",
       " '₆',\n",
       " 'ｖ',\n",
       " 'τ',\n",
       " 'ʕ',\n",
       " 'ȯ',\n",
       " '܀',\n",
       " 'ː',\n",
       " '΄',\n",
       " '６',\n",
       " 'ˤ',\n",
       " 'ǫ',\n",
       " '๖',\n",
       " 'ｐ',\n",
       " 'ͤ',\n",
       " '̱',\n",
       " '١',\n",
       " '۳',\n",
       " '̎',\n",
       " '⁷',\n",
       " 'ɩ',\n",
       " 'ổ',\n",
       " '̐',\n",
       " '̓',\n",
       " 'ρ',\n",
       " 'ƪ',\n",
       " '̷',\n",
       " 'ˉ',\n",
       " 'ʞ',\n",
       " '₅',\n",
       " 'ɚ',\n",
       " 'ɯ',\n",
       " 'ʺ',\n",
       " 'ɲ',\n",
       " '\\u06dd',\n",
       " 'ɻ',\n",
       " '˂',\n",
       " 'ˡ',\n",
       " '位',\n",
       " 'ʹ',\n",
       " 'ʂ',\n",
       " 'ｂ',\n",
       " '８',\n",
       " 'ǭ',\n",
       " '⅙',\n",
       " 'ʧ',\n",
       " '９',\n",
       " 'ʐ',\n",
       " 'ｑ',\n",
       " 'ʋ',\n",
       " '۴',\n",
       " '̕',\n",
       " '̗',\n",
       " 'ʱ',\n",
       " 'ƶ',\n",
       " 'ǁ',\n",
       " '˻',\n",
       " '̴',\n",
       " 'ｔ',\n",
       " 'ḳ',\n",
       " 'ｘ',\n",
       " 'ǯ',\n",
       " 'ɵ',\n",
       " 'ʀ',\n",
       " 'ȱ',\n",
       " '号']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_chars = [c for c in list(embeddings_dict.keys()) if len(c) == 1]\n",
    "glove_symbols = [c for c in glove_chars if not c in safe_characters]\n",
    "glove_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['³',\n",
       " '₤',\n",
       " '*',\n",
       " ')',\n",
       " ',',\n",
       " '»',\n",
       " 'ן',\n",
       " '★',\n",
       " '\\x96',\n",
       " 'ו',\n",
       " 'י',\n",
       " '®',\n",
       " '¤',\n",
       " '«',\n",
       " 'ל',\n",
       " '!',\n",
       " '>',\n",
       " '|',\n",
       " '$',\n",
       " ':',\n",
       " '`',\n",
       " 'º',\n",
       " '\\x97',\n",
       " '\\xad',\n",
       " '½',\n",
       " '\\x9e',\n",
       " '·',\n",
       " 'ג',\n",
       " '£',\n",
       " '^',\n",
       " '[',\n",
       " '´',\n",
       " 'כ',\n",
       " '\\x84',\n",
       " '§',\n",
       " '°',\n",
       " '¢',\n",
       " '}',\n",
       " '▼',\n",
       " '\\x8d',\n",
       " '\\xa0',\n",
       " '.',\n",
       " 'א',\n",
       " '{',\n",
       " '…',\n",
       " '&',\n",
       " '\\x8e',\n",
       " '¡',\n",
       " '\\uf04a',\n",
       " '\\x85',\n",
       " ';',\n",
       " '\\uf0b7',\n",
       " '(',\n",
       " '\\x91',\n",
       " '%',\n",
       " '”',\n",
       " '+',\n",
       " '_',\n",
       " '@',\n",
       " 'ר',\n",
       " '#',\n",
       " '<',\n",
       " '¿',\n",
       " '“',\n",
       " '\\x95',\n",
       " '?',\n",
       " '\\\\',\n",
       " '\"',\n",
       " ']',\n",
       " '~',\n",
       " '/',\n",
       " '\\t',\n",
       " '¨',\n",
       " '\\x9a',\n",
       " '-',\n",
       " '–',\n",
       " '¾',\n",
       " 'מ',\n",
       " '=',\n",
       " '¦',\n",
       " '\\x80']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jigsaw_chars = set(w for sentence in x_train for w in sentence)\n",
    "jigsaw_symbols = [c for c in jigsaw_chars if not c in safe_characters]\n",
    "jigsaw_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['³',\n",
       " '*',\n",
       " 'ן',\n",
       " '★',\n",
       " '\\x96',\n",
       " 'ו',\n",
       " 'י',\n",
       " '®',\n",
       " 'ל',\n",
       " 'º',\n",
       " '\\x97',\n",
       " '\\xad',\n",
       " '\\x9e',\n",
       " '·',\n",
       " 'ג',\n",
       " '´',\n",
       " 'כ',\n",
       " '\\x84',\n",
       " '§',\n",
       " '°',\n",
       " '▼',\n",
       " '\\x8d',\n",
       " '\\xa0',\n",
       " 'א',\n",
       " '\\x8e',\n",
       " '\\uf04a',\n",
       " '\\x85',\n",
       " '\\uf0b7',\n",
       " '\\x91',\n",
       " 'ר',\n",
       " '\\x95',\n",
       " '\\t',\n",
       " '¨',\n",
       " '\\x9a',\n",
       " 'מ',\n",
       " '¦',\n",
       " '\\x80']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols_to_delete = [c for c in jigsaw_symbols if not c in glove_symbols]\n",
    "symbols_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['₤',\n",
       " ')',\n",
       " ',',\n",
       " '»',\n",
       " '¤',\n",
       " '«',\n",
       " '!',\n",
       " '>',\n",
       " '|',\n",
       " '$',\n",
       " ':',\n",
       " '`',\n",
       " '½',\n",
       " '£',\n",
       " '^',\n",
       " '[',\n",
       " '¢',\n",
       " '}',\n",
       " '.',\n",
       " '{',\n",
       " '…',\n",
       " '&',\n",
       " '¡',\n",
       " ';',\n",
       " '(',\n",
       " '%',\n",
       " '”',\n",
       " '+',\n",
       " '_',\n",
       " '@',\n",
       " '#',\n",
       " '<',\n",
       " '¿',\n",
       " '“',\n",
       " '?',\n",
       " '\\\\',\n",
       " '\"',\n",
       " ']',\n",
       " '~',\n",
       " '/',\n",
       " '-',\n",
       " '–',\n",
       " '¾',\n",
       " '=']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols_to_isolate = [c for c in jigsaw_symbols if c in glove_symbols]\n",
    "symbols_to_isolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "    for symbol in symbols_to_delete:\n",
    "        x = x.replace(symbol, ' ')\n",
    "    for symbol in symbols_to_isolate:\n",
    "        x = x.replace(symbol, ' ' + symbol + ' ')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [clean_text(sentence) for sentence in x_train]\n",
    "x_test = [clean_text(sentence) for sentence in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(c for c in set(w for sentence in x_train for w in sentence) if not c in safe_characters) == set(symbols_to_isolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_contractions(x):\n",
    "    x = tokenizer.tokenize(x)\n",
    "    x = ' '.join(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [handle_contractions(sentence) for sentence in x_train]\n",
    "x_test = [handle_contractions(sentence) for sentence in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rutched',\n",
       " 'misfocused',\n",
       " 'wordwhat',\n",
       " 'bablon',\n",
       " \"'frankie\",\n",
       " 'evilmaker',\n",
       " 'nightimmunity',\n",
       " 'sabeva',\n",
       " 'lazarous',\n",
       " \"'puckoon\",\n",
       " 'eréndira',\n",
       " 'famishius',\n",
       " 'ratcher',\n",
       " \"'gigantismoses\",\n",
       " 'brockeridge',\n",
       " 'frivoli',\n",
       " 'fmlb',\n",
       " \"sonny'y\",\n",
       " 'a666333',\n",
       " 'permaybe',\n",
       " 'schya',\n",
       " 'scuzziness',\n",
       " 'tt0117979',\n",
       " \"'return\",\n",
       " 'andromina',\n",
       " 'bhangladesh',\n",
       " 'xenophobicjust',\n",
       " 'wdisescreen',\n",
       " 'stillm',\n",
       " 'blankwall',\n",
       " \"'taboo\",\n",
       " 'kamanglish',\n",
       " 'happensthey',\n",
       " \"'forbidden\",\n",
       " 'digonales',\n",
       " 'aruman',\n",
       " \"'distressed\",\n",
       " \"'caper\",\n",
       " 'walkees',\n",
       " 'miscasted',\n",
       " 'stéphanois',\n",
       " 'devadharshini',\n",
       " \"'sea\",\n",
       " 'smittened',\n",
       " 'grinderlin',\n",
       " 'niellson',\n",
       " \"'conceiving\",\n",
       " 'meshugaas',\n",
       " 'disconforting',\n",
       " \"'ice\",\n",
       " 'lovetrapmovie',\n",
       " 'nonproportionally',\n",
       " \"'tiny\",\n",
       " 'fredos',\n",
       " 'yokhai',\n",
       " 'enormenent',\n",
       " 'tricolli',\n",
       " 'scifimaybe',\n",
       " 'westdijk',\n",
       " 'masladar',\n",
       " 'overcaution',\n",
       " 'incensere',\n",
       " 'féodor',\n",
       " \"'official\",\n",
       " \"'visual\",\n",
       " \"'met\",\n",
       " \"'songs\",\n",
       " 'slaptick',\n",
       " 'provolking',\n",
       " \"'debacle\",\n",
       " 'henchgirl',\n",
       " \"'stan\",\n",
       " \"'utter\",\n",
       " 'unambitiously',\n",
       " 'nostras',\n",
       " \"'into\",\n",
       " 'shintarô',\n",
       " 'lanscaper',\n",
       " 'maurren',\n",
       " \"'oddball\",\n",
       " 'comparance',\n",
       " \"'signs\",\n",
       " 'altıoklar',\n",
       " 'womanness',\n",
       " 'appelagate',\n",
       " 'zanghief',\n",
       " 'delacroixs',\n",
       " 'reaccounting',\n",
       " 'broodish',\n",
       " 'momsem',\n",
       " \"'ultimatum\",\n",
       " 'uncapturable',\n",
       " \"'vertigo\",\n",
       " \"r'n'b\",\n",
       " 'oorie',\n",
       " 'waissbluth',\n",
       " \"'focus\",\n",
       " 'deadler',\n",
       " 'hensema',\n",
       " \"'studied\",\n",
       " \"'introspective\",\n",
       " 'scorscese',\n",
       " 'lubtchansky',\n",
       " 'scarole',\n",
       " 'milfune',\n",
       " 'wewwy',\n",
       " \"'accessible\",\n",
       " \"'cute\",\n",
       " \"'flood\",\n",
       " 'wm14',\n",
       " 'overvoice',\n",
       " '10thlevel',\n",
       " 'unuiversal',\n",
       " 'terrrrrrrrrrrrrrrriiiiiiiiiiiible',\n",
       " 'hobbitslord',\n",
       " 'swasa',\n",
       " 'teensploitation',\n",
       " 'unredemable',\n",
       " 'careddu',\n",
       " 'stripteasers',\n",
       " 'tlkg',\n",
       " \"'joxer\",\n",
       " 'skinkons',\n",
       " 'poulsson',\n",
       " \"'nightmares\",\n",
       " \"'tv\",\n",
       " 'magicbecause',\n",
       " 'batzella',\n",
       " 'blackbuster',\n",
       " 'macmahone',\n",
       " 'interviú',\n",
       " 'motivationsit',\n",
       " 'contravert',\n",
       " 'hanggliders',\n",
       " 'ahhi',\n",
       " 'soulplane',\n",
       " 'zudina',\n",
       " 'the1940',\n",
       " 'fioreavanti',\n",
       " 'bregnans',\n",
       " \"'tourist\",\n",
       " 'acommercial',\n",
       " \"youv'e\",\n",
       " \"'damien\",\n",
       " 'buntch',\n",
       " 'slaussen',\n",
       " 'lidth',\n",
       " \"'hitch\",\n",
       " 'pevensy',\n",
       " 'moviesfreddy',\n",
       " 'thebom',\n",
       " 'disneylike',\n",
       " 'canunderstand',\n",
       " 'luftens',\n",
       " 'archchristian',\n",
       " \"'mucs\",\n",
       " 'turrco',\n",
       " 'muzeyyen',\n",
       " 'kersy',\n",
       " \"should'nt\",\n",
       " 'jaynetts',\n",
       " 'medioacre',\n",
       " \"5'000\",\n",
       " 'dormal',\n",
       " \"'fight\",\n",
       " \"'carry\",\n",
       " 'actor’s',\n",
       " '05nomactr',\n",
       " 'ratbatspidercrab',\n",
       " 'louhimes',\n",
       " 'welisch',\n",
       " 'encorew',\n",
       " \"n'synch\",\n",
       " 'überwoman',\n",
       " \"''the\",\n",
       " \"'if\",\n",
       " 'prètre',\n",
       " \"'cuban\",\n",
       " 'bandekartrivia',\n",
       " 'cinemaphile',\n",
       " \"'genre\",\n",
       " \"'crashers\",\n",
       " 'allsuperb',\n",
       " 'ttkk',\n",
       " 'telets',\n",
       " 'rahhhhhh',\n",
       " 'êxtase',\n",
       " 'viusit',\n",
       " 'kráska',\n",
       " \"'possible\",\n",
       " '1930ies',\n",
       " 'beginsthis',\n",
       " 'katzelmacher',\n",
       " \"'its\",\n",
       " 'mikeandvicki',\n",
       " 'usmoviegoers',\n",
       " \"'premonition\",\n",
       " 'kuriyami',\n",
       " \"'grandmas\",\n",
       " \"'meant\",\n",
       " \"'nsna\",\n",
       " 'hottttttttttttttttttttt',\n",
       " 'santimoniousness',\n",
       " 'agathaclosing',\n",
       " 'sufferíngs',\n",
       " '9of10',\n",
       " 'vimbley',\n",
       " 'carnotaur',\n",
       " \"'fiddler\",\n",
       " 'manoven',\n",
       " \"'another\",\n",
       " \"'nurse\",\n",
       " \"'rumours\",\n",
       " 'bakersfeild',\n",
       " \"'coon\",\n",
       " \"'cheesefest\",\n",
       " 'naverone',\n",
       " 'longdavid',\n",
       " 'chemotrodes',\n",
       " \"'merely\",\n",
       " 'shayamalan',\n",
       " 'wilnona',\n",
       " 'baudelairian',\n",
       " \"'hating\",\n",
       " 'pambieri',\n",
       " \"'steal\",\n",
       " \"'st\",\n",
       " 'rrrowrr',\n",
       " 'hadek',\n",
       " 'bronenosets',\n",
       " 'spookishly',\n",
       " 'lumpke',\n",
       " 'stemmin',\n",
       " \"'divine\",\n",
       " \"'upstanding\",\n",
       " 'maaaannnn',\n",
       " \"'goodies\",\n",
       " \"'crap\",\n",
       " 'illtreated',\n",
       " 'kareeena',\n",
       " 'dureyea',\n",
       " 'andpulled',\n",
       " 'mcmohan',\n",
       " 'shanao',\n",
       " \"'rogue\",\n",
       " 'baritoned',\n",
       " \"'rescue\",\n",
       " 'unintense',\n",
       " \"'married\",\n",
       " 'smithapatel',\n",
       " 'rhyes',\n",
       " 'pleantly',\n",
       " 'attepted',\n",
       " \"'bloody\",\n",
       " \"'pre\",\n",
       " 'annoynimous',\n",
       " \"'strangers\",\n",
       " 'oceanologists',\n",
       " \"'spriggins\",\n",
       " \"'lillie\",\n",
       " \"'lupinesque\",\n",
       " \"'don\",\n",
       " 'halholbrook',\n",
       " '003830',\n",
       " 'sobiesky',\n",
       " 'xvichia',\n",
       " \"off'ed\",\n",
       " 'ferality',\n",
       " 'jumbledthe',\n",
       " 'carroça',\n",
       " 'llshit',\n",
       " 'ivanowitch',\n",
       " 'revealedi',\n",
       " \"'injury\",\n",
       " \"'north\",\n",
       " 'hourslong',\n",
       " \"'symphony\",\n",
       " 'closups',\n",
       " 'lynchesque',\n",
       " \"'usual\",\n",
       " \"'gosh\",\n",
       " 'patheticmusic',\n",
       " \"'dean\",\n",
       " 'can’t',\n",
       " \"'goofs\",\n",
       " \"'rotoscope\",\n",
       " \"'successful\",\n",
       " \"'cake\",\n",
       " 'whattim',\n",
       " 'blackxploitation',\n",
       " \"'slashing\",\n",
       " \"'hunter\",\n",
       " '10entertainment',\n",
       " 'jaongi',\n",
       " 'ossessioneluchino',\n",
       " 'bondless',\n",
       " 'pancino',\n",
       " \"'rambha\",\n",
       " \"'windbag\",\n",
       " 'sumptiously',\n",
       " 'bikumatre',\n",
       " \"'annoying\",\n",
       " 'thomkat',\n",
       " 'martinezexcellent',\n",
       " 'anniko',\n",
       " \"'delightful\",\n",
       " 'sorken',\n",
       " 'umilak',\n",
       " 'elemnents',\n",
       " 'saluesen',\n",
       " 'playedcons',\n",
       " 'frightworld',\n",
       " \"'inspired\",\n",
       " 'phsycotic',\n",
       " 'gustatorial',\n",
       " \"love'expresses\",\n",
       " 'paganography',\n",
       " \"'talky\",\n",
       " 'whocomically',\n",
       " 'fe5rdin',\n",
       " 'byzantiums',\n",
       " 'synonomus',\n",
       " 'polonis',\n",
       " \"'bowling\",\n",
       " \"'braveheart\",\n",
       " 'zomedy',\n",
       " \"'trancers\",\n",
       " \"'saloon\",\n",
       " 'nigmatullin',\n",
       " 'podges',\n",
       " 'kosleck',\n",
       " \"'liberation\",\n",
       " 'andcompelling',\n",
       " 'solendz',\n",
       " \"'streets\",\n",
       " 'instead3',\n",
       " 'glazedly',\n",
       " 'firstmans',\n",
       " \"'wes\",\n",
       " 'poories',\n",
       " 'tatoya',\n",
       " 'bujeau',\n",
       " 'kellagher',\n",
       " 'astronuat',\n",
       " 'vetchý',\n",
       " \"'wizards\",\n",
       " 'ftagn',\n",
       " \"'satirical\",\n",
       " 'malenkaya',\n",
       " 'surewhile',\n",
       " 'movietheatre',\n",
       " '2006smackdown',\n",
       " 'houselessness',\n",
       " 'thatregard',\n",
       " \"'4\",\n",
       " 'doulittle',\n",
       " \"'stuart\",\n",
       " \"'regular\",\n",
       " 'walchek',\n",
       " 'cmndt',\n",
       " 'kilairn',\n",
       " \"'dekho\",\n",
       " 'minigenre',\n",
       " 'seltzberg',\n",
       " \"'wholesome\",\n",
       " 'walburton',\n",
       " 'sparinglytake',\n",
       " \"'a\",\n",
       " 'mcboy',\n",
       " 'junkermann',\n",
       " 'superdooper',\n",
       " 'konerak',\n",
       " \"'slacker\",\n",
       " \"'zankiku\",\n",
       " \"'oggi\",\n",
       " 'terrifyng',\n",
       " 'andalways',\n",
       " 'thispicture',\n",
       " \"'sartana\",\n",
       " \"'jason\",\n",
       " 'chickboxer',\n",
       " 'rakastin',\n",
       " \"'user\",\n",
       " 'overemoting',\n",
       " \"'goodfellas\",\n",
       " 'revetting',\n",
       " 'kitschier',\n",
       " 'sikintisi',\n",
       " \"'unrealistic\",\n",
       " 'nonintentional',\n",
       " 'roquevert',\n",
       " 'almast',\n",
       " 'stanislofsky',\n",
       " 'bipolarly',\n",
       " 'fingersmiths',\n",
       " 'yourhusband',\n",
       " 'whaddayagonndo',\n",
       " 'dyptic',\n",
       " 'mooments',\n",
       " 'scoobidoo',\n",
       " 'zemljic',\n",
       " 'ianguage',\n",
       " 'corporealizes',\n",
       " \"'oldboy\",\n",
       " 'hotari',\n",
       " 'amerterish',\n",
       " \"'godzilla\",\n",
       " \"'newest\",\n",
       " 'lacanians',\n",
       " \"'sister\",\n",
       " 'akuzi',\n",
       " 'brunnberg',\n",
       " 'darwinianed',\n",
       " 'bechlar',\n",
       " 'mitchim',\n",
       " \"'meat\",\n",
       " 'cinemaniacs',\n",
       " 'izuruha',\n",
       " 'cinematek',\n",
       " 'glenfiditch',\n",
       " 'bloodstolling',\n",
       " 'herngren',\n",
       " 'cacaphonous',\n",
       " 'austreheim',\n",
       " \"'wondered\",\n",
       " 'hjerner',\n",
       " 'chinawell',\n",
       " 'imoogi',\n",
       " 'malmar',\n",
       " 'znyyqhuzzah',\n",
       " 'matthison',\n",
       " 'hsyterical',\n",
       " 'plasitcine',\n",
       " 'kerkour',\n",
       " 'pomeii',\n",
       " \"'maddox\",\n",
       " 'nyaako',\n",
       " 'unacurate',\n",
       " 'violenceare',\n",
       " \"'umi\",\n",
       " \"'absolutely\",\n",
       " 'espadrillas',\n",
       " 'guilliotined',\n",
       " 'cwhere',\n",
       " 'francisaco',\n",
       " 'ünfaithful',\n",
       " 'benatatos',\n",
       " 'direcxtor',\n",
       " 'thadblog',\n",
       " 'branugh',\n",
       " 'medieve',\n",
       " 'denigrati',\n",
       " 'meiks',\n",
       " 'diepardieu',\n",
       " 'bruceploitation',\n",
       " 'abemethie',\n",
       " 'gunfires',\n",
       " '10kane',\n",
       " 'muthital',\n",
       " 'sponagle',\n",
       " \"'help\",\n",
       " 'scenesbhumika',\n",
       " 'ravensback',\n",
       " 'kirstey',\n",
       " 'stoopidest',\n",
       " 'allosauros',\n",
       " \"'schindler\",\n",
       " 'cnvrmzx2kms',\n",
       " 'pgby',\n",
       " \"'sucks\",\n",
       " 'imdbs',\n",
       " 'messianistic',\n",
       " 'roebson',\n",
       " 'franknsteiner',\n",
       " 'lionsgate’s',\n",
       " 'realquickly',\n",
       " \"'intelliocracy\",\n",
       " 'rylott',\n",
       " 'schlockiness',\n",
       " \"'alrite\",\n",
       " 'freudstein',\n",
       " 'bernède',\n",
       " 'bachachan',\n",
       " '578i',\n",
       " 'dissasatisfied',\n",
       " 'yanomani',\n",
       " 'blaintly',\n",
       " 'demystifed',\n",
       " 'bibbidy',\n",
       " 'ridiculousone',\n",
       " 'pfink',\n",
       " \"'empty\",\n",
       " \"'follow\",\n",
       " \"'actors\",\n",
       " 'wwwwhhhyyyyyyy',\n",
       " \"'transporter\",\n",
       " '‘collaborations’',\n",
       " \"'nine\",\n",
       " 'arvide',\n",
       " 'peculating',\n",
       " 'macleodparental',\n",
       " 'settlefor',\n",
       " 'enumerous',\n",
       " 'varagas',\n",
       " 'yusoufzai',\n",
       " 'belitski',\n",
       " \"'007\",\n",
       " 'cinematographical',\n",
       " 'cauklin',\n",
       " \"'external\",\n",
       " \"'thankyou\",\n",
       " 'studiosand',\n",
       " 'byniarski',\n",
       " \"seedy'n'sordid\",\n",
       " 'uprosing',\n",
       " 'delightfulvillainy',\n",
       " 'hitcock',\n",
       " 'bisericanu',\n",
       " 'ayben',\n",
       " 'geneticell',\n",
       " 'fraulien',\n",
       " 'crowone',\n",
       " 'khakkee',\n",
       " \"'buck\",\n",
       " 'yurets777',\n",
       " 'browningish',\n",
       " 'belengur',\n",
       " 'sarandon’s',\n",
       " 'marivaudage',\n",
       " \"'hunting\",\n",
       " 'skunker',\n",
       " \"'scape\",\n",
       " 'hiarity',\n",
       " 'propagandamovie',\n",
       " 'alriiight',\n",
       " 'abruptlydirection',\n",
       " 'toorture',\n",
       " \"'them\",\n",
       " \"'ruining\",\n",
       " 'longago',\n",
       " \"'seventies\",\n",
       " 'faldaas',\n",
       " \"'kidnap\",\n",
       " 'maupins',\n",
       " 'hiasashi',\n",
       " \"'negative\",\n",
       " 'böger',\n",
       " \"'pssst\",\n",
       " 'k10c',\n",
       " \"'doghi\",\n",
       " 'mjyoung',\n",
       " \"'gary\",\n",
       " 'darkit',\n",
       " \"'relationships\",\n",
       " 'tographers',\n",
       " 'jumpkicks',\n",
       " \"'everyman\",\n",
       " 'dimitru',\n",
       " \"'call\",\n",
       " 'pannings',\n",
       " 'sentuna',\n",
       " 'belush',\n",
       " 'goodravi',\n",
       " 'carpethia',\n",
       " 'sheriff’s',\n",
       " 'nucyaler',\n",
       " 'transunto',\n",
       " 'maximumanother',\n",
       " '1934earl',\n",
       " \"'educational\",\n",
       " 'extremeeye',\n",
       " \"'yet\",\n",
       " 'bauerisch',\n",
       " 'compères',\n",
       " 'osenniy',\n",
       " 'brambury',\n",
       " 'fairmindedness',\n",
       " 'venantino',\n",
       " \"nell'ombra\",\n",
       " \"'progressive\",\n",
       " 'shlitz',\n",
       " \"'target\",\n",
       " \"'butch\",\n",
       " 'supersoftie',\n",
       " 'anconina',\n",
       " \"'bizet\",\n",
       " 'vespasians',\n",
       " 'mammarian',\n",
       " 'jokevijay',\n",
       " \"'poets\",\n",
       " 'buddyism',\n",
       " 'sextmus',\n",
       " 'tverdokhlebov',\n",
       " 'claudiuses',\n",
       " 'holoband',\n",
       " 'jebidia',\n",
       " 'zerifferelli',\n",
       " \"gov't\",\n",
       " 'vodyanoi',\n",
       " 'ascellular',\n",
       " 'christmastreeshops',\n",
       " 'yewbenighted',\n",
       " 'loathable',\n",
       " \"'trash\",\n",
       " \"'eaten\",\n",
       " 'comsymp',\n",
       " 'flawing',\n",
       " 'wismaster',\n",
       " 'sexegenarian',\n",
       " \"ev'rybody\",\n",
       " 'disillusional',\n",
       " 'lassander’s',\n",
       " 'défroqué',\n",
       " 'patresi',\n",
       " 'screamplay',\n",
       " 'goldhunt',\n",
       " 'mmbm',\n",
       " 'kwricehere',\n",
       " 'apossibly',\n",
       " 'gamg',\n",
       " 'maughm',\n",
       " \"hip'n'flip\",\n",
       " 'mäger',\n",
       " 'vertido',\n",
       " \"'mythical\",\n",
       " \"'uncle\",\n",
       " 'razrukha',\n",
       " 'shakespearen',\n",
       " 'tocherish',\n",
       " 'greebling',\n",
       " \"ser'vuce\",\n",
       " 'sochenge',\n",
       " 'rebenga',\n",
       " 'simonsons',\n",
       " 'unutilzed',\n",
       " \"'miss\",\n",
       " \"'hear\",\n",
       " 'admarible',\n",
       " 'condecension',\n",
       " 'egolatry',\n",
       " 'eurselas',\n",
       " \"'aryan\",\n",
       " \"'plane\",\n",
       " \"'tombstone\",\n",
       " 'threemen',\n",
       " 'villedo',\n",
       " \"'cookie\",\n",
       " 'russoholds',\n",
       " \"'amar\",\n",
       " \"'stray\",\n",
       " 'thalluri',\n",
       " \"'mononoke\",\n",
       " 'bloodsurf',\n",
       " \"'fills\",\n",
       " 'fuhrerbunker',\n",
       " 'oedpius',\n",
       " 'thunderossa',\n",
       " 'lubins',\n",
       " 'hemoglobens',\n",
       " 'championshipcatcus',\n",
       " 'tupinambás',\n",
       " 'clonation',\n",
       " 'montrocity',\n",
       " \"'mean\",\n",
       " 'coolneß',\n",
       " \"'salem\",\n",
       " 'myrtile',\n",
       " 'okerland',\n",
       " \"did'nt\",\n",
       " 'cringethe',\n",
       " 'ververgaert',\n",
       " 'rombero',\n",
       " 'casomai',\n",
       " 'saranadon',\n",
       " \"'hornophobia\",\n",
       " 'brucev13',\n",
       " \"'classes\",\n",
       " 'ariauna',\n",
       " 'formula4',\n",
       " \"'boomer\",\n",
       " 'dodgily',\n",
       " \"'hades\",\n",
       " \"'cheerleader\",\n",
       " \"'tra\",\n",
       " \"'l'enfant\",\n",
       " 'sharoma',\n",
       " \"'uzi\",\n",
       " \"'punch\",\n",
       " \"d'linz\",\n",
       " 'robespierres',\n",
       " 'cattlett',\n",
       " 'daagemusic',\n",
       " 'whomemilion',\n",
       " 'disslikes',\n",
       " \"'love\",\n",
       " \"'incubus\",\n",
       " 'tantalisation',\n",
       " \"'porpoise\",\n",
       " \"'laura\",\n",
       " 'lameinator',\n",
       " 'aldofo',\n",
       " 'johgn',\n",
       " 'hixploitation',\n",
       " 'schanzkowska',\n",
       " 'multiethnical',\n",
       " 'kogenta',\n",
       " 'theison',\n",
       " \"'trip\",\n",
       " 'pointeblank',\n",
       " 'zb1',\n",
       " 'notmeant',\n",
       " 'ablazin',\n",
       " \"years'70\",\n",
       " \"'garbo\",\n",
       " \"'clobber\",\n",
       " 'carltio',\n",
       " 'tovati',\n",
       " \"'sassiness\",\n",
       " 'trekkish',\n",
       " 'hornophobia',\n",
       " 'ductcher',\n",
       " 'mousiness',\n",
       " 'leporids',\n",
       " \"'lawless\",\n",
       " 'delinko',\n",
       " \"'slam\",\n",
       " 'digicorps',\n",
       " \"'garden\",\n",
       " 'conahay',\n",
       " 'visials',\n",
       " 'marlonbrando',\n",
       " \"'counterculture\",\n",
       " 'dyonisian',\n",
       " 'sondhemim',\n",
       " 'k9k9',\n",
       " 'ovbviously',\n",
       " 'arnetia',\n",
       " \"'tart\",\n",
       " \"'incoming\",\n",
       " 'girlygirl148',\n",
       " 'melancholiness',\n",
       " 'patrick’s',\n",
       " \"'moviefreak\",\n",
       " 'dysantry',\n",
       " 'mickery',\n",
       " 'sparkdirection',\n",
       " 'harrleson',\n",
       " 'milkwoman',\n",
       " 'hammily',\n",
       " 'ccxs',\n",
       " 'ship’s',\n",
       " 'unwlecomed',\n",
       " 'angelwas',\n",
       " 'apharan',\n",
       " 'springerland',\n",
       " 'intellectualises',\n",
       " \"'x\",\n",
       " 'massacessi',\n",
       " 'spoilerishit',\n",
       " 'priestliness',\n",
       " \"'masterpiece\",\n",
       " 'emmontional',\n",
       " 'hisboss',\n",
       " \"'devil\",\n",
       " \"tight'n'trim\",\n",
       " \"'arc\",\n",
       " \"'van\",\n",
       " 'eastwod',\n",
       " 'necropole',\n",
       " 'gayniggers',\n",
       " 'memeric',\n",
       " \"'carribean\",\n",
       " 'blaxsploitation',\n",
       " 'himmelskibet',\n",
       " 'serrazina',\n",
       " 'remboutsika',\n",
       " 'golthwaite',\n",
       " 'astronautships',\n",
       " 'thesigner',\n",
       " 'documentarie',\n",
       " 'soultendieck',\n",
       " 'cándida',\n",
       " 'basternook',\n",
       " \"'mince\",\n",
       " 'bashirov',\n",
       " \"'idea\",\n",
       " 'belmonndo',\n",
       " 'beauticin',\n",
       " 'catogoricaly',\n",
       " 'fairyfloss',\n",
       " 'universalsoldier',\n",
       " \"'saudades\",\n",
       " 'aristorcats',\n",
       " 'plaaaain',\n",
       " \"'quartet\",\n",
       " \"'smoke\",\n",
       " \"'romeo\",\n",
       " \"'passionate\",\n",
       " 'poorlyrehearsed',\n",
       " 'dearable',\n",
       " 'localer',\n",
       " 'juvenated',\n",
       " 'quentessential',\n",
       " \"'paper\",\n",
       " 'shubash',\n",
       " 'shadowzone',\n",
       " 'tbor',\n",
       " \"'signifie\",\n",
       " \"'stage\",\n",
       " \"'oooh\",\n",
       " 'lol10',\n",
       " \"'untouchable\",\n",
       " \"'australian\",\n",
       " 'nonaquatic',\n",
       " 'noltie',\n",
       " 'misreably',\n",
       " 'morphosynthesis',\n",
       " 'viciente',\n",
       " 'blaznee',\n",
       " \"'watchable\",\n",
       " 'strelby',\n",
       " 'thugees',\n",
       " \"'robbed\",\n",
       " 'workjason',\n",
       " \"have'nt\",\n",
       " \"'there\",\n",
       " 'arseholing',\n",
       " 'vouryistic',\n",
       " \"victoria'a\",\n",
       " 'kraggartians',\n",
       " 'kid’s',\n",
       " \"'lion\",\n",
       " \"'unintentional\",\n",
       " 'wingism',\n",
       " 'mingozzi',\n",
       " \"'rising\",\n",
       " 'gh1215',\n",
       " \"'erasurehead\",\n",
       " 'unsteerable',\n",
       " 'harriosn',\n",
       " 'longendecker',\n",
       " 'stechino',\n",
       " 'itgets',\n",
       " 'isgeorge',\n",
       " 'zchaundi',\n",
       " \"'he\",\n",
       " \"'chapeau\",\n",
       " 'secdonly',\n",
       " 'trymane',\n",
       " 'bouccy',\n",
       " \"'hypnosis\",\n",
       " \"does'n\",\n",
       " 'instic',\n",
       " 'steroeypes',\n",
       " \"'celebrity\",\n",
       " 'dissilusioned',\n",
       " 'unmercilessly',\n",
       " 'platefull',\n",
       " 'egib',\n",
       " 'imdfb',\n",
       " \"'wish\",\n",
       " 'divana',\n",
       " 'vinterbergs',\n",
       " \"'shawn\",\n",
       " 'guillespe',\n",
       " 'hymilayan',\n",
       " 'propagandish',\n",
       " \"'duel\",\n",
       " 'thescreamonline',\n",
       " \"'henpecked\",\n",
       " 'eliniak',\n",
       " \"'rosemary\",\n",
       " 'kusturika',\n",
       " 'stepmotherhood',\n",
       " \"'corrida\",\n",
       " 'göttland',\n",
       " \"me'style\",\n",
       " 'yamad',\n",
       " 'bevcause',\n",
       " 'sintown',\n",
       " '58minutes',\n",
       " 'stragely',\n",
       " 'psychologising',\n",
       " \"'standards\",\n",
       " 'eschnapur',\n",
       " \"'tender\",\n",
       " 'minnieapolis',\n",
       " 'melyvn',\n",
       " 'lugusi',\n",
       " 'underpanted',\n",
       " 'renoue',\n",
       " 'trashings',\n",
       " 'bouvril',\n",
       " 'retrophiles',\n",
       " 'cinemademerde',\n",
       " 'cameramancharacters',\n",
       " 'bonisseur',\n",
       " 'dmd2222',\n",
       " \"'aladdin\",\n",
       " 'thestuffblag',\n",
       " 'aymler',\n",
       " \"'oldie\",\n",
       " 'nana’s',\n",
       " 'surpremely',\n",
       " 'emannuelle',\n",
       " 'credentialdo',\n",
       " \"'well\",\n",
       " \"rock'n'roll\",\n",
       " 'streetists',\n",
       " 'condoli',\n",
       " \"'breakfast\",\n",
       " 'dprobably',\n",
       " 'iñarritu',\n",
       " 'dataeven',\n",
       " 'framke',\n",
       " \"'wendigo\",\n",
       " \"'seryozha\",\n",
       " '10eliason',\n",
       " \"lots'o'characters\",\n",
       " \"'conflict\",\n",
       " 'limpest',\n",
       " 'silverwheels',\n",
       " 'ooherh',\n",
       " \"'observe\",\n",
       " 'vividscenery',\n",
       " 'hchjnnhw',\n",
       " \"'ciao\",\n",
       " 'voorhess',\n",
       " 'woppi',\n",
       " 'phileine',\n",
       " \"'marry\",\n",
       " 'krvavac',\n",
       " 'hoojah',\n",
       " \"'gina\",\n",
       " 'peline',\n",
       " 'blahactually',\n",
       " 'tenannt',\n",
       " 'crackd',\n",
       " 'bachstage',\n",
       " 'clisché',\n",
       " 'appareantly',\n",
       " 'stopkewich',\n",
       " \"'gory\",\n",
       " \"'kiki\",\n",
       " 'soutendjik',\n",
       " 'tabanga',\n",
       " 'bogoslaw',\n",
       " \"'loving\",\n",
       " 'asawari',\n",
       " 'singin’',\n",
       " 'doxen',\n",
       " \"'kno\",\n",
       " 'trochenbrod',\n",
       " 'mittschnittservice',\n",
       " \"'chi\",\n",
       " 'lesking',\n",
       " 'wrenchmuller',\n",
       " 'thestrup',\n",
       " \"'talkie\",\n",
       " \"cant't\",\n",
       " 'treatsie',\n",
       " 'oxenby',\n",
       " 'superfluouse',\n",
       " 'iloverot',\n",
       " 'clenchingly',\n",
       " 'uncronological',\n",
       " 'eattheblinds',\n",
       " 'sheanimal',\n",
       " \"'tubes\",\n",
       " 'contemporizes',\n",
       " 'mischon',\n",
       " 'miyogi',\n",
       " 'amytiville',\n",
       " 'lundren',\n",
       " \"'french\",\n",
       " 'moskve',\n",
       " \"'coast\",\n",
       " \"'sequel\",\n",
       " \"'muppet\",\n",
       " \"'football\",\n",
       " 'fukuky',\n",
       " 'ungainfully',\n",
       " 'pempeit',\n",
       " 'patriate',\n",
       " \"'otherworldly\",\n",
       " \"'teach\",\n",
       " 'independantists',\n",
       " 'stiltedness',\n",
       " \"'pon\",\n",
       " \"'renegade\",\n",
       " 'preachie',\n",
       " 'giraurd',\n",
       " \"'divorce\",\n",
       " 'barbarellish',\n",
       " 'sandscapes',\n",
       " 'screamqueen',\n",
       " 'lkhubble2',\n",
       " 'tabonga',\n",
       " 'cajunaccent',\n",
       " 'sopkiw',\n",
       " ...}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(w for sentence in x_train for w in sentence.split() if w not in embeddings_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_quote(text):\n",
    "    return ' '.join(x[1:] if x.startswith(\"'\") and len(x) > 1 else x for x in text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [fix_quote(sentence) for sentence in x_train]\n",
    "x_test = [fix_quote(sentence) for sentence in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(text):\n",
    "    ps= PorterStemmer()\n",
    "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "def lemmatize (text):\n",
    "    lm = WordNetLemmatizer()\n",
    "    text = ' '.join([lm.lemmatize(word) for word in text.split()])\n",
    "    text = ' '.join([lm.lemmatize(word, 'v') for word in text.split()]) #verbs\n",
    "    return text\n",
    "\n",
    "def remove_stopwords( text):\n",
    "    tokens = ToktokTokenizer().tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stopword_list = stopwords.words('english')\n",
    "\n",
    "x_train = [remove_stopwords(sentence) for sentence in x_train]\n",
    "x_test = [remove_stopwords(sentence) for sentence in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = [lemmatize(sentence) for sentence in x_train]\n",
    "#x_test = [lemmatize(sentence) for sentence in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = [stemmer(sentence) for sentence in x_train]\n",
    "#x_test = [stemmer(sentence) for sentence in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train_preprocessed = x_train\n",
    "\n",
    "x_test_preprocessed = x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['text'] = df['text'].apply(lambda x: preprocesser.text_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import pickle\\n\\nos.makedirs('pickle', exist_ok=True)\\n\\nwith open('pickle\\\\data.pickle', 'wb') as f:\\n    pickle.dump([x_test, y_test], f)\\nf.close()\\n\""
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pickle\n",
    "\n",
    "os.makedirs('pickle', exist_ok=True)\n",
    "\n",
    "with open('pickle\\\\data.pickle', 'wb') as f:\n",
    "    pickle.dump([x_test, y_test], f)\n",
    "f.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_preprocessed = [preprocesser.text_preprocessing(sentence) for sentence in x_train]\n",
    "#x_test_preprocessed = [preprocesser.text_preprocessing(sentence) for sentence in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Preprocessed texts')\n",
    "#print(x_train_preprocessed[:3])\n",
    "#print(x_test_preprocessed[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['text'] = df['text'].apply(lambda x: preprocesser.text_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import pickle\\n\\nos.makedirs('pickle', exist_ok=True)\\n\\nwith open('pickle\\\\data.pickle', 'wb') as f:\\n    pickle.dump([x_test, y_test], f)\\nf.close()\""
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pickle\n",
    "\n",
    "os.makedirs('pickle', exist_ok=True)\n",
    "\n",
    "with open('pickle\\\\data.pickle', 'wb') as f:\n",
    "    pickle.dump([x_test, y_test], f)\n",
    "f.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.utils import shuffle\\n\\nx_train, x_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size = 0.33, shuffle = True)\\n\\nx_train = list(x_train)\\nx_test = list(x_test)\\n\\ny_train = list(y_train)\\ny_test = list(y_test)\\n\""
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size = 0.33, shuffle = True)\n",
    "\n",
    "x_train = list(x_train)\n",
    "x_test = list(x_test)\n",
    "\n",
    "y_train = list(y_train)\n",
    "y_test = list(y_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 87608 unique tokens.\n",
      "Shape of train data tensor: (33500, 1639)\n",
      "Shape of train label tensor: (33500,)\n",
      "Shape of test data tensor: (16500, 1639)\n",
      "Shape of test label tensor: (16500,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train_preprocessed)\n",
    "\n",
    "maxlen = max([len(t.split()) for t in x_train_preprocessed])\n",
    "\n",
    "words_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train_preprocessed)\n",
    "test_sequences = tokenizer.texts_to_sequences(x_test_preprocessed)\n",
    "\n",
    "print('Found %s unique tokens.' % len(tokenizer.word_index))\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen = maxlen)\n",
    "test_data = pad_sequences(test_sequences, maxlen = maxlen)\n",
    "\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n",
    "print('Shape of train data tensor:', train_data.shape)\n",
    "print('Shape of train label tensor:', y_train.shape)\n",
    "\n",
    "print('Shape of test data tensor:', test_data.shape)\n",
    "print('Shape of test label tensor:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import pickle\\n\\nos.makedirs('pickle', exist_ok=True)\\n\\nwith open('pickle\\\\tokenizer.pickle', 'wb') as f:\\n    pickle.dump([tokenizer, maxlen], f)\\nf.close()\""
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pickle\n",
    "\n",
    "os.makedirs('pickle', exist_ok=True)\n",
    "\n",
    "with open('pickle\\\\tokenizer.pickle', 'wb') as f:\n",
    "    pickle.dump([tokenizer, maxlen], f)\n",
    "f.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,   589,  2040, 73657],\n",
       "       [    0,     0,     0, ...,   972,   203,    68],\n",
       "       [    0,     0,     0, ...,  9901,    11,   267],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,   873,   931,   850],\n",
       "       [    0,     0,     0, ...,     5,   723,    12],\n",
       "       [    0,     0,     0, ...,  3187, 39771,   848]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%store test_data\\n%store x_test\\n%store y_test\\n'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%store test_data\n",
    "%store x_test\n",
    "%store y_test\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout, LSTM, Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=3\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.1,\n",
    "        patience=3,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fitted_model(dropout = 0.5, layer_num = 1, init_mode='uniform', batch_size = 128):\n",
    "    \n",
    "    print('\\n', f'Training Model with:', '\\n',\n",
    "    f'* dropout = {dropout};', '\\n',\n",
    "    f'* number of hidden layers = {layer_num};', '\\n',\n",
    "    f'* init mode = {init_mode};', '\\n',\n",
    "    f'* batch size = {batch_size}')\n",
    "    \n",
    "    def add_layers():\n",
    "        for i in range (0, layer_num):\n",
    "            model.add(Dense(64, kernel_initializer=init_mode, activation='relu'))\n",
    "            model.add(Dropout(rate=dropout))\n",
    "    \n",
    "    EMBEDDING_DIM = 100\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=words_size, output_dim=EMBEDDING_DIM, input_length=maxlen))\n",
    "    model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(64, kernel_initializer=init_mode, activation='relu'))\n",
    "    model.add(Dropout(rate=dropout))\n",
    "    #add_layers()\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "    history = model.fit(train_data, y_train,\n",
    "                        epochs=10,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_split=0.2,\n",
    "                        verbose = 2)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict(dropout = [0.2, 0.5, 0.65, 0.8],\n",
    "                       layer_num = [1,2,3],\n",
    "                       batch_size =[128,512],\n",
    "                       init_mode = ['uniform', 'lecun_uniform', 'normal', \n",
    "                                    'glorot_normal', 'glorot_uniform']\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Model with: \n",
      " * dropout = 0.2; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = uniform; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 37s - loss: 0.3973 - acc: 0.8232 - val_loss: 0.2714 - val_acc: 0.8948\n",
      "Epoch 2/10\n",
      "26800/26800 - 35s - loss: 0.1994 - acc: 0.9253 - val_loss: 0.3118 - val_acc: 0.8804\n",
      "Epoch 3/10\n"
     ]
    }
   ],
   "source": [
    "dict_dropout_histories = {}\n",
    "best_dropout = 0.5\n",
    "best_dropout_acc = 0\n",
    "for i in hyperparameters['dropout']:\n",
    "    history = get_fitted_model(dropout = i)\n",
    "    if max(history.history['val_acc']) > best_dropout_acc:\n",
    "        best_dropout = i\n",
    "        best_dropout_acc = max(history.history['val_acc'])\n",
    "    dict_dropout_histories[str(i)] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(dict_dropout_histories[str(best_dropout)].history['val_acc']))\n",
    "print(best_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Model with: \n",
      " * dropout = 0.65; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = uniform; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 16s - loss: 0.8018 - acc: 0.4979 - val_loss: 0.6932 - val_acc: 0.4985\n",
      "Epoch 2/10\n",
      "26800/26800 - 16s - loss: 0.5531 - acc: 0.6836 - val_loss: 0.3413 - val_acc: 0.8518\n",
      "Epoch 3/10\n",
      "26800/26800 - 16s - loss: 0.2274 - acc: 0.9158 - val_loss: 0.2581 - val_acc: 0.8967\n",
      "Epoch 4/10\n",
      "26800/26800 - 16s - loss: 0.1162 - acc: 0.9597 - val_loss: 0.2725 - val_acc: 0.8928\n",
      "Epoch 5/10\n",
      "26800/26800 - 16s - loss: 0.0463 - acc: 0.9847 - val_loss: 0.3877 - val_acc: 0.8903\n",
      "Epoch 6/10\n",
      "26800/26800 - 16s - loss: 0.0130 - acc: 0.9962 - val_loss: 0.5050 - val_acc: 0.8897\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.65; \n",
      " * number of hidden layers = 2; \n",
      " * init mode = uniform; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 17s - loss: 0.7003 - acc: 0.4987 - val_loss: 0.6931 - val_acc: 0.4985\n",
      "Epoch 2/10\n",
      "26800/26800 - 16s - loss: 0.6942 - acc: 0.5060 - val_loss: 0.6929 - val_acc: 0.5379\n",
      "Epoch 3/10\n",
      "26800/26800 - 16s - loss: 0.5125 - acc: 0.7265 - val_loss: 0.2836 - val_acc: 0.8833\n",
      "Epoch 4/10\n",
      "26800/26800 - 16s - loss: 0.2329 - acc: 0.9166 - val_loss: 0.2773 - val_acc: 0.8945\n",
      "Epoch 5/10\n",
      "26800/26800 - 16s - loss: 0.1273 - acc: 0.9559 - val_loss: 0.3511 - val_acc: 0.8860\n",
      "Epoch 6/10\n",
      "26800/26800 - 16s - loss: 0.0567 - acc: 0.9812 - val_loss: 0.5103 - val_acc: 0.8851\n",
      "Epoch 7/10\n",
      "26800/26800 - 16s - loss: 0.0291 - acc: 0.9913 - val_loss: 0.7353 - val_acc: 0.8815\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.65; \n",
      " * number of hidden layers = 3; \n",
      " * init mode = uniform; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 17s - loss: 0.6954 - acc: 0.4944 - val_loss: 0.6932 - val_acc: 0.4985\n",
      "Epoch 2/10\n",
      "26800/26800 - 16s - loss: 0.6939 - acc: 0.4994 - val_loss: 0.6931 - val_acc: 0.5036\n",
      "Epoch 3/10\n",
      "26800/26800 - 16s - loss: 0.5868 - acc: 0.6511 - val_loss: 0.3203 - val_acc: 0.8682\n",
      "Epoch 4/10\n",
      "26800/26800 - 16s - loss: 0.2730 - acc: 0.9017 - val_loss: 0.3537 - val_acc: 0.8728\n",
      "Epoch 5/10\n",
      "26800/26800 - 16s - loss: 0.1630 - acc: 0.9457 - val_loss: 0.3557 - val_acc: 0.8925\n",
      "Epoch 6/10\n",
      "26800/26800 - 16s - loss: 0.0896 - acc: 0.9719 - val_loss: 0.6244 - val_acc: 0.8670\n",
      "Epoch 7/10\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc,lr\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-8521e8186a04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbest_layer_num_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'layer_num'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_fitted_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_dropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_layer_num_acc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mbest_layer_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-103-6745fa82e05e>\u001b[0m in \u001b[0;36mget_fitted_model\u001b[1;34m(dropout, layer_num, init_mode, batch_size)\u001b[0m\n\u001b[0;32m     28\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m                         verbose = 2)\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dict_layers_num_histories = {}\n",
    "best_layer_num = 1\n",
    "best_layer_num_acc = 0\n",
    "for i in hyperparameters['layer_num']:\n",
    "    history = get_fitted_model(dropout = best_dropout, layer_num = i)\n",
    "    if max(history.history['val_acc']) > best_layer_num_acc:\n",
    "        best_layer_num = i\n",
    "        best_layer_num_acc = max(history.history['val_acc'])\n",
    "    dict_layers_num_histories[str(i)] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(dict_layers_num_histories[str(best_layer_num)].history['val_acc']))\n",
    "print(best_layer_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = uniform; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 24s - loss: 0.8725 - acc: 0.4993 - val_loss: 0.6931 - val_acc: 0.5066\n",
      "Epoch 2/10\n",
      "26800/26800 - 24s - loss: 0.7035 - acc: 0.4997 - val_loss: 0.6933 - val_acc: 0.4922\n",
      "Epoch 3/10\n",
      "26800/26800 - 24s - loss: 0.7029 - acc: 0.5096 - val_loss: 0.6906 - val_acc: 0.5739\n",
      "Epoch 4/10\n",
      "26800/26800 - 24s - loss: 0.6187 - acc: 0.6384 - val_loss: 0.3721 - val_acc: 0.8519\n",
      "Epoch 5/10\n",
      "26800/26800 - 24s - loss: 0.3225 - acc: 0.8694 - val_loss: 0.2793 - val_acc: 0.8827\n",
      "Epoch 6/10\n",
      "26800/26800 - 24s - loss: 0.1860 - acc: 0.9310 - val_loss: 0.2715 - val_acc: 0.8930\n",
      "Epoch 7/10\n",
      "26800/26800 - 24s - loss: 0.1105 - acc: 0.9613 - val_loss: 0.2745 - val_acc: 0.8999\n",
      "Epoch 8/10\n",
      "26800/26800 - 24s - loss: 0.0599 - acc: 0.9791 - val_loss: 0.4513 - val_acc: 0.8772\n",
      "Epoch 9/10\n",
      "26800/26800 - 24s - loss: 0.0308 - acc: 0.9897 - val_loss: 0.4388 - val_acc: 0.8894\n",
      "Epoch 10/10\n",
      "26800/26800 - 24s - loss: 0.0107 - acc: 0.9971 - val_loss: 0.4732 - val_acc: 0.8994\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = lecun_uniform; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 24s - loss: 0.8676 - acc: 0.5007 - val_loss: 0.6931 - val_acc: 0.5067\n",
      "Epoch 2/10\n",
      "26800/26800 - 24s - loss: 0.7023 - acc: 0.4990 - val_loss: 0.6931 - val_acc: 0.5079\n",
      "Epoch 3/10\n",
      "26800/26800 - 24s - loss: 0.6732 - acc: 0.5601 - val_loss: 0.4752 - val_acc: 0.7973\n",
      "Epoch 4/10\n",
      "26800/26800 - 24s - loss: 0.3588 - acc: 0.8523 - val_loss: 0.2722 - val_acc: 0.8890\n",
      "Epoch 5/10\n",
      "26800/26800 - 24s - loss: 0.2085 - acc: 0.9226 - val_loss: 0.2829 - val_acc: 0.8897\n",
      "Epoch 6/10\n",
      "26800/26800 - 24s - loss: 0.1311 - acc: 0.9532 - val_loss: 0.3172 - val_acc: 0.8921\n",
      "Epoch 7/10\n",
      "26800/26800 - 24s - loss: 0.0745 - acc: 0.9743 - val_loss: 0.3348 - val_acc: 0.9030\n",
      "Epoch 8/10\n",
      "26800/26800 - 24s - loss: 0.0270 - acc: 0.9929 - val_loss: 0.4006 - val_acc: 0.9030\n",
      "Epoch 9/10\n",
      "26800/26800 - 24s - loss: 0.0219 - acc: 0.9935 - val_loss: 0.3972 - val_acc: 0.9028\n",
      "Epoch 10/10\n",
      "26800/26800 - 24s - loss: 0.0197 - acc: 0.9949 - val_loss: 0.3984 - val_acc: 0.9025\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = normal; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 24s - loss: 0.8741 - acc: 0.4890 - val_loss: 0.6931 - val_acc: 0.5069\n",
      "Epoch 2/10\n",
      "26800/26800 - 24s - loss: 0.6996 - acc: 0.4953 - val_loss: 0.6932 - val_acc: 0.4930\n",
      "Epoch 3/10\n",
      "26800/26800 - 24s - loss: 0.6969 - acc: 0.5035 - val_loss: 0.6939 - val_acc: 0.5073\n",
      "Epoch 4/10\n",
      "26800/26800 - 24s - loss: 0.6961 - acc: 0.5169 - val_loss: 0.6756 - val_acc: 0.6407\n",
      "Epoch 5/10\n",
      "26800/26800 - 24s - loss: 0.5817 - acc: 0.6722 - val_loss: 0.3590 - val_acc: 0.8540\n",
      "Epoch 6/10\n",
      "26800/26800 - 24s - loss: 0.3347 - acc: 0.8586 - val_loss: 0.2635 - val_acc: 0.8936\n",
      "Epoch 7/10\n",
      "26800/26800 - 24s - loss: 0.2158 - acc: 0.9188 - val_loss: 0.2527 - val_acc: 0.9004\n",
      "Epoch 8/10\n",
      "26800/26800 - 24s - loss: 0.1321 - acc: 0.9512 - val_loss: 0.3730 - val_acc: 0.8694\n",
      "Epoch 9/10\n",
      "26800/26800 - 24s - loss: 0.0797 - acc: 0.9735 - val_loss: 0.3277 - val_acc: 0.8978\n",
      "Epoch 10/10\n",
      "26800/26800 - 24s - loss: 0.0455 - acc: 0.9840 - val_loss: 0.4259 - val_acc: 0.8937\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = glorot_normal; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 24s - loss: 0.8386 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5067\n",
      "Epoch 2/10\n",
      "26800/26800 - 24s - loss: 0.7005 - acc: 0.4992 - val_loss: 0.6931 - val_acc: 0.4933\n",
      "Epoch 3/10\n",
      "26800/26800 - 24s - loss: 0.6954 - acc: 0.5007 - val_loss: 0.6917 - val_acc: 0.5796\n",
      "Epoch 4/10\n",
      "26800/26800 - 24s - loss: 0.5697 - acc: 0.6906 - val_loss: 0.3283 - val_acc: 0.8639\n",
      "Epoch 5/10\n",
      "26800/26800 - 24s - loss: 0.2769 - acc: 0.8953 - val_loss: 0.2653 - val_acc: 0.8937\n",
      "Epoch 6/10\n",
      "26800/26800 - 24s - loss: 0.1691 - acc: 0.9377 - val_loss: 0.2682 - val_acc: 0.9051\n",
      "Epoch 7/10\n",
      "26800/26800 - 24s - loss: 0.1007 - acc: 0.9643 - val_loss: 0.3012 - val_acc: 0.8954\n",
      "Epoch 8/10\n",
      "26800/26800 - 24s - loss: 0.0533 - acc: 0.9814 - val_loss: 0.3929 - val_acc: 0.9027\n",
      "Epoch 9/10\n",
      "26800/26800 - 24s - loss: 0.0165 - acc: 0.9957 - val_loss: 0.4305 - val_acc: 0.9034\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = glorot_uniform; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 24s - loss: 0.8379 - acc: 0.4943 - val_loss: 0.6931 - val_acc: 0.5067\n",
      "Epoch 2/10\n",
      "26800/26800 - 24s - loss: 0.7035 - acc: 0.4995 - val_loss: 0.6932 - val_acc: 0.4933\n",
      "Epoch 3/10\n",
      "26800/26800 - 24s - loss: 0.7016 - acc: 0.4983 - val_loss: 0.6932 - val_acc: 0.4933\n",
      "Epoch 4/10\n",
      "26800/26800 - 24s - loss: 0.6973 - acc: 0.5198 - val_loss: 0.6604 - val_acc: 0.5230\n",
      "Epoch 5/10\n",
      "26800/26800 - 24s - loss: 0.4240 - acc: 0.8099 - val_loss: 0.2897 - val_acc: 0.8740\n",
      "Epoch 6/10\n",
      "26800/26800 - 24s - loss: 0.2310 - acc: 0.9132 - val_loss: 0.2750 - val_acc: 0.8937\n",
      "Epoch 7/10\n",
      "26800/26800 - 24s - loss: 0.1361 - acc: 0.9511 - val_loss: 0.2875 - val_acc: 0.9018\n",
      "Epoch 8/10\n",
      "26800/26800 - 24s - loss: 0.0745 - acc: 0.9737 - val_loss: 0.3974 - val_acc: 0.8937\n",
      "Epoch 9/10\n",
      "26800/26800 - 24s - loss: 0.0390 - acc: 0.9867 - val_loss: 0.4460 - val_acc: 0.8957\n",
      "Epoch 10/10\n",
      "26800/26800 - 24s - loss: 0.0123 - acc: 0.9965 - val_loss: 0.4803 - val_acc: 0.8969\n"
     ]
    }
   ],
   "source": [
    "dict_init_mode_histories = {}\n",
    "best_init_mode = 'uniform'\n",
    "best_init_mode_acc = 0\n",
    "for i in hyperparameters['init_mode']:\n",
    "    history = get_fitted_model(dropout = best_dropout, layer_num = best_layer_num, init_mode = i)\n",
    "    if max(history.history['val_acc']) > best_init_mode_acc:\n",
    "        best_init_mode = i\n",
    "        best_init_mode_acc = max(history.history['val_acc'])\n",
    "    dict_init_mode_histories[str(i)] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90507466\n",
      "glorot_normal\n"
     ]
    }
   ],
   "source": [
    "print(max(dict_init_mode_histories[str(best_init_mode)].history['val_acc']))\n",
    "print(best_init_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = glorot_normal; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 23s - loss: 0.7815 - acc: 0.4925 - val_loss: 0.6931 - val_acc: 0.5067\n",
      "Epoch 2/10\n",
      "26800/26800 - 23s - loss: 0.6961 - acc: 0.4986 - val_loss: 0.6932 - val_acc: 0.4933\n",
      "Epoch 3/10\n",
      "26800/26800 - 23s - loss: 0.6987 - acc: 0.4971 - val_loss: 0.6932 - val_acc: 0.4936\n",
      "Epoch 4/10\n",
      "26800/26800 - 23s - loss: 0.6326 - acc: 0.6016 - val_loss: 0.3993 - val_acc: 0.8261\n",
      "Epoch 5/10\n",
      "26800/26800 - 24s - loss: 0.3192 - acc: 0.8741 - val_loss: 0.2728 - val_acc: 0.8870\n",
      "Epoch 6/10\n",
      "26800/26800 - 24s - loss: 0.1895 - acc: 0.9306 - val_loss: 0.2896 - val_acc: 0.8888\n",
      "Epoch 7/10\n",
      "26800/26800 - 24s - loss: 0.1131 - acc: 0.9593 - val_loss: 0.2787 - val_acc: 0.9016\n",
      "Epoch 8/10\n",
      "26800/26800 - 24s - loss: 0.0637 - acc: 0.9787 - val_loss: 0.4394 - val_acc: 0.8828\n",
      "Epoch 9/10\n",
      "26800/26800 - 24s - loss: 0.0228 - acc: 0.9936 - val_loss: 0.4159 - val_acc: 0.8985\n",
      "Epoch 10/10\n",
      "26800/26800 - 24s - loss: 0.0200 - acc: 0.9950 - val_loss: 0.4215 - val_acc: 0.8969\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = glorot_normal; \n",
      " * batch size = 512\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 17s - loss: 1.4291 - acc: 0.4974 - val_loss: 0.6932 - val_acc: 0.4934\n",
      "Epoch 2/10\n",
      "26800/26800 - 16s - loss: 0.7129 - acc: 0.4940 - val_loss: 0.6932 - val_acc: 0.4933\n",
      "Epoch 3/10\n",
      "26800/26800 - 16s - loss: 0.7010 - acc: 0.4990 - val_loss: 0.6932 - val_acc: 0.4933\n",
      "Epoch 4/10\n",
      "26800/26800 - 17s - loss: 0.6934 - acc: 0.4967 - val_loss: 0.6932 - val_acc: 0.4933\n"
     ]
    }
   ],
   "source": [
    "dict_batch_size_histories = {}\n",
    "best_batch_size = 128\n",
    "best_batch_size_acc = 0\n",
    "for i in hyperparameters['batch_size']:\n",
    "    history = get_fitted_model(dropout = best_dropout, layer_num = best_layer_num, \n",
    "                              init_mode = best_init_mode, batch_size = i)\n",
    "    if max(history.history['val_acc']) > best_batch_size_acc:\n",
    "        best_batch_size = i\n",
    "        best_batch_size_acc = max(history.history['val_acc'])\n",
    "    dict_batch_size_histories[str(i)] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9016418\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "print(max(dict_batch_size_histories[str(best_batch_size)].history['val_acc']))\n",
    "print(best_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "callbacks_list.append(\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath= 'models\\\\best_model_redone.h5',\n",
    "        save_weights_only=False,\n",
    "        monitor='val_acc',\n",
    "        save_best_only=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 25s - loss: 0.8556 - acc: 0.4916 - val_loss: 0.6931 - val_acc: 0.5067\n",
      "Epoch 2/10\n",
      "26800/26800 - 24s - loss: 0.6972 - acc: 0.5008 - val_loss: 0.6932 - val_acc: 0.4933\n",
      "Epoch 3/10\n",
      "26800/26800 - 24s - loss: 0.6977 - acc: 0.4999 - val_loss: 0.6932 - val_acc: 0.4933\n",
      "Epoch 4/10\n",
      "26800/26800 - 24s - loss: 0.7023 - acc: 0.5109 - val_loss: 0.6759 - val_acc: 0.5087\n",
      "Epoch 5/10\n",
      "26800/26800 - 24s - loss: 0.4609 - acc: 0.7788 - val_loss: 0.3065 - val_acc: 0.8687\n",
      "Epoch 6/10\n",
      "26800/26800 - 24s - loss: 0.2361 - acc: 0.9093 - val_loss: 0.2786 - val_acc: 0.8904\n",
      "Epoch 7/10\n",
      "26800/26800 - 24s - loss: 0.1415 - acc: 0.9491 - val_loss: 0.2854 - val_acc: 0.8991\n",
      "Epoch 8/10\n",
      "26800/26800 - 24s - loss: 0.0774 - acc: 0.9730 - val_loss: 0.3776 - val_acc: 0.8984\n",
      "Epoch 9/10\n",
      "26800/26800 - 24s - loss: 0.0394 - acc: 0.9865 - val_loss: 0.4137 - val_acc: 0.8973\n",
      "Epoch 10/10\n",
      "26800/26800 - 24s - loss: 0.0140 - acc: 0.9965 - val_loss: 0.4750 - val_acc: 0.8979\n"
     ]
    }
   ],
   "source": [
    "def get_best_model(dropout = 0.5, layer_num = 1, init_mode='uniform', batch_size = 128):\n",
    "\n",
    "    def add_layers():\n",
    "        for i in range (0, layer_num):\n",
    "            model.add(Dense(64, kernel_initializer=init_mode, activation='relu'))\n",
    "            model.add(Dropout(rate=dropout))\n",
    "    \n",
    "    EMBEDDING_DIM = 100\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(words_size, EMBEDDING_DIM, input_length=maxlen))\n",
    "    model.add(Flatten())\n",
    "    add_layers()\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(words_size, EMBEDDING_DIM, input_length=maxlen))\n",
    "    model.add(Flatten())\n",
    "    add_layers()\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "    history = model.fit(train_data, y_train,\n",
    "                        epochs=10,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_split=0.2,\n",
    "                        verbose=2)\n",
    "    #model.load_weights('./models/best_model.h5')\n",
    "    \n",
    "    #return model\n",
    "    return tf.keras.models.load_model(\"models\\\\best_model_redone.h5\" )\n",
    "\n",
    "best_model = get_best_model(dropout = best_dropout, layer_num = best_layer_num, \n",
    "                            init_mode = best_init_mode, batch_size = best_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500/16500 [==============================] - 5s 280us/sample - loss: 0.2882 - acc: 0.9032\n",
      "accuracy: 0.90321213%\n"
     ]
    }
   ],
   "source": [
    "#Testing the accuracy of the model\n",
    "\n",
    "test_result = best_model.evaluate(test_data, y_test)\n",
    "\n",
    "print ('accuracy: ' + str(test_result[1]) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16500, 2640)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tf.keras.models.load_model(\"models\\\\best_model_redone.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500/16500 [==============================] - 5s 279us/sample - loss: 0.2882 - acc: 0.9032s - loss: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28816987042354814, 0.90321213]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(test_data, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the black box algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('scripts', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/blackBox.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/blackBox.py\n",
    "\n",
    "import tensorflow as tf\n",
    "from scripts.preprocessing import Preprocesser\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "class BlackBox:\n",
    "    \n",
    "    def __init__(self):\n",
    "        with open('pickle\\\\tokenizer.pickle', 'rb') as f:\n",
    "            tokenizer, maxlen = pickle.load(f)\n",
    "            self.__tokenizer = tokenizer\n",
    "            self.__maxlen = maxlen\n",
    "        f.close()\n",
    "        self.__model = tf.keras.models.load_model(\"models\\\\best_model.h5\")\n",
    "        \n",
    "    def __text_preprocessing(self, text):\n",
    "        return Preprocesser.text_preprocessing(text)      \n",
    "        \n",
    "    def __tokenize(self, text):\n",
    "        sequences = self.__tokenizer.texts_to_sequences(text)\n",
    "        return pad_sequences(sequences, maxlen = self.__maxlen)\n",
    "        \n",
    "    def predict_sentiment(self, text):\n",
    "        text = self.__text_preprocessing(text)\n",
    "        seq = self.__tokenize([text])\n",
    "        return self.__model.predict(seq).take(0)\n",
    "    \n",
    "    def evaluate(self, test, label):\n",
    "        self.__model.evaluate(test,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.blackBox import BlackBox\n",
    "\n",
    "#import scripts.blackBox as blackbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "black_box = BlackBox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('pickle\\\\data.pickle', 'rb') as f:\n",
    "    x_test, y_test = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#black_box.evaluate(test_data, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[y_test[7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8256238"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "black_box.predict_sentiment(x_test[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
