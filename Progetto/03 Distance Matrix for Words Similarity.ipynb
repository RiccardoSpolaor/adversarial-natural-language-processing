{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.seed_setter import set_seed\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,os\n",
    "\n",
    "with open(os.path.join('./pickle_data/train_test_data/test_data.pickle'), 'rb') as f:\n",
    "    x_test, y_test = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISTANCE MATRIX CALCULATION\n",
    "\n",
    "```\n",
    "using counter fitted word vectors\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "embeddings_dict = {}\n",
    "\n",
    "\n",
    "with open(os.path.join(\"./counter_fitted_word_vectors/counter-fitted-vectors.txt\"), \"r\",errors ='ignore', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_dict[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ PREPROCESSING #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "latin_similar = \"’'‘ÆÐƎƏƐƔĲŊŒẞÞǷȜæðǝəɛɣĳŋœĸſßþƿȝĄƁÇĐƊĘĦĮƘŁØƠŞȘŢȚŦŲƯY̨Ƴąɓçđɗęħįƙłøơşșţțŧųưy̨ƴÁÀÂÄǍĂĀÃÅǺĄÆǼǢƁĆĊĈČÇĎḌĐƊÐÉÈĖÊËĚĔĒĘẸƎƏƐĠĜǦĞĢƔáàâäǎăāãåǻąæǽǣɓćċĉčçďḍđɗðéèėêëěĕēęẹǝəɛġĝǧğģɣĤḤĦIÍÌİÎÏǏĬĪĨĮỊĲĴĶƘĹĻŁĽĿʼNŃN̈ŇÑŅŊÓÒÔÖǑŎŌÕŐỌØǾƠŒĥḥħıíìiîïǐĭīĩįịĳĵķƙĸĺļłľŀŉńn̈ňñņŋóòôöǒŏōõőọøǿơœŔŘŖŚŜŠŞȘṢẞŤŢṬŦÞÚÙÛÜǓŬŪŨŰŮŲỤƯẂẀŴẄǷÝỲŶŸȲỸƳŹŻŽẒŕřŗſśŝšşșṣßťţṭŧþúùûüǔŭūũűůųụưẃẁŵẅƿýỳŷÿȳỹƴźżžẓ\"\n",
    "safe_characters = string.ascii_letters + string.digits + latin_similar + ' '\n",
    "safe_characters += \"'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non latin symbols in the Counter Fitted GloVe Embeddings:\n",
      "['º', 'ª', 'с', 'δ', 'в', 'и']\n"
     ]
    }
   ],
   "source": [
    "glove_chars = [c for c in list(embeddings_dict.keys()) if len(c) == 1]\n",
    "glove_symbols = [c for c in glove_chars if not c in safe_characters]\n",
    "print('Non latin symbols in the Counter Fitted GloVe Embeddings:')\n",
    "print(glove_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non latin symbols in the Train Data:\n",
      "['\\x80', '^', '¦', '·', '”', '–', '\\\\', 'א', '!', 'ן', 'ו', '«', '=', ';', '\\t', '~', '¾', 'כ', '\\xad', 'ל', '³', '$', 'ג', ')', '¿', '?', '\"', '|', '\\x91', 'ר', '}', '>', '.', '&', ':', '*', ']', '@', '▼', 'º', '§', '#', '\\xa0', '+', '“', '(', '\\x97', '¨', '%', '°', '\\x95', '\\x96', '<', 'י', '\\x84', '/', '…', '_', ',', '\\x85', '½', '»', '{', '¢', '¡', '`', '£', '´', '-', '[', 'מ']\n"
     ]
    }
   ],
   "source": [
    "test_chars = set(w for sentence in x_test for w in sentence)\n",
    "test_symbols = [c for c in test_chars if not c in safe_characters]\n",
    "print('Non latin symbols in the Train Data:')\n",
    "print(test_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbols to delete in the Train Data:\n",
      "['\\x80', '^', '¦', '·', '”', '–', '\\\\', 'א', '!', 'ן', 'ו', '«', '=', ';', '\\t', '~', '¾', 'כ', '\\xad', 'ל', '³', '$', 'ג', ')', '¿', '?', '\"', '|', '\\x91', 'ר', '}', '>', '.', '&', ':', '*', ']', '@', '▼', '§', '#', '\\xa0', '+', '“', '(', '\\x97', '¨', '%', '°', '\\x95', '\\x96', '<', 'י', '\\x84', '/', '…', '_', ',', '\\x85', '½', '»', '{', '¢', '¡', '`', '£', '´', '-', '[', 'מ']\n"
     ]
    }
   ],
   "source": [
    "symbols_to_delete = [c for c in test_symbols if not c in glove_symbols]\n",
    "print('Symbols to delete in the Train Data:')\n",
    "print(symbols_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbols to isolate in the Train Data:\n",
      "['º']\n"
     ]
    }
   ],
   "source": [
    "symbols_to_isolate = [c for c in test_symbols if c in glove_symbols]\n",
    "print('Symbols to isolate in the Train Data:')\n",
    "print(symbols_to_isolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_symbols(text):\n",
    "    for symbol in symbols_to_delete:\n",
    "        text = text.replace(symbol, ' ')\n",
    "    for symbol in symbols_to_isolate:\n",
    "        text = text.replace(symbol, ' ' + symbol + ' ')\n",
    "    return text\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "\n",
    "\n",
    "def handle_contractions(text):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    text = tokenizer.tokenize(text)\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "def fix_quote(text):\n",
    "    return ' '.join(w[1:] if w.startswith(\"'\") and len(w) > 1 else w for w in text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_preprocessing_for_tokenization(text, embeddings_dict):\n",
    "    text = text.lower()\n",
    "    text = handle_symbols(text)\n",
    "    text = handle_contractions(text)\n",
    "    text = fix_quote(text)\n",
    "    words_in_embedding_dict = [w for w in text.split() if w in embeddings_dict.keys()]\n",
    "    text = ' '.join(words_in_embedding_dict)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%%writefile ./utils/attack_preprocessing.py\\n\\nimport pickle\\nfrom nltk.tokenize.treebank import TreebankWordTokenizer\\n\\nclass AttackPreprocesser(object):\\n    \\n    def __init__(self):\\n        with open(\\'./pickle_data/preprocesser_utils/utils.pickle\\', \\'rb\\') as f:\\n            symbols_to_delete, symbols_to_isolate = pickle.load(f)\\n            self.__symbols_to_delete = symbols_to_delete\\n            self.__symbols_to_isolate = symbols_to_isolate\\n        f.close()\\n        self.__tree_bank_word_tokenizer = TreebankWordTokenizer()\\n    \\n    def __handle_symbols(self, text):\\n        for symbol in self.__symbols_to_delete:\\n            text = text.replace(symbol, \\' \\')\\n        for symbol in self.__symbols_to_isolate:\\n            text = text.replace(symbol, \\' \\' + symbol + \\' \\')\\n        return text\\n    \\n    def __handle_contractions(self, text):\\n        text = self.__tree_bank_word_tokenizer.tokenize(text)\\n        return \\' \\'.join(text)\\n    \\n    def __fix_quotes(self, text):\\n        return \\' \\'.join(w[1:] if w.startswith(\"\\'\") and len(w) > 1 else w for w in text.split())\\n        \\n    def preprocess_text(self, text):\\n        text = text.lower()\\n        text = self.__handle_symbols(text)\\n        text = self.__handle_contractions(text)\\n        text = self.__fix_quotes(text)\\n        return text\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%%writefile ./utils/attack_preprocessing.py\n",
    "\n",
    "import pickle\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "\n",
    "class AttackPreprocesser(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        with open('./pickle_data/preprocesser_utils/utils.pickle', 'rb') as f:\n",
    "            symbols_to_delete, symbols_to_isolate = pickle.load(f)\n",
    "            self.__symbols_to_delete = symbols_to_delete\n",
    "            self.__symbols_to_isolate = symbols_to_isolate\n",
    "        f.close()\n",
    "        self.__tree_bank_word_tokenizer = TreebankWordTokenizer()\n",
    "    \n",
    "    def __handle_symbols(self, text):\n",
    "        for symbol in self.__symbols_to_delete:\n",
    "            text = text.replace(symbol, ' ')\n",
    "        for symbol in self.__symbols_to_isolate:\n",
    "            text = text.replace(symbol, ' ' + symbol + ' ')\n",
    "        return text\n",
    "    \n",
    "    def __handle_contractions(self, text):\n",
    "        text = self.__tree_bank_word_tokenizer.tokenize(text)\n",
    "        return ' '.join(text)\n",
    "    \n",
    "    def __fix_quotes(self, text):\n",
    "        return ' '.join(w[1:] if w.startswith(\"'\") and len(w) > 1 else w for w in text.split())\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = self.__handle_symbols(text)\n",
    "        text = self.__handle_contractions(text)\n",
    "        text = self.__fix_quotes(text)\n",
    "        return text\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40567\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer() #(MAXLEN)\n",
    "\n",
    "tokenizer.fit_on_texts([test_data_preprocessing_for_tokenization(text, embeddings_dict) for text in x_test])\n",
    "\n",
    "print(len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef words_in_embedding(embeddings_dict, tokenizer):\\n    elem = []\\n    for w in tokenizer.word_index.keys():\\n        if w not in embeddings_dict.keys():\\n            elem += [w]\\n    print(elem)\\n\\nwords_in_embedding(embeddings_dict, tokenizer)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def words_in_embedding(embeddings_dict, tokenizer):\n",
    "    elem = []\n",
    "    for w in tokenizer.word_index.keys():\n",
    "        if w not in embeddings_dict.keys():\n",
    "            elem += [w]\n",
    "    print(elem)\n",
    "\n",
    "words_in_embedding(embeddings_dict, tokenizer)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fawn',\n",
       " 'schlegel',\n",
       " 'tilton',\n",
       " 'clotted',\n",
       " 'trawling',\n",
       " 'kalmar',\n",
       " 'tasos',\n",
       " 'canes',\n",
       " 'sprague',\n",
       " 'brockton',\n",
       " 'mutinies',\n",
       " 'vano',\n",
       " 'crossbar',\n",
       " 'hermano',\n",
       " 'jemmy',\n",
       " 'grenadiers',\n",
       " 'stipulate',\n",
       " 'capoeira',\n",
       " 'broward',\n",
       " 'caramels',\n",
       " 'chameleons',\n",
       " 'asami',\n",
       " 'immunities',\n",
       " 'fuera',\n",
       " 'thrace',\n",
       " 'kublai',\n",
       " 'gaskets',\n",
       " 'snuggles',\n",
       " 'splendiferous',\n",
       " 'scraper',\n",
       " 'ffor',\n",
       " 'deadheads',\n",
       " 'selassie',\n",
       " 'centimeter',\n",
       " 'opportunists',\n",
       " 'warmongering',\n",
       " 'numeral',\n",
       " 'widget',\n",
       " 'zlotys',\n",
       " 'chine',\n",
       " 'chino',\n",
       " 'sheung',\n",
       " 'quart',\n",
       " 'naturel',\n",
       " 'kumbaya',\n",
       " 'kido',\n",
       " 'millimetres',\n",
       " 'topography',\n",
       " 'jäger',\n",
       " 'battista',\n",
       " 'ramstein',\n",
       " 'caned',\n",
       " 'grahams',\n",
       " 'excu',\n",
       " 'borstal',\n",
       " 'hermana',\n",
       " 'expeditionary',\n",
       " 'unpack',\n",
       " 'murchison',\n",
       " 'lomax',\n",
       " 'matilde',\n",
       " 'zinnias',\n",
       " 'hyatt',\n",
       " 'wudang',\n",
       " 'pooper',\n",
       " 'pinta',\n",
       " 'carew',\n",
       " 'rayon',\n",
       " 'cocksucker',\n",
       " 'mcmuffin',\n",
       " 'sugarless',\n",
       " 'clews',\n",
       " 'cutback',\n",
       " 'essie',\n",
       " 'canaries',\n",
       " 'shaitan',\n",
       " 'stoller',\n",
       " 'pigment',\n",
       " 'domed',\n",
       " 'souci',\n",
       " 'amaya',\n",
       " 'tulio',\n",
       " 'farmlands',\n",
       " 'disengaging',\n",
       " 'kuen',\n",
       " 'peux',\n",
       " 'assing',\n",
       " 'patricide',\n",
       " 'peut',\n",
       " 'airbags',\n",
       " 'oceana',\n",
       " 'activating',\n",
       " 'avondale',\n",
       " 'burleigh',\n",
       " 'fiv',\n",
       " 'fiy',\n",
       " 'fib',\n",
       " 'fif',\n",
       " 'fie',\n",
       " 'menachem',\n",
       " 'fii',\n",
       " 'fio',\n",
       " 'fil',\n",
       " 'bristle',\n",
       " 'foolin',\n",
       " 'vouchers',\n",
       " 'hypoxia',\n",
       " 'preemie',\n",
       " 'être',\n",
       " 'defecate',\n",
       " 'bartok',\n",
       " 'mcgonagall',\n",
       " 'lucero',\n",
       " 'klinger',\n",
       " 'arroz',\n",
       " 'platelets',\n",
       " 'phoenicians',\n",
       " 'grapefruits',\n",
       " 'veracruz',\n",
       " 'stabilised',\n",
       " 'buttercups',\n",
       " 'smelters',\n",
       " 'whaddya',\n",
       " 'indiscretion',\n",
       " 'mazur',\n",
       " 'constants',\n",
       " 'nastya',\n",
       " 'guitarists',\n",
       " 'frenchie',\n",
       " 'nightdress',\n",
       " 'fansub',\n",
       " 'berwick',\n",
       " 'chickamauga',\n",
       " 'parkson',\n",
       " 'chiara',\n",
       " 'ministries',\n",
       " 'miaow',\n",
       " 'mendacity',\n",
       " 'curiouser',\n",
       " 'suhani',\n",
       " 'mesmer',\n",
       " 'maladies',\n",
       " 'kashima',\n",
       " 'chevron',\n",
       " 'dupuis',\n",
       " 'antilles',\n",
       " 'viets',\n",
       " 'reuben',\n",
       " 'olda',\n",
       " 'caitlyn',\n",
       " 'cisterns',\n",
       " 'aaaahhh',\n",
       " 'knowwhere',\n",
       " 'awacs',\n",
       " 'regulator',\n",
       " 'shampoos',\n",
       " 'dozer',\n",
       " 'whitcomb',\n",
       " 'sukie',\n",
       " 'longo',\n",
       " 'resignations',\n",
       " 'soave',\n",
       " 'comandante',\n",
       " 'britches',\n",
       " 'workmates',\n",
       " 'greenback',\n",
       " 'concedes',\n",
       " 'sugarcane',\n",
       " 'volkan',\n",
       " 'baldy',\n",
       " 'purges',\n",
       " 'maja',\n",
       " 'ragnarok',\n",
       " 'nettle',\n",
       " 'cumbia',\n",
       " 'ofhis',\n",
       " 'trompe',\n",
       " 'hereford',\n",
       " 'terese',\n",
       " 'halpern',\n",
       " 'locos',\n",
       " 'fennel',\n",
       " 'ulcer',\n",
       " 'chelios',\n",
       " 'faure',\n",
       " 'bullshitted',\n",
       " 'excavator',\n",
       " 'pianissimo',\n",
       " 'kerns',\n",
       " 'pawel',\n",
       " 'neighs',\n",
       " 'hoeing',\n",
       " 'venez',\n",
       " 'plata',\n",
       " 'puchi',\n",
       " 'umbilical',\n",
       " 'firenze',\n",
       " 'iate',\n",
       " 'mcnab',\n",
       " 'chyron',\n",
       " 'platz',\n",
       " 'onight',\n",
       " 'goomba',\n",
       " 'conant',\n",
       " 'cohan',\n",
       " 'heirloom',\n",
       " 'clarified',\n",
       " 'tlhe',\n",
       " 'sudanese',\n",
       " 'pinot',\n",
       " 'eriko',\n",
       " 'lalita',\n",
       " 'youngstown',\n",
       " 'irr',\n",
       " 'irv',\n",
       " 'irn',\n",
       " 'conductive',\n",
       " 'lota',\n",
       " 'awway',\n",
       " 'overfed',\n",
       " 'pharisee',\n",
       " 'gwyn',\n",
       " 'tendons',\n",
       " 'flathead',\n",
       " 'airflow',\n",
       " 'miyake',\n",
       " 'lookit',\n",
       " 'windham',\n",
       " 'lookie',\n",
       " 'fondled',\n",
       " 'charnel',\n",
       " 'blondi',\n",
       " 'godhead',\n",
       " 'fra',\n",
       " 'fre',\n",
       " 'fri',\n",
       " 'frm',\n",
       " 'sommelier',\n",
       " 'tyr',\n",
       " 'sextus',\n",
       " 'speedin',\n",
       " 'spic',\n",
       " 'davi',\n",
       " 'kennan',\n",
       " 'zalman',\n",
       " 'contingencies',\n",
       " 'droppin',\n",
       " 'majid',\n",
       " 'unos',\n",
       " 'taxicabs',\n",
       " 'ditching',\n",
       " 'kohl',\n",
       " 'kayama',\n",
       " 'preachin',\n",
       " 'memoriam',\n",
       " 'honk',\n",
       " 'codename',\n",
       " 'dunkirk',\n",
       " 'altas',\n",
       " 'myocardial',\n",
       " 'cloquet',\n",
       " 'featherweight',\n",
       " 'chidori',\n",
       " 'guaran',\n",
       " 'kamini',\n",
       " 'smallness',\n",
       " 'tomkins',\n",
       " 'lonelier',\n",
       " 'dirtying',\n",
       " 'haberdashery',\n",
       " 'spassky',\n",
       " 'maketh',\n",
       " 'lassi',\n",
       " 'brandish',\n",
       " 'haa',\n",
       " 'hab',\n",
       " 'insubordination',\n",
       " 'haf',\n",
       " 'brays',\n",
       " 'hau',\n",
       " 'sulfa',\n",
       " 'osman',\n",
       " 'dagan',\n",
       " 'lron',\n",
       " 'clustered',\n",
       " 'gade',\n",
       " 'lacroix',\n",
       " 'misdemeanors',\n",
       " 'trudie',\n",
       " 'bboys',\n",
       " 'postmaster',\n",
       " 'proportionally',\n",
       " 'deflection',\n",
       " 'tooken',\n",
       " 'altoona',\n",
       " 'choctaw',\n",
       " 'lockdown',\n",
       " 'varicose',\n",
       " 'christabel',\n",
       " 'laci',\n",
       " 'considerin',\n",
       " 'ades',\n",
       " 'anomalous',\n",
       " 'oneness',\n",
       " 'adel',\n",
       " 'adem',\n",
       " 'lemmings',\n",
       " 'quaaludes',\n",
       " 'ginevra',\n",
       " 'bonsoir',\n",
       " 'mangos',\n",
       " 'talc',\n",
       " 'addington',\n",
       " 'handmaidens',\n",
       " 'tama',\n",
       " 'raper',\n",
       " 'avocados',\n",
       " 'contusions',\n",
       " 'bogies',\n",
       " 'verde',\n",
       " 'perfumed',\n",
       " 'bridgework',\n",
       " 'paneled',\n",
       " 'rousted',\n",
       " 'arrays',\n",
       " 'musik',\n",
       " 'layin',\n",
       " 'baze',\n",
       " 'smasher',\n",
       " 'servings',\n",
       " 'azt',\n",
       " 'moderne',\n",
       " 'minibar',\n",
       " 'megahertz',\n",
       " 'wacken',\n",
       " 'minar',\n",
       " 'minas',\n",
       " 'roly',\n",
       " 'oxygenated',\n",
       " 'schemin',\n",
       " 'rolo',\n",
       " 'tunas',\n",
       " 'suwon',\n",
       " 'moresby',\n",
       " 'devoe',\n",
       " 'hawker',\n",
       " 'eilat',\n",
       " 'aomori',\n",
       " 'cootchie',\n",
       " 'ordination',\n",
       " 'candiru',\n",
       " 'steelhead',\n",
       " 'rickson',\n",
       " 'chaim',\n",
       " 'disassociate',\n",
       " 'bandito',\n",
       " 'osu',\n",
       " 'seabees',\n",
       " 'sigismund',\n",
       " 'oso',\n",
       " 'supervises',\n",
       " 'ose',\n",
       " 'macht',\n",
       " 'amplification',\n",
       " 'netherworlds',\n",
       " 'unbeaten',\n",
       " 'precognition',\n",
       " 'silencers',\n",
       " 'laundryman',\n",
       " 'flatbed',\n",
       " 'lard',\n",
       " 'olympus',\n",
       " 'knapsack',\n",
       " 'southpaw',\n",
       " 'tangina',\n",
       " 'catalonia',\n",
       " 'homebody',\n",
       " 'refueled',\n",
       " 'batsmen',\n",
       " 'lapierre',\n",
       " 'adorno',\n",
       " 'sangha',\n",
       " 'madames',\n",
       " 'lengthened',\n",
       " 'thrumming',\n",
       " 'corazon',\n",
       " 'swizzle',\n",
       " 'gome',\n",
       " 'baggins',\n",
       " 'casse',\n",
       " 'azusa',\n",
       " 'coccyx',\n",
       " 'colson',\n",
       " 'vanderbilt',\n",
       " 'dusts',\n",
       " 'shamelessness',\n",
       " 'greely',\n",
       " 'cette',\n",
       " 'timbuktu',\n",
       " 'halverson',\n",
       " 'sprouted',\n",
       " 'abnormals',\n",
       " 'stencil',\n",
       " 'brewin',\n",
       " 'giraud',\n",
       " 'tsubaki',\n",
       " 'moseying',\n",
       " 'kimiko',\n",
       " 'moonrise',\n",
       " 'umph',\n",
       " 'swamiji',\n",
       " 'oldham',\n",
       " 'awaywith',\n",
       " 'callendar',\n",
       " 'pavilions',\n",
       " 'perishes',\n",
       " 'dampness',\n",
       " 'nembutal',\n",
       " 'avaunt',\n",
       " 'titanium',\n",
       " 'gregoire',\n",
       " 'genki',\n",
       " 'osage',\n",
       " 'locka',\n",
       " 'mailboxes',\n",
       " 'matzo',\n",
       " 'lookouts',\n",
       " 'locky',\n",
       " 'byung',\n",
       " 'litre',\n",
       " 'oswaldo',\n",
       " 'thermodynamics',\n",
       " 'meddler',\n",
       " 'ilia',\n",
       " 'atac',\n",
       " 'echidna',\n",
       " 'aici',\n",
       " 'mangy',\n",
       " 'remotes',\n",
       " 'forebears',\n",
       " 'wastepaper',\n",
       " 'professore',\n",
       " 'quarrelled',\n",
       " 'tula',\n",
       " 'jie',\n",
       " 'infirmities',\n",
       " 'tull',\n",
       " 'jiu',\n",
       " 'antipathy',\n",
       " 'relapsed',\n",
       " 'milker',\n",
       " 'dingleberry',\n",
       " 'shhh',\n",
       " 'frontage',\n",
       " 'sonal',\n",
       " 'hickory',\n",
       " 'akbar',\n",
       " 'deads',\n",
       " 'dido',\n",
       " 'didi',\n",
       " 'pealing',\n",
       " 'recharged',\n",
       " 'instability',\n",
       " 'withyou',\n",
       " 'savarin',\n",
       " 'commuting',\n",
       " 'prudently',\n",
       " 'keizer',\n",
       " 'vipassana',\n",
       " 'salads',\n",
       " 'rajni',\n",
       " 'isidro',\n",
       " 'hibernate',\n",
       " 'tienes',\n",
       " 'svalbard',\n",
       " 'workmanship',\n",
       " 'brasil',\n",
       " 'marlo',\n",
       " 'thurmond',\n",
       " 'marly',\n",
       " 'draggin',\n",
       " 'hemorrhoids',\n",
       " 'onn',\n",
       " 'onl',\n",
       " 'ony',\n",
       " 'mignon',\n",
       " 'veldt',\n",
       " 'ont',\n",
       " 'penitence',\n",
       " 'tï',\n",
       " 'magnifique',\n",
       " 'herptiles',\n",
       " 'cutty',\n",
       " 'cerebro',\n",
       " 'almond',\n",
       " 'portside',\n",
       " 'hoyle',\n",
       " 'boxin',\n",
       " 'southgate',\n",
       " 'jewelz',\n",
       " 'ishida',\n",
       " 'auroras',\n",
       " 'erudition',\n",
       " 'leeching',\n",
       " 'pimply',\n",
       " 'zoc',\n",
       " 'pekingese',\n",
       " 'zog',\n",
       " 'zon',\n",
       " 'jahn',\n",
       " 'zandra',\n",
       " 'ointments',\n",
       " 'casely',\n",
       " 'printer',\n",
       " 'dinghies',\n",
       " 'brioni',\n",
       " 'amby',\n",
       " 'unpronounceable',\n",
       " 'phin',\n",
       " 'estan',\n",
       " 'messier',\n",
       " 'yucatan',\n",
       " 'whirlybird',\n",
       " 'bellefontaine',\n",
       " 'redirected',\n",
       " 'rihana',\n",
       " 'jewry',\n",
       " 'ooohhh',\n",
       " 'stimulator',\n",
       " 'thoug',\n",
       " 'anshu',\n",
       " 'kools',\n",
       " 'subsidiary',\n",
       " 'qoo',\n",
       " 'transact',\n",
       " 'presse',\n",
       " 'estar',\n",
       " 'ambi',\n",
       " 'hermanos',\n",
       " 'platts',\n",
       " 'balconies',\n",
       " 'rejoices',\n",
       " 'mechanization',\n",
       " 'gerbils',\n",
       " 'timeyou',\n",
       " 'pims',\n",
       " 'bathwater',\n",
       " 'tiko',\n",
       " 'tika',\n",
       " 'homeopathy',\n",
       " 'ramblers',\n",
       " 'zuzana',\n",
       " 'scalia',\n",
       " 'youself',\n",
       " 'kisaragi',\n",
       " 'vaccines',\n",
       " 'yumi',\n",
       " 'kilometer',\n",
       " 'dunleavy',\n",
       " 'moony',\n",
       " 'punta',\n",
       " 'tsun',\n",
       " 'marietta',\n",
       " 'circumference',\n",
       " 'uou',\n",
       " 'dorkus',\n",
       " 'willets',\n",
       " 'steinmetz',\n",
       " 'drogo',\n",
       " 'kaputt',\n",
       " 'lemoyne',\n",
       " 'baden',\n",
       " 'harumi',\n",
       " 'gaiety',\n",
       " 'buoyancy',\n",
       " 'borgen',\n",
       " 'practises',\n",
       " 'deuteronomy',\n",
       " 'offworld',\n",
       " 'wesa',\n",
       " 'spyglass',\n",
       " 'mekong',\n",
       " 'gget',\n",
       " 'wanta',\n",
       " 'wilco',\n",
       " 'tomer',\n",
       " 'runnin',\n",
       " 'kimura',\n",
       " 'annabeth',\n",
       " 'iver',\n",
       " 'defeatism',\n",
       " 'snuffbox',\n",
       " 'mutineers',\n",
       " 'vuk',\n",
       " 'hermaphrodite',\n",
       " 'hollinger',\n",
       " 'gelding',\n",
       " 'yessir',\n",
       " 'merovingian',\n",
       " 'aitch',\n",
       " 'curi',\n",
       " 'tolar',\n",
       " 'marella',\n",
       " 'profiteer',\n",
       " 'agronomist',\n",
       " 'teepee',\n",
       " 'basie',\n",
       " 'sixer',\n",
       " 'sunnyvale',\n",
       " 'benzi',\n",
       " 'steamed',\n",
       " 'recycles',\n",
       " 'recycler',\n",
       " 'ngos',\n",
       " 'unknowable',\n",
       " 'wank',\n",
       " 'bannisters',\n",
       " 'plunderers',\n",
       " 'plumed',\n",
       " 'kiri',\n",
       " 'pipelines',\n",
       " 'plumes',\n",
       " 'kaveri',\n",
       " 'dicks',\n",
       " 'unerring',\n",
       " 'sportsmanship',\n",
       " 'assemblyman',\n",
       " 'penta',\n",
       " 'ayahuasca',\n",
       " 'rajput',\n",
       " 'sumi',\n",
       " 'fitzsimmons',\n",
       " 'suma',\n",
       " 'babylonian',\n",
       " 'pomoc',\n",
       " 'homeroom',\n",
       " 'futch',\n",
       " 'pleats',\n",
       " 'tuula',\n",
       " 'goosey',\n",
       " 'grub',\n",
       " 'gooses',\n",
       " 'magma',\n",
       " 'ezo',\n",
       " 'horsewhipped',\n",
       " 'rottweilers',\n",
       " 'yesss',\n",
       " 'scuffed',\n",
       " 'pentathlon',\n",
       " 'transmutation',\n",
       " 'timeout',\n",
       " 'jahan',\n",
       " 'fareed',\n",
       " 'palmdale',\n",
       " 'majoring',\n",
       " 'unmannerly',\n",
       " 'whackin',\n",
       " 'leann',\n",
       " 'newsboy',\n",
       " 'sharkboy',\n",
       " 'rumpus',\n",
       " 'locklear',\n",
       " 'rabbinical',\n",
       " 'skippin',\n",
       " 'cowhide',\n",
       " 'pistache',\n",
       " 'hominids',\n",
       " 'rumbled',\n",
       " 'missa',\n",
       " 'morehead',\n",
       " 'detachments',\n",
       " 'harpsichord',\n",
       " 'trusses',\n",
       " 'saris',\n",
       " 'lubin',\n",
       " 'bohemians',\n",
       " 'corvus',\n",
       " 'wite',\n",
       " 'matterwhat',\n",
       " 'witi',\n",
       " 'sarin',\n",
       " 'rusk',\n",
       " 'raga',\n",
       " 'ruso',\n",
       " 'swole',\n",
       " 'pula',\n",
       " 'rago',\n",
       " 'abhimanyu',\n",
       " 'tripp',\n",
       " 'creat',\n",
       " 'freon',\n",
       " 'mundu',\n",
       " 'mundy',\n",
       " 'sayest',\n",
       " 'shakal',\n",
       " 'yogo',\n",
       " 'rarin',\n",
       " 'ashoka',\n",
       " 'faxes',\n",
       " 'monir',\n",
       " 'hydraulics',\n",
       " 'yunus',\n",
       " 'linebackers',\n",
       " 'infirmity',\n",
       " 'lightfoot',\n",
       " 'sidewinders',\n",
       " 'madrigal',\n",
       " 'maliciously',\n",
       " 'piak',\n",
       " 'holman',\n",
       " 'senna',\n",
       " 'sch',\n",
       " 'ceramic',\n",
       " 'sca',\n",
       " 'internist',\n",
       " 'miggy',\n",
       " 'scr',\n",
       " 'anderton',\n",
       " 'bismarck',\n",
       " 'carters',\n",
       " 'checkin',\n",
       " 'floated',\n",
       " 'bator',\n",
       " 'rm',\n",
       " 'grannie',\n",
       " 'boozed',\n",
       " 'faddle',\n",
       " 'adina',\n",
       " 'sanz',\n",
       " 'sana',\n",
       " 'sano',\n",
       " 'bracha',\n",
       " 'courtesans',\n",
       " 'pasa',\n",
       " 'pash',\n",
       " 'milord',\n",
       " 'correcte',\n",
       " 'quicken',\n",
       " 'neddy',\n",
       " 'menswear',\n",
       " 'accreditation',\n",
       " 'pretrial',\n",
       " 'psychosomatic',\n",
       " 'corked',\n",
       " 'corker',\n",
       " 'jurist',\n",
       " 'leid',\n",
       " 'shinwa',\n",
       " 'sarasota',\n",
       " 'cabled',\n",
       " 'ganda',\n",
       " 'interoffice',\n",
       " 'hast',\n",
       " 'melancholia',\n",
       " 'ohhhhh',\n",
       " 'wogan',\n",
       " 'cessation',\n",
       " 'countin',\n",
       " 'ischia',\n",
       " 'norville',\n",
       " 'retroactive',\n",
       " 'adidas',\n",
       " 'reclaimed',\n",
       " 'aeschylus',\n",
       " 'pyo',\n",
       " 'pliny',\n",
       " 'mory',\n",
       " 'odometer',\n",
       " 'mori',\n",
       " 'moro',\n",
       " 'fragrance',\n",
       " 'beatriz',\n",
       " 'morg',\n",
       " 'beatrix',\n",
       " 'marsch',\n",
       " 'doos',\n",
       " 'testes',\n",
       " 'vesalius',\n",
       " 'doof',\n",
       " 'makeovers',\n",
       " 'mí',\n",
       " 'kaminski',\n",
       " 'godown',\n",
       " 'knocker',\n",
       " 'greaseball',\n",
       " 'siku',\n",
       " 'thermite',\n",
       " 'ponte',\n",
       " 'ponti',\n",
       " 'allegro',\n",
       " 'winemaker',\n",
       " 'boga',\n",
       " 'resync',\n",
       " 'bluestone',\n",
       " 'sequins',\n",
       " 'marylebone',\n",
       " 'juve',\n",
       " 'hens',\n",
       " 'gannets',\n",
       " 'footman',\n",
       " 'mender',\n",
       " 'oleander',\n",
       " 'amparo',\n",
       " 'gdansk',\n",
       " 'ricard',\n",
       " 'jiang',\n",
       " 'tulane',\n",
       " 'pinewood',\n",
       " 'barbarous',\n",
       " 'trellis',\n",
       " 'weirdoes',\n",
       " 'eveyone',\n",
       " 'roote',\n",
       " 'gearhart',\n",
       " 'roshi',\n",
       " 'papen',\n",
       " 'papes',\n",
       " 'rooty',\n",
       " 'refried',\n",
       " 'trivialities',\n",
       " 'deliberating',\n",
       " 'shitless',\n",
       " 'balaban',\n",
       " 'arakawa',\n",
       " 'ellos',\n",
       " 'purdue',\n",
       " 'mandarins',\n",
       " 'betweens',\n",
       " 'repayment',\n",
       " 'corine',\n",
       " 'necrosis',\n",
       " 'spinnaker',\n",
       " 'gilberts',\n",
       " 'numeric',\n",
       " 'brunetti',\n",
       " 'gilberto',\n",
       " 'treasonable',\n",
       " 'inquired',\n",
       " 'atwater',\n",
       " 'cokehead',\n",
       " 'squarepants',\n",
       " 'alrighty',\n",
       " 'windmills',\n",
       " 'colonnade',\n",
       " 'airway',\n",
       " 'ecologically',\n",
       " 'plowing',\n",
       " 'pyne',\n",
       " 'zala',\n",
       " 'ceasefire',\n",
       " 'weatherby',\n",
       " 'faom',\n",
       " 'hyderabad',\n",
       " 'scuffling',\n",
       " 'noma',\n",
       " 'nome',\n",
       " 'tove',\n",
       " 'toluca',\n",
       " 'aaaah',\n",
       " 'peaceably',\n",
       " 'smithers',\n",
       " 'nunchucks',\n",
       " 'talvez',\n",
       " 'elling',\n",
       " 'daytona',\n",
       " 'eiling',\n",
       " 'syringes',\n",
       " 'condolences',\n",
       " 'tummy',\n",
       " 'shlomo',\n",
       " 'navigators',\n",
       " 'ordway',\n",
       " 'mitzvah',\n",
       " 'waft',\n",
       " 'nikolay',\n",
       " 'luzon',\n",
       " 'nikolas',\n",
       " 'odors',\n",
       " 'selo',\n",
       " 'fermented',\n",
       " 'destabilizing',\n",
       " 'extorted',\n",
       " 'mian',\n",
       " 'antagonized',\n",
       " 'unnamable',\n",
       " 'astrea',\n",
       " 'pringles',\n",
       " 'mends',\n",
       " 'departmental',\n",
       " 'mende',\n",
       " 'stoning',\n",
       " 'pré',\n",
       " 'ugo',\n",
       " 'uga',\n",
       " 'patris',\n",
       " 'northwestern',\n",
       " 'clipboard',\n",
       " 'calibers',\n",
       " 'mousetraps',\n",
       " 'ninjutsu',\n",
       " 'sunni',\n",
       " 'planchet',\n",
       " 'boze',\n",
       " 'anniversaries',\n",
       " 'redoubt',\n",
       " 'schmoozing',\n",
       " 'twittering',\n",
       " 'hibbert',\n",
       " 'deactivate',\n",
       " 'richelieu',\n",
       " 'oishi',\n",
       " 'borgias',\n",
       " 'manas',\n",
       " 'condones',\n",
       " 'ocular',\n",
       " 'vickey',\n",
       " 'outlying',\n",
       " 'jawing',\n",
       " 'scatterbrain',\n",
       " 'franny',\n",
       " 'lasy',\n",
       " 'khadija',\n",
       " 'nanotech',\n",
       " 'ludes',\n",
       " 'sergej',\n",
       " 'sadat',\n",
       " 'olli',\n",
       " 'waitressing',\n",
       " 'beli',\n",
       " 'belo',\n",
       " 'olla',\n",
       " 'selden',\n",
       " 'olle',\n",
       " 'screwdrivers',\n",
       " 'olly',\n",
       " 'hetman',\n",
       " 'safet',\n",
       " 'galvatron',\n",
       " 'scherz',\n",
       " 'patrolled',\n",
       " 'thinkthat',\n",
       " 'nrc',\n",
       " 'cretan',\n",
       " 'crenshaw',\n",
       " 'skynyrd',\n",
       " 'nanao',\n",
       " 'nucleotides',\n",
       " 'osan',\n",
       " 'budged',\n",
       " 'cager',\n",
       " 'lade',\n",
       " 'lynette',\n",
       " 'vom',\n",
       " 'mycroft',\n",
       " 'mouche',\n",
       " 'vou',\n",
       " 'vor',\n",
       " 'vos',\n",
       " 'wouid',\n",
       " 'vox',\n",
       " 'voy',\n",
       " 'rifat',\n",
       " 'ddn',\n",
       " 'matchmakers',\n",
       " 'geeta',\n",
       " 'tauntaun',\n",
       " 'ashland',\n",
       " 'mongrels',\n",
       " 'giorno',\n",
       " 'kno',\n",
       " 'merel',\n",
       " 'raters',\n",
       " 'knd',\n",
       " 'thain',\n",
       " 'fuckwit',\n",
       " 'looted',\n",
       " 'nazarene',\n",
       " 'clasped',\n",
       " 'salva',\n",
       " 'salve',\n",
       " 'pesetas',\n",
       " 'joder',\n",
       " 'timur',\n",
       " 'crinkling',\n",
       " 'abstention',\n",
       " 'eero',\n",
       " 'andorra',\n",
       " 'horseradish',\n",
       " 'aimin',\n",
       " 'anklets',\n",
       " 'khun',\n",
       " 'cordon',\n",
       " 'maata',\n",
       " 'hayat',\n",
       " 'fritzy',\n",
       " 'counterintuitive',\n",
       " 'spink',\n",
       " 'wagonload',\n",
       " 'spina',\n",
       " 'retrovirus',\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################################################\n",
    "def embeddings_not_in_tokenizer(embeddings_dict, tokenizer):\n",
    "    elems = []\n",
    "    for w in embeddings_dict.keys():\n",
    "        if w not in tokenizer.word_index.keys():\n",
    "            elems += [w]\n",
    "    return elems\n",
    "    \n",
    "embeddings_not_in_tokenizer(embeddings_dict, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40567"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################################################\n",
    "tokenizer.word_index[list(tokenizer.word_index.keys())[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "for w in embeddings_not_in_tokenizer(embeddings_dict, tokenizer):\n",
    "    tokenizer.word_index[w] = len(tokenizer.word_index) + 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65713"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################################################\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_dictionary = tokenizer.word_index\n",
    "\n",
    "inverse_tokens_dictionary = {v : k for (k, v) in tokenizer.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "os.makedirs(os.path.join('./pickle_data/attack_utils'), exist_ok=True)\n",
    "\n",
    "with open(os.path.join('./pickle_data/attack_utils/tokens_dictionary.pickle'), 'wb') as f:\n",
    "    pickle.dump([tokens_dictionary, inverse_tokens_dictionary], f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1), ('and', 2), ('a', 3)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokens_dictionary.items())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'the'), (2, 'and'), (3, 'a')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(inverse_tokens_dictionary.items())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import pickle\\n\\nwith open('pickle\\\\tokens_dicts.pickle', 'wb') as f:\\n    pickle.dump([tokens_dictionary,inverse_tokens_dictionary], f)\\nf.close()\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pickle\n",
    "\n",
    "with open('pickle\\\\tokens_dicts.pickle', 'wb') as f:\n",
    "    pickle.dump([tokens_dictionary,inverse_tokens_dictionary], f)\n",
    "f.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import pickle\\n\\nwith open('pickle\\\\tokens_dicts.pickle', 'rb') as f:\\n    tokens_dictionary,inverse_tokens_dictionary = pickle.load(f)\\nf.close()\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pickle\n",
    "\n",
    "with open('pickle\\\\tokens_dicts.pickle', 'rb') as f:\n",
    "    tokens_dictionary,inverse_tokens_dictionary = pickle.load(f)\n",
    "f.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 45_000#50_000\n",
    "\n",
    "embedding_matrix = np.zeros(shape = (MAXLEN + 1, 300), dtype= 'float32')\n",
    "\n",
    "for w, i in tokenizer.word_index.items():\n",
    "    if w in embeddings_dict and i < MAXLEN + 1:\n",
    "        embedding_matrix[i,:] = embeddings_dict[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45001, 300)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from scipy import sparse\n",
    "\n",
    "distance_matrix = cosine_distances(embedding_matrix, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45001, 45001)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('numpy_files', exist_ok=True)\n",
    "np.save(os.path.join('./numpy_files/distance_matrix.npy'), distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distance_matrix = np.load(os.path.join('./numpy_files/distance_matrix.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens_dictionary['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, delta = 0.5, num_words = 20):\n",
    "    \n",
    "    try:\n",
    "        index = tokens_dictionary[word]\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    if (index > distance_matrix.shape[0]):\n",
    "        return []\n",
    "    \n",
    "    dist_order = np.argsort(distance_matrix[index,:])[1:num_words+1]\n",
    "    dist_list = distance_matrix[index][dist_order]\n",
    "    \n",
    "    mask = np.ones_like(dist_list)\n",
    "    mask = np.where(dist_list < delta)\n",
    "    return [inverse_tokens_dictionary[index] for index in dist_order[mask]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_words_to_great = most_similar('great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to \"great\" are:\n",
      "['grand', 'tremendous', 'awesome', 'phenomenal', 'terrific', 'prodigious', 'magnificent', 'wonderful', 'splendid', 'fantastic', 'marvelous', 'fabulous', 'marvellous', 'whopping', 'wondrous', 'formidable', 'huge', 'super', 'resplendent', 'excellent']\n"
     ]
    }
   ],
   "source": [
    "print('Most similar words to \"great\" are:')\n",
    "print(nearest_words_to_great)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GOOGLE 1 BILLION WORDS LANGUAGE MODEL\n",
    "\n",
    "To filter out words that are not in the context of a sentence\n",
    "\n",
    "\n",
    "https://github.com/tensorflow/models/tree/archive/research/lm_1b\n",
    "\n",
    "DOWNLOADS:<br>\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/graph-2016-09-10.pbtxt\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-base\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-char-embedding\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-lstm\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax0\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax1\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax2\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax3\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax4\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax5\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax6\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax7\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax8\n",
    "* http://download.tensorflow.org/models/LM_LSTM_CNN/vocab-2016-09-10.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il codice nella seguente cella è modificato da una base il cui copyright è espresso di seguito:\n",
    "\n",
    "```\n",
    "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import licensed_scripts.lm_1b_eval as google_language_model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM vocab loading done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recovering graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Recovering Graph google_language_model\\graph-2016-09-10.pbtxt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recovering checkpoint google_language_model\\ckpt-*\n"
     ]
    }
   ],
   "source": [
    "google_language_model = google_language_model_utils.LM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original phrase:')\n",
    "print('\"It has been such a great holiday\"')\n",
    "\n",
    "prefix = \"It has been such a\"\n",
    "suffix = 'holiday'\n",
    "\n",
    "lm_preds = google_language_model.get_words_probs(prefix, nearest_words_to_great, suffix)\n",
    "\n",
    "print('The most probable substitute is:') \n",
    "print(nearest_words_to_great[np.argmax(lm_preds)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
