{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "\n",
    "SEED = 123\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 Training and evaluating a DNN model on the IMDB Dataset\n",
    "## Downloading and data preprocessing\n",
    "\n",
    "Downloaded the dataset at http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "```\n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  month     = {June},\n",
    "  year      = {2011},\n",
    "  address   = {Portland, Oregon, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {142--150},\n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns = ['text','sentiment'])\n",
    "\n",
    "imdb_dir = \"./datasets/aclImdb\"\n",
    "\n",
    "for dir_kind in ['train','test']:\n",
    "    for label_type in ['neg', 'pos']:\n",
    "        dir_name = os.path.join(imdb_dir, dir_kind, label_type)\n",
    "        for fname in os.listdir(dir_name):\n",
    "            if fname[-4:] == '.txt':\n",
    "                f = open(os.path.join(dir_name, fname), encoding = \"utf8\")\n",
    "                df = df.append({'text': f.read(), 'sentiment': ['neg','pos'].index(label_type)}, ignore_index = True)\n",
    "                f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Story of a man who has unnatural feelings for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Airport '77 starts as a brand new luxury 747 p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This film lacked something I couldn't put my f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sorry everyone,,, I know this is supposed to b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When I was little my parents took me along to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  Story of a man who has unnatural feelings for ...         0\n",
       "1  Airport '77 starts as a brand new luxury 747 p...         0\n",
       "2  This film lacked something I couldn't put my f...         0\n",
       "3  Sorry everyone,,, I know this is supposed to b...         0\n",
       "4  When I was little my parents took me along to ...         0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative istances: 25000\n",
      "Number of positive istances: 25000\n",
      "Il dataset risulta essere bilanciato!\n"
     ]
    }
   ],
   "source": [
    "print ('Number of negative istances:', len(df[df['sentiment'] == 0]))\n",
    "print ('Number of positive istances:', len(df[df['sentiment'] == 1]))\n",
    "print ('Il dataset risulta essere bilanciato!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\n"
     ]
    }
   ],
   "source": [
    "print(df['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing Train and Test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size = 0.33, shuffle = True)\n",
    "\n",
    "x_train = list(x_train)\n",
    "x_test = list(x_test)\n",
    "\n",
    "y_train = list(y_train)\n",
    "y_test = list(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove preprocessing\n",
    "\n",
    "https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "\n",
    "with open(\"glove\\\\glove.42B.300d.txt\", \"r\",errors ='ignore', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_dict[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.5632e-01,  7.0167e-02, -1.0856e-01,  6.3920e-02,  4.4188e-01,\n",
       "        1.6448e-01, -2.2552e+00,  4.1941e-01, -3.1636e-01, -2.8735e-01,\n",
       "       -1.0089e-01,  2.8728e-01, -1.9072e-01,  1.9813e-01,  1.4305e-01,\n",
       "       -1.9234e-02,  7.8137e-03, -2.7725e-01, -1.7461e-01, -2.7296e-02,\n",
       "        2.0745e-01, -3.8855e-02, -6.2267e-01,  2.0114e-01,  1.8017e-01,\n",
       "       -1.4309e-01,  7.3436e-03,  4.5914e-02,  1.2701e-01,  1.9567e-01,\n",
       "       -3.3800e-01, -5.2403e-02,  3.8635e-01,  3.2452e-01,  4.3314e-02,\n",
       "        5.5894e-02, -2.7400e-01,  2.3822e-01,  3.5066e-01,  9.3277e-02,\n",
       "       -2.3778e-01, -2.3854e-01, -1.3535e-01,  1.5447e-01,  9.6359e-02,\n",
       "        9.1433e-02,  2.2692e-01, -7.4975e-02, -5.9885e-01,  1.0320e-01,\n",
       "        3.8681e-01, -3.0790e-01, -9.9559e-02, -2.6215e-02, -2.2730e-01,\n",
       "       -4.7876e-01, -7.3886e-02,  1.3225e-01, -3.0348e-01,  5.2221e-01,\n",
       "        4.4130e-02, -5.5885e-02, -3.4364e-01,  2.9747e-01, -1.1198e-01,\n",
       "       -6.0315e-01, -2.7066e-01,  1.9420e-01,  1.5879e-01, -1.2067e-01,\n",
       "       -3.9149e-01, -2.2446e-01,  5.7599e-03,  1.0279e-02, -1.6890e-01,\n",
       "       -2.1680e-01, -2.0914e-01,  4.8150e-01, -3.9147e-01, -2.8953e-01,\n",
       "        2.5419e-01, -4.6174e-01,  4.4380e-01, -2.3713e-01,  7.2150e-02,\n",
       "        4.8336e-01,  9.3756e-02, -3.7705e-02,  1.4864e-01,  2.9109e-01,\n",
       "       -6.0434e-02, -5.9944e-01, -1.0500e-01, -7.4636e-02,  1.8786e-01,\n",
       "        6.8264e-01, -2.0351e+00, -4.9578e-02,  1.5642e-01, -3.9180e-02,\n",
       "       -1.4909e-01, -7.3187e-02,  5.0762e-01,  2.6983e-02, -5.5783e-01,\n",
       "        4.7270e-01,  1.0095e-01,  4.5862e-01, -6.2622e-02, -2.1382e-01,\n",
       "       -5.4910e-03,  5.2229e-02, -1.0026e-01,  1.4687e-01,  1.1650e-01,\n",
       "       -6.3272e-02,  9.9580e-02,  5.0915e-01,  1.0498e-01, -9.4182e-02,\n",
       "       -2.3452e-01, -1.8576e-02,  2.0310e-02, -1.3528e-02,  1.8450e-01,\n",
       "       -4.1091e-01,  3.9790e-02,  2.5298e-01, -5.5495e-02,  3.3062e-01,\n",
       "       -1.6355e-01,  3.5122e-01, -6.7532e-02, -1.5499e-01, -4.0283e-01,\n",
       "       -4.1311e-01, -1.0714e-01, -6.2246e-01,  7.9395e-02,  8.4307e-01,\n",
       "        3.3514e-01, -1.1906e-01, -4.9424e-01, -4.3044e-01, -1.3389e-01,\n",
       "       -5.1032e-01,  6.8153e-01, -1.2873e-01, -1.8020e-01, -1.0992e-02,\n",
       "       -3.6983e-02, -2.3680e-01, -2.1248e-01,  3.2912e-01,  1.0232e-01,\n",
       "        3.4121e-02, -2.4824e-01, -5.4069e-02,  3.1243e-01, -3.4853e-01,\n",
       "        5.6615e-02,  2.5936e-01, -1.7554e-01, -1.7332e-01,  2.2569e-03,\n",
       "       -4.3400e-01,  1.0858e-01, -1.4214e-01,  5.6738e-01, -3.7382e-01,\n",
       "       -2.7801e-01,  3.7660e-02, -8.6311e-02, -1.7032e-02,  3.4599e-01,\n",
       "        3.8407e-02,  3.9709e-01,  3.7436e-02,  2.0677e-02, -1.2647e-01,\n",
       "        9.9674e-02,  3.9934e-01,  2.5696e-01,  1.8549e-01, -1.2690e-01,\n",
       "        1.7089e-01, -1.6896e-01,  1.3489e-01,  1.8779e-02, -2.7880e-02,\n",
       "        1.3588e-01, -4.5849e-02,  3.0583e-01,  8.1116e-02,  4.4987e-02,\n",
       "       -3.3128e-01,  5.3009e-01,  4.1595e-01, -3.3131e-01, -4.0157e-02,\n",
       "        3.2195e-01,  3.7154e-01,  3.8202e-01,  1.1299e-01, -1.5610e-01,\n",
       "       -8.5718e-02,  3.3083e-01,  3.1913e-01, -3.4404e-01, -2.1760e-01,\n",
       "       -1.6266e-01, -1.1864e-01,  6.5893e-02, -3.1143e-02, -1.2251e-02,\n",
       "       -1.0847e-01, -1.4923e-01, -7.1291e-01,  5.5287e-02,  6.7633e-02,\n",
       "       -1.0804e-01, -1.2727e-03, -3.8229e-01, -6.0203e-02, -4.2665e+00,\n",
       "        4.2173e-01,  1.3024e-01,  9.4837e-02, -1.9178e-03,  1.9432e-01,\n",
       "       -2.4945e-01,  1.4912e-01,  7.1274e-02, -1.9299e-01,  3.1941e-01,\n",
       "        6.0742e-02,  9.5889e-02, -1.7302e-01, -4.8453e-01, -4.7914e-01,\n",
       "       -3.3519e-01, -3.8776e-01,  1.3326e-01,  3.5923e-01, -9.1952e-02,\n",
       "       -2.8467e-01, -2.0948e-01,  2.3090e-01, -3.5915e-01,  2.1352e-01,\n",
       "       -2.4381e-01,  6.1679e-02, -2.4739e-01, -1.2886e-01, -2.5595e-01,\n",
       "        4.4218e-01, -2.1272e-01, -1.4728e-02, -1.9043e-01,  3.4073e-01,\n",
       "       -2.8298e-01,  2.4248e-01,  8.2348e-02, -3.8178e-01,  5.7402e-01,\n",
       "       -3.1505e-01,  4.2794e-02,  1.1085e-01,  4.6410e-01, -3.4755e-01,\n",
       "       -4.3290e-01,  3.4554e-01,  2.8313e-02,  8.4112e-02,  1.5865e-01,\n",
       "        9.6038e-02, -9.5528e-02,  7.9629e-02, -1.8217e-01,  9.5811e-02,\n",
       "        2.1544e-01, -5.4085e-02,  4.0857e-01,  2.4174e-01, -8.1513e-02,\n",
       "       -2.0428e-02,  1.9264e-01, -3.6382e-01, -2.8863e-02,  1.0432e-01,\n",
       "        1.9712e-01, -3.3538e-02,  2.5351e-01, -3.0916e-01, -1.0009e-01,\n",
       "        2.8267e-01, -1.8034e-01, -5.9228e-02,  3.3472e-01, -2.1873e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dict['10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def remove_html_tags(text):\n",
    "    return BeautifulSoup(text, 'lxml').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [remove_html_tags(sentence) for sentence in x_train]\n",
    "x_test = [remove_html_tags(sentence) for sentence in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = [sentence.replace(\"\\x85\", \"\") for sentence in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in embeddings_dict.keys() if w[0].isupper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [sentence.lower() for sentence in x_train]\n",
    "x_test = [sentence.lower() for sentence in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "latin_similar = \"’'‘ÆÐƎƏƐƔĲŊŒẞÞǷȜæðǝəɛɣĳŋœĸſßþƿȝĄƁÇĐƊĘĦĮƘŁØƠŞȘŢȚŦŲƯY̨Ƴąɓçđɗęħįƙłøơşșţțŧųưy̨ƴÁÀÂÄǍĂĀÃÅǺĄÆǼǢƁĆĊĈČÇĎḌĐƊÐÉÈĖÊËĚĔĒĘẸƎƏƐĠĜǦĞĢƔáàâäǎăāãåǻąæǽǣɓćċĉčçďḍđɗðéèėêëěĕēęẹǝəɛġĝǧğģɣĤḤĦIÍÌİÎÏǏĬĪĨĮỊĲĴĶƘĹĻŁĽĿʼNŃN̈ŇÑŅŊÓÒÔÖǑŎŌÕŐỌØǾƠŒĥḥħıíìiîïǐĭīĩįịĳĵķƙĸĺļłľŀŉńn̈ňñņŋóòôöǒŏōõőọøǿơœŔŘŖŚŜŠŞȘṢẞŤŢṬŦÞÚÙÛÜǓŬŪŨŰŮŲỤƯẂẀŴẄǷÝỲŶŸȲỸƳŹŻŽẒŕřŗſśŝšşșṣßťţṭŧþúùûüǔŭūũűůųụưẃẁŵẅƿýỳŷÿȳỹƴźżžẓ\"\n",
    "safe_characters = string.ascii_letters + string.digits + latin_similar + ' '\n",
    "safe_characters += \"'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " '.',\n",
       " '\"',\n",
       " ':',\n",
       " ')',\n",
       " '(',\n",
       " '-',\n",
       " '!',\n",
       " '?',\n",
       " '|',\n",
       " ';',\n",
       " '$',\n",
       " '&',\n",
       " '/',\n",
       " '[',\n",
       " ']',\n",
       " '>',\n",
       " '%',\n",
       " '=',\n",
       " '#',\n",
       " '+',\n",
       " '@',\n",
       " '~',\n",
       " '£',\n",
       " '\\\\',\n",
       " '_',\n",
       " '{',\n",
       " '}',\n",
       " '^',\n",
       " '`',\n",
       " '<',\n",
       " '€',\n",
       " '›',\n",
       " '½',\n",
       " '…',\n",
       " '“',\n",
       " '”',\n",
       " '–',\n",
       " '¢',\n",
       " '¡',\n",
       " '¿',\n",
       " '―',\n",
       " '¥',\n",
       " '—',\n",
       " '‹',\n",
       " '¼',\n",
       " '¤',\n",
       " '¾',\n",
       " '、',\n",
       " '»',\n",
       " '。',\n",
       " '‟',\n",
       " '￥',\n",
       " '«',\n",
       " '฿',\n",
       " 'ª',\n",
       " '˚',\n",
       " 'ƒ',\n",
       " 'ˈ',\n",
       " 'ˑ',\n",
       " '⅓',\n",
       " '˜',\n",
       " '₤',\n",
       " 'ˆ',\n",
       " '￡',\n",
       " '₂',\n",
       " '˙',\n",
       " '؟',\n",
       " '˝',\n",
       " '⅛',\n",
       " '„',\n",
       " 'ɡ',\n",
       " '۞',\n",
       " '๑',\n",
       " '⅔',\n",
       " 'ˌ',\n",
       " 'ﾟ',\n",
       " '⅜',\n",
       " '‛',\n",
       " '܂',\n",
       " '⁰',\n",
       " 'ở',\n",
       " '⅝',\n",
       " 'ﬁ',\n",
       " '͡',\n",
       " '̅',\n",
       " '۩',\n",
       " 'α',\n",
       " 'ʈ',\n",
       " '⅞',\n",
       " 'ɪ',\n",
       " '￦',\n",
       " ';',\n",
       " '̣',\n",
       " '˛',\n",
       " '٠',\n",
       " '₃',\n",
       " 'ȃ',\n",
       " '‚',\n",
       " 'ν',\n",
       " '۶',\n",
       " 'ǡ',\n",
       " 'ʿ',\n",
       " 'ʃ',\n",
       " '₁',\n",
       " 'β',\n",
       " 'ʤ',\n",
       " '˘',\n",
       " '٩',\n",
       " '̵',\n",
       " '￠',\n",
       " 'в',\n",
       " '̶',\n",
       " 'ǥ',\n",
       " 'λ',\n",
       " '２',\n",
       " 'δ',\n",
       " '٤',\n",
       " '۵',\n",
       " 'ˇ',\n",
       " '۲',\n",
       " '́',\n",
       " '１',\n",
       " 'ー',\n",
       " '۰',\n",
       " 'ƃ',\n",
       " 'ɔ',\n",
       " 'ɑ',\n",
       " '̂',\n",
       " 'ǀ',\n",
       " 'ω',\n",
       " '۱',\n",
       " 'ʡ',\n",
       " 'ʊ',\n",
       " '̃',\n",
       " '日',\n",
       " '⁴',\n",
       " 'ʒ',\n",
       " '̳',\n",
       " '３',\n",
       " '։',\n",
       " 'μ',\n",
       " 'ɂ',\n",
       " '₄',\n",
       " 'θ',\n",
       " 'ɨ',\n",
       " 'ｏ',\n",
       " 'ͧ',\n",
       " '年',\n",
       " 'ǰ',\n",
       " 'φ',\n",
       " 'ȥ',\n",
       " '７',\n",
       " 'ɿ',\n",
       " 'ـ',\n",
       " 'γ',\n",
       " 'ʌ',\n",
       " 'ǂ',\n",
       " 'ʻ',\n",
       " 'ɐ',\n",
       " 'ﬂ',\n",
       " 'ǹ',\n",
       " '̿',\n",
       " '̊',\n",
       " 'ƥ',\n",
       " 'ɒ',\n",
       " 'и',\n",
       " 'π',\n",
       " '４',\n",
       " 'ɹ',\n",
       " 'а',\n",
       " 'ｓ',\n",
       " '̏',\n",
       " 'ʔ',\n",
       " 'σ',\n",
       " 'ａ',\n",
       " 'ｉ',\n",
       " 'ȡ',\n",
       " 'ǵ',\n",
       " 'は',\n",
       " 'ǩ',\n",
       " '⁵',\n",
       " '̀',\n",
       " 'ʹ',\n",
       " '５',\n",
       " 'ᴥ',\n",
       " '߂',\n",
       " '˃',\n",
       " '˹',\n",
       " 'ȣ',\n",
       " '͂',\n",
       " 'ｗ',\n",
       " '̄',\n",
       " 'ȭ',\n",
       " 'ȿ',\n",
       " 'ｍ',\n",
       " 'ɤ',\n",
       " 'ȸ',\n",
       " 'с',\n",
       " 'ƽ',\n",
       " '₀',\n",
       " 'ｃ',\n",
       " 'ạ',\n",
       " 'ε',\n",
       " '⁶',\n",
       " 'の',\n",
       " '๐',\n",
       " '月',\n",
       " '̌',\n",
       " 'ɾ',\n",
       " 'ﾞ',\n",
       " '̸',\n",
       " 'ʘ',\n",
       " 'ɸ',\n",
       " 'ȫ',\n",
       " '⁸',\n",
       " '⅕',\n",
       " '̾',\n",
       " '₆',\n",
       " 'ｖ',\n",
       " 'τ',\n",
       " 'ʕ',\n",
       " 'ȯ',\n",
       " '܀',\n",
       " 'ː',\n",
       " '΄',\n",
       " '６',\n",
       " 'ˤ',\n",
       " 'ǫ',\n",
       " '๖',\n",
       " 'ｐ',\n",
       " 'ͤ',\n",
       " '̱',\n",
       " '١',\n",
       " '۳',\n",
       " '̎',\n",
       " '⁷',\n",
       " 'ɩ',\n",
       " 'ổ',\n",
       " '̐',\n",
       " '̓',\n",
       " 'ρ',\n",
       " 'ƪ',\n",
       " '̷',\n",
       " 'ˉ',\n",
       " 'ʞ',\n",
       " '₅',\n",
       " 'ɚ',\n",
       " 'ɯ',\n",
       " 'ʺ',\n",
       " 'ɲ',\n",
       " '\\u06dd',\n",
       " 'ɻ',\n",
       " '˂',\n",
       " 'ˡ',\n",
       " '位',\n",
       " 'ʹ',\n",
       " 'ʂ',\n",
       " 'ｂ',\n",
       " '８',\n",
       " 'ǭ',\n",
       " '⅙',\n",
       " 'ʧ',\n",
       " '９',\n",
       " 'ʐ',\n",
       " 'ｑ',\n",
       " 'ʋ',\n",
       " '۴',\n",
       " '̕',\n",
       " '̗',\n",
       " 'ʱ',\n",
       " 'ƶ',\n",
       " 'ǁ',\n",
       " '˻',\n",
       " '̴',\n",
       " 'ｔ',\n",
       " 'ḳ',\n",
       " 'ｘ',\n",
       " 'ǯ',\n",
       " 'ɵ',\n",
       " 'ʀ',\n",
       " 'ȱ',\n",
       " '号']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_chars = [c for c in list(embeddings_dict.keys()) if len(c) == 1]\n",
    "glove_symbols = [c for c in glove_chars if not c in safe_characters]\n",
    "glove_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " '[',\n",
       " ':',\n",
       " '¿',\n",
       " '·',\n",
       " '\\\\',\n",
       " ';',\n",
       " '\\x8e',\n",
       " '¤',\n",
       " '\\x9e',\n",
       " '´',\n",
       " '\\x96',\n",
       " '>',\n",
       " '\\x85',\n",
       " '\\uf04a',\n",
       " '«',\n",
       " '¨',\n",
       " '“',\n",
       " '+',\n",
       " '\"',\n",
       " '–',\n",
       " '\\x91',\n",
       " '，',\n",
       " '&',\n",
       " '=',\n",
       " '\\xa0',\n",
       " '\\x95',\n",
       " '»',\n",
       " '½',\n",
       " '%',\n",
       " 'ª',\n",
       " ')',\n",
       " '、',\n",
       " '!',\n",
       " '`',\n",
       " ']',\n",
       " '-',\n",
       " '{',\n",
       " '<',\n",
       " '\\x8d',\n",
       " '_',\n",
       " '\\x84',\n",
       " '$',\n",
       " '(',\n",
       " '\\x9a',\n",
       " '₤',\n",
       " ',',\n",
       " '}',\n",
       " '\\x80',\n",
       " '/',\n",
       " '°',\n",
       " '?',\n",
       " '*',\n",
       " '\\x97',\n",
       " '¦',\n",
       " '.',\n",
       " '★',\n",
       " '©',\n",
       " '\\uf0b7',\n",
       " '¾',\n",
       " '”',\n",
       " 'º',\n",
       " '|',\n",
       " '¡',\n",
       " '^',\n",
       " '£',\n",
       " '®',\n",
       " '…',\n",
       " '\\t',\n",
       " '#',\n",
       " '~',\n",
       " '″']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jigsaw_chars = set(w for sentence in x_train for w in sentence)\n",
    "jigsaw_symbols = [c for c in jigsaw_chars if not c in safe_characters]\n",
    "jigsaw_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['·',\n",
       " '\\x8e',\n",
       " '\\x9e',\n",
       " '´',\n",
       " '\\x96',\n",
       " '\\x85',\n",
       " '\\uf04a',\n",
       " '¨',\n",
       " '\\x91',\n",
       " '，',\n",
       " '\\xa0',\n",
       " '\\x95',\n",
       " '\\x8d',\n",
       " '\\x84',\n",
       " '\\x9a',\n",
       " '\\x80',\n",
       " '°',\n",
       " '*',\n",
       " '\\x97',\n",
       " '¦',\n",
       " '★',\n",
       " '©',\n",
       " '\\uf0b7',\n",
       " 'º',\n",
       " '®',\n",
       " '\\t',\n",
       " '″']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols_to_delete = [c for c in jigsaw_symbols if not c in glove_symbols]\n",
    "symbols_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " '[',\n",
       " ':',\n",
       " '¿',\n",
       " '\\\\',\n",
       " ';',\n",
       " '¤',\n",
       " '>',\n",
       " '«',\n",
       " '“',\n",
       " '+',\n",
       " '\"',\n",
       " '–',\n",
       " '&',\n",
       " '=',\n",
       " '»',\n",
       " '½',\n",
       " '%',\n",
       " 'ª',\n",
       " ')',\n",
       " '、',\n",
       " '!',\n",
       " '`',\n",
       " ']',\n",
       " '-',\n",
       " '{',\n",
       " '<',\n",
       " '_',\n",
       " '$',\n",
       " '(',\n",
       " '₤',\n",
       " ',',\n",
       " '}',\n",
       " '/',\n",
       " '?',\n",
       " '.',\n",
       " '¾',\n",
       " '”',\n",
       " '|',\n",
       " '¡',\n",
       " '^',\n",
       " '£',\n",
       " '…',\n",
       " '#',\n",
       " '~']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols_to_isolate = [c for c in jigsaw_symbols if c in glove_symbols]\n",
    "symbols_to_isolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "    for symbol in symbols_to_delete:\n",
    "        x = x.replace(symbol, ' ')\n",
    "    for symbol in symbols_to_isolate:\n",
    "        x = x.replace(symbol, ' ' + symbol + ' ')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [clean_text(sentence) for sentence in x_train]\n",
    "x_test = [clean_text(sentence) for sentence in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(c for c in set(w for sentence in x_train for w in sentence) if not c in safe_characters) == set(symbols_to_isolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_contractions(x):\n",
    "    x = tokenizer.tokenize(x)\n",
    "    x = ' '.join(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [handle_contractions(sentence) for sentence in x_train]\n",
    "x_test = [handle_contractions(sentence) for sentence in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deesh',\n",
       " 'settleling',\n",
       " 'unterwaldt',\n",
       " 'redemeption',\n",
       " 'frederikson',\n",
       " 'kiiling',\n",
       " \"'brendan\",\n",
       " \"'probie\",\n",
       " 'gunsels',\n",
       " \"nunez'writing\",\n",
       " 'dhéry',\n",
       " \"'anatomy\",\n",
       " 'man’s',\n",
       " 'bobiddi',\n",
       " \"i'am\",\n",
       " \"'undesirables\",\n",
       " 'ranthorincus',\n",
       " 'erkia',\n",
       " 'hackdom',\n",
       " 'disneyisque',\n",
       " 'gigeresque',\n",
       " 'thimig',\n",
       " \"'scent\",\n",
       " 'physche',\n",
       " 'forgeorge',\n",
       " 'grubiness',\n",
       " \"'chipmunks\",\n",
       " 'standpointeven',\n",
       " \"'l'enfant\",\n",
       " 'afficinados',\n",
       " \"'destiny\",\n",
       " \"'roll\",\n",
       " 'tywanna',\n",
       " \"'story\",\n",
       " 'cloutish',\n",
       " 'ain’t',\n",
       " 'reasembling',\n",
       " \"'jembés\",\n",
       " 'milafon',\n",
       " \"'sex\",\n",
       " 'dukakas',\n",
       " 'sowhile',\n",
       " 'kudso',\n",
       " 'superwonderscope',\n",
       " 'arzenta',\n",
       " 'unattuned',\n",
       " 'scenesdirection',\n",
       " \"'scarecrow\",\n",
       " 'bejeepers',\n",
       " 'eastood',\n",
       " 'surrrender',\n",
       " \"'movies\",\n",
       " 'mammonist',\n",
       " 'vakulinchuk',\n",
       " 'oléander',\n",
       " \"'delirious\",\n",
       " 'ge999',\n",
       " 'macchesney',\n",
       " 'chyra',\n",
       " 'dwuids',\n",
       " 'whitezombie',\n",
       " 'charlesmanson',\n",
       " \"'gaira\",\n",
       " 'hynckle',\n",
       " 'jamshied',\n",
       " 'eliz7212',\n",
       " 'interceeding',\n",
       " \"fam'ly\",\n",
       " 'posest',\n",
       " 'alderich',\n",
       " 'pedicaris',\n",
       " 'nordham',\n",
       " \"'angles\",\n",
       " 'poolguy',\n",
       " 'cadfile',\n",
       " \"'mamma\",\n",
       " 'andsensitivity',\n",
       " 'muppetism',\n",
       " 'chuchnov',\n",
       " 'chupacabre',\n",
       " \"'sexploitation\",\n",
       " 'inkansas',\n",
       " 'akshey',\n",
       " 'stracult',\n",
       " 'sexploiters',\n",
       " \"'eastern\",\n",
       " 'disharmoniously',\n",
       " 'beslon',\n",
       " \"'idea\",\n",
       " \"'short\",\n",
       " 'aghashe',\n",
       " 'buduschego',\n",
       " \"'loser\",\n",
       " 'wtse',\n",
       " 'actriss',\n",
       " \"'pickpocket\",\n",
       " 'myoshi',\n",
       " \"'sexism\",\n",
       " 'spoily',\n",
       " \"off'ed\",\n",
       " \"'absorbed\",\n",
       " 'godamnawful',\n",
       " 'don’t',\n",
       " \"'uns\",\n",
       " 'wantsthe',\n",
       " 'theatrecompany',\n",
       " 'firmine',\n",
       " 'godather',\n",
       " 'fictionalizations',\n",
       " 'dépardieu',\n",
       " 'burkewaite',\n",
       " 'andnobody',\n",
       " 'mctaketh',\n",
       " '5corporal',\n",
       " \"'trading\",\n",
       " \"munnera'na\",\n",
       " \"'jackson\",\n",
       " 'filmdaraar',\n",
       " 'yrigoyens',\n",
       " 'kelada',\n",
       " 'brawlings',\n",
       " 'teensploitation',\n",
       " \"90'sthe\",\n",
       " 'woppi',\n",
       " \"'enlightened\",\n",
       " '19sep2009',\n",
       " \"'comedians\",\n",
       " '1984ish',\n",
       " 'fouquier',\n",
       " 'denuccio',\n",
       " 'ashenbach',\n",
       " \"'lavish\",\n",
       " 'vepsaian',\n",
       " 'janset',\n",
       " 'swiztertland',\n",
       " 'gittai',\n",
       " 'blockbutser',\n",
       " 'ourintrepid',\n",
       " 'buaku',\n",
       " \"'treasure\",\n",
       " 'everwonder',\n",
       " \"'delilah\",\n",
       " 'fakeeven',\n",
       " 'ctgsr',\n",
       " 'vikea',\n",
       " 'illaria',\n",
       " 'unlikelies',\n",
       " \"'mean\",\n",
       " \"'goodness\",\n",
       " \"'personnel\",\n",
       " \"'pulp\",\n",
       " \"'control\",\n",
       " '5battle',\n",
       " 'ephata',\n",
       " 'stagebound',\n",
       " 'smashan',\n",
       " 'herngren',\n",
       " 'rocll',\n",
       " 'whitepaddy',\n",
       " 'vetchy',\n",
       " \"'update\",\n",
       " \"'panda\",\n",
       " 'berniah',\n",
       " \"'lifer\",\n",
       " 'cocunutshell',\n",
       " \"does'nt\",\n",
       " 'lionsgate’s',\n",
       " \"'ninja\",\n",
       " 'sharikovs',\n",
       " 'whilhelm',\n",
       " 'booooooooooooooooooooooooooooooooooooooooooooooo',\n",
       " 'doesn’t',\n",
       " \"'they\",\n",
       " \"'totally\",\n",
       " 'scareless',\n",
       " \"'realism\",\n",
       " \"'medical\",\n",
       " \"'puffed\",\n",
       " 'counteratc',\n",
       " \"'boriac\",\n",
       " \"'amantes\",\n",
       " 'kuttram',\n",
       " 'latter’s',\n",
       " \"'er\",\n",
       " 'goudry',\n",
       " 'kameena',\n",
       " 'reechi',\n",
       " \"'dumbed\",\n",
       " 'rfff',\n",
       " 'acholoic',\n",
       " 'subsequenet',\n",
       " 'srtikes',\n",
       " \"'rubbish\",\n",
       " 'dunstin',\n",
       " 'fagabefe',\n",
       " 'warecki',\n",
       " 'negrophile',\n",
       " \"'half\",\n",
       " \"'fictitious\",\n",
       " 'stagnantly',\n",
       " \"'feature\",\n",
       " 'georgievna',\n",
       " 'valentinov',\n",
       " \"'cut\",\n",
       " 'televisionnot',\n",
       " 'bubolova',\n",
       " \"'soapdish\",\n",
       " 'horrorvision',\n",
       " 'overstudiously',\n",
       " 'prosatanos',\n",
       " 'emtombs',\n",
       " 'breillet',\n",
       " 'auie',\n",
       " \"'bluff\",\n",
       " 'annnother',\n",
       " \"'action\",\n",
       " 'mantagna',\n",
       " 'borowcyzk',\n",
       " \"'keep\",\n",
       " 'shintarô',\n",
       " 'dedovshina',\n",
       " 'vishwatama',\n",
       " \"'ask\",\n",
       " 'skatewheels',\n",
       " 'hillermans',\n",
       " \"'gilda\",\n",
       " 'manckiewitz',\n",
       " 'kathly',\n",
       " 'ee03128',\n",
       " 'kalirai',\n",
       " 'cannibalistically',\n",
       " 'underacting',\n",
       " 'navokov',\n",
       " 'outgovinda',\n",
       " 'waacky',\n",
       " \"d'force\",\n",
       " \"'racier\",\n",
       " \"'ooops\",\n",
       " 'emptour',\n",
       " 'kavogianni',\n",
       " 'can’t',\n",
       " 'jaadugar',\n",
       " \"'rough\",\n",
       " 'lengthyajay',\n",
       " 'physicalisation',\n",
       " 'vità',\n",
       " 'narishma',\n",
       " 'cowatonga',\n",
       " 'giorgelli',\n",
       " 'flackering',\n",
       " 'timetraveling',\n",
       " \"cau'se\",\n",
       " 'ammmmmbbbererrrrrrrrrgerrrrrrrssss',\n",
       " 'sim0ne',\n",
       " 'bowlofsoul23',\n",
       " 'podewell',\n",
       " 'depardeu',\n",
       " \"'struggling\",\n",
       " \"quis'aiment\",\n",
       " 'kumarswamypillai',\n",
       " 'stupiditiy',\n",
       " 'movieanyways',\n",
       " 'trelayne',\n",
       " 'lovehatedreamslifeworkplayfriends',\n",
       " 'reviewied',\n",
       " 'scheitz',\n",
       " 'sarsgard',\n",
       " \"'ole\",\n",
       " 'semiticism',\n",
       " \"'condenado\",\n",
       " \"'har\",\n",
       " 'capitaes',\n",
       " 'ecclesten',\n",
       " 'hanzos',\n",
       " 'mancici',\n",
       " \"'cinderella\",\n",
       " \"'trek\",\n",
       " 'oooooozzzzzzed',\n",
       " \"'unfolds\",\n",
       " 'l946',\n",
       " 'unimaginary',\n",
       " 'barreler',\n",
       " \"'cheerleader\",\n",
       " 'smuttier',\n",
       " 'kusminsky',\n",
       " 'mohinish',\n",
       " 'timewarped',\n",
       " \"'rag\",\n",
       " 'nondas',\n",
       " 'annabela',\n",
       " \"'vanilla\",\n",
       " \"'reverend\",\n",
       " \"'chokingly\",\n",
       " 'timeexpect',\n",
       " \"'illusion\",\n",
       " 'darwinianed',\n",
       " 'minglun',\n",
       " 'crythin',\n",
       " 'heroine’s',\n",
       " \"'sea\",\n",
       " 'brynnerfinally',\n",
       " 'dorday',\n",
       " 'djjohn',\n",
       " 'elsehere',\n",
       " 'convenienced',\n",
       " \"'doghi\",\n",
       " '10molly',\n",
       " \"'hostel\",\n",
       " \"'gangster\",\n",
       " 'martinaud',\n",
       " 'soooooooooooooooooooooooooooooooooo',\n",
       " 'editionwilliam',\n",
       " \"'peace\",\n",
       " 'zelniker',\n",
       " \"'lorenzo\",\n",
       " 'davitz',\n",
       " 'nuddie',\n",
       " '98mins',\n",
       " 'sondhemim',\n",
       " 'harchard',\n",
       " 'goodgfellas',\n",
       " 'gunghroo',\n",
       " 'walmington',\n",
       " 'birgette',\n",
       " 'unredeemably',\n",
       " \"'whirlpool\",\n",
       " 'technicolorian',\n",
       " \"'mob\",\n",
       " 'overract',\n",
       " 'onelikeable',\n",
       " 'shinbei',\n",
       " \"'nilsson\",\n",
       " 'armpitted',\n",
       " 'manierism',\n",
       " 'ooherh',\n",
       " 'dalamatians',\n",
       " \"'hare\",\n",
       " 'kulbushan',\n",
       " \"'howling\",\n",
       " \"'orientalist\",\n",
       " \"d'ennery\",\n",
       " 'dyzack',\n",
       " 'midkoff',\n",
       " 'appallingness',\n",
       " 'weirdling',\n",
       " \"'fleshed\",\n",
       " 'kasporov',\n",
       " 'saint405',\n",
       " \"'perfect\",\n",
       " 'orchestrail',\n",
       " \"as'kentucky\",\n",
       " 'radlitch',\n",
       " 'cockpuncher',\n",
       " 'gharlie',\n",
       " 'snozzcumbers',\n",
       " \"hum'n'shiver\",\n",
       " 'mechenosets',\n",
       " \"'officer\",\n",
       " 'neizvestnostlab',\n",
       " 'mouthery',\n",
       " 'fowzi',\n",
       " \"'go\",\n",
       " 'endectomy',\n",
       " 'frakken',\n",
       " 'independantmasterpiece',\n",
       " 'suspecions',\n",
       " \"'clerks\",\n",
       " \"'overpower\",\n",
       " \"'punk\",\n",
       " 'insectish',\n",
       " 'explopitative',\n",
       " \"'nightingale\",\n",
       " 'samotari',\n",
       " 'frenhoffer',\n",
       " 'sfxthere',\n",
       " 'channelcarpe',\n",
       " \"'jehaan\",\n",
       " 'b3rd',\n",
       " \"'national\",\n",
       " 'shammer',\n",
       " \"'mar\",\n",
       " 'intendend',\n",
       " 'orhal',\n",
       " \"'charmed\",\n",
       " 'mcgarten',\n",
       " \"'afraid\",\n",
       " 'lygter',\n",
       " 'cheeesy',\n",
       " 'anfractuosity',\n",
       " 'disneyish',\n",
       " 'kabuliwallah',\n",
       " \"'isoyc\",\n",
       " 'quimnn',\n",
       " \"'coronation\",\n",
       " 'riiiiiike',\n",
       " 'amedala',\n",
       " 'transvestive',\n",
       " 'envahisseurs',\n",
       " \"'license\",\n",
       " \"'whistler\",\n",
       " 'puzzu',\n",
       " \"'blade\",\n",
       " 'gambrelli',\n",
       " \"'group\",\n",
       " 'femalesother',\n",
       " \"'balkanized\",\n",
       " 'lalica',\n",
       " 'cofffeenut',\n",
       " 'mehras',\n",
       " 'pacingly',\n",
       " 'nallae',\n",
       " \"franc'l'isco\",\n",
       " 'yusoufzai',\n",
       " \"'cruising\",\n",
       " \"'alien\",\n",
       " 'redack',\n",
       " 'phillimines',\n",
       " 'chandelere',\n",
       " 'beenbetter',\n",
       " 'luthercorp',\n",
       " \"'mass\",\n",
       " 'annanka',\n",
       " \"'o\",\n",
       " 'reconstituirea',\n",
       " \"'carrie\",\n",
       " 'dreimaderlhaus',\n",
       " \"'success\",\n",
       " 'mamardashvili',\n",
       " \"'awful\",\n",
       " \"'eddies\",\n",
       " \"'kno\",\n",
       " 'filmette',\n",
       " 'anywasy',\n",
       " 'géne',\n",
       " 'hadleyville',\n",
       " 'surpirsisngly',\n",
       " 'embeciles',\n",
       " \"'speedy\",\n",
       " 'createsandre',\n",
       " 'pangborne',\n",
       " 'reaccounting',\n",
       " 'ploteven',\n",
       " 'cominski',\n",
       " 'parapyschologist',\n",
       " \"'honeymoon\",\n",
       " \"'lionel\",\n",
       " \"'succubus\",\n",
       " 'hammiest',\n",
       " \"'leading\",\n",
       " 'bolwieser',\n",
       " 'tmmvds',\n",
       " \"'early\",\n",
       " \"'frightening\",\n",
       " 'alivealive',\n",
       " 'pambieri',\n",
       " 'cagnard',\n",
       " 'bjornstrand',\n",
       " \"'vincent\",\n",
       " '1h53',\n",
       " 'puuuuleeese',\n",
       " 'exitenz',\n",
       " 'muhwa',\n",
       " 'pataker',\n",
       " 'companymanwho',\n",
       " 'stanislofsky',\n",
       " 'infrontokay',\n",
       " 'semprinni',\n",
       " \"'cyril\",\n",
       " 'afrikkaner',\n",
       " 'olizzi',\n",
       " 'vculek',\n",
       " 'mcgranery',\n",
       " \"'conformist\",\n",
       " \"'trauma'soon\",\n",
       " 'claythen',\n",
       " 'potepolov',\n",
       " 'svenon',\n",
       " 'brontean',\n",
       " 'brycer',\n",
       " \"'screen\",\n",
       " \"'pest\",\n",
       " 'bergorra',\n",
       " 'stratofreighter',\n",
       " 'charactercan',\n",
       " 'approporiately',\n",
       " 'caractor',\n",
       " 'tgotl',\n",
       " 'outpopulated',\n",
       " 'nadjiwarra',\n",
       " 'boucci',\n",
       " 'who’s',\n",
       " 'rigomortis',\n",
       " 'thoongadae',\n",
       " 'wlikerson',\n",
       " 'minghellian',\n",
       " 'takuand',\n",
       " 'burâddo',\n",
       " 'desenstizing',\n",
       " \"'anticlimactic\",\n",
       " 'formulatic',\n",
       " \"'nods\",\n",
       " 'hegalhuzen',\n",
       " \"'shell\",\n",
       " 'msyterious',\n",
       " 'mecbeths',\n",
       " 'dataeven',\n",
       " \"'aussie\",\n",
       " 'howblessed',\n",
       " '1946oscars',\n",
       " 'concels',\n",
       " 'pattipatti',\n",
       " \"'sub\",\n",
       " 'bunkerfox',\n",
       " \"'dickie\",\n",
       " 'jusassic',\n",
       " 'rylott',\n",
       " 'spoilersthere',\n",
       " \"'ed\",\n",
       " 'messalso',\n",
       " 'caradan',\n",
       " 'treasureable',\n",
       " 'gimbotheghoulies',\n",
       " 'brothering',\n",
       " 'alesandr',\n",
       " 'spaceys',\n",
       " \"'thumbs\",\n",
       " 'vierya',\n",
       " 'bakedthe',\n",
       " 'solendz',\n",
       " 'soister',\n",
       " \"'unsees\",\n",
       " 'pollad',\n",
       " 'summfield',\n",
       " '193o',\n",
       " \"'raiders\",\n",
       " 'revoew',\n",
       " 'kaborka',\n",
       " \"'reckless\",\n",
       " 'cinemaniacs',\n",
       " \"dream'n\",\n",
       " 'usualthe',\n",
       " 'sinkewicz',\n",
       " 'ortrentin',\n",
       " 'wnbq',\n",
       " 'sawahla',\n",
       " \"'life\",\n",
       " 'viay',\n",
       " 'sinisterness',\n",
       " 'businesstiger',\n",
       " \"'ne\",\n",
       " 'why2',\n",
       " 'paralellism',\n",
       " 'drearymovies',\n",
       " 'associates’',\n",
       " 'hopfel',\n",
       " \"'geezer\",\n",
       " 'bashki',\n",
       " '1960sthere',\n",
       " 'mcewee',\n",
       " \"'bohemian\",\n",
       " 'litghow',\n",
       " 'multizillion',\n",
       " 'underfelt',\n",
       " \"'store\",\n",
       " 'columbous',\n",
       " 'moley75',\n",
       " 'childlish',\n",
       " 'scuddamore',\n",
       " 'remendados',\n",
       " 'zarzo',\n",
       " 'biblefest',\n",
       " 'realisator',\n",
       " \"'male\",\n",
       " 'altioklar',\n",
       " 'notgr',\n",
       " 'bicenntinial',\n",
       " 'howdoilooknyc',\n",
       " \"'4\",\n",
       " 'masturbationscene',\n",
       " 'qwuiet',\n",
       " 'durdurdur',\n",
       " \"'satya\",\n",
       " 'moughal',\n",
       " 'hendirx',\n",
       " 'starfix',\n",
       " 'michaelesque',\n",
       " 'bookdom',\n",
       " \"'mild\",\n",
       " 'maegi',\n",
       " 'ariesgemini100',\n",
       " \"'pretentiousness\",\n",
       " 'apotheosising',\n",
       " \"'gringos\",\n",
       " 'directorially',\n",
       " 'sydrow',\n",
       " 'jenniferbeals',\n",
       " 'brigthly',\n",
       " 'rollnecks',\n",
       " 'perejaslav',\n",
       " 'ludacras',\n",
       " 'cinematagraph',\n",
       " 'effacingly',\n",
       " 'skankville',\n",
       " 'givesthe',\n",
       " 'coverbox',\n",
       " \"'humour\",\n",
       " \"would'nt\",\n",
       " \"'green\",\n",
       " \"'mosntres\",\n",
       " 'szararem',\n",
       " 'kk0gtfuk98u',\n",
       " 'inevitabally',\n",
       " \"'effing\",\n",
       " 'baigelmann',\n",
       " 'eritated',\n",
       " \"'deliver\",\n",
       " 'bailero',\n",
       " \"'years\",\n",
       " \"'borrow\",\n",
       " \"'robocop\",\n",
       " 'tynge',\n",
       " 'sparach',\n",
       " 'pshycological',\n",
       " 'loulla',\n",
       " 'skeptiscism',\n",
       " \"sh't\",\n",
       " 'trashified',\n",
       " 'impacciatore',\n",
       " 'pavlosky',\n",
       " 'ryholite',\n",
       " 'marabre',\n",
       " \"'overthrow\",\n",
       " \"'eccentric\",\n",
       " 'gafones',\n",
       " 'encw',\n",
       " 'funjatta',\n",
       " 'brownesque',\n",
       " \"'erotic\",\n",
       " 'representseverything',\n",
       " \"'terreur\",\n",
       " 'chewbaka',\n",
       " \"'screaming\",\n",
       " 'footmats',\n",
       " 'mendelito',\n",
       " 'ketin',\n",
       " 'boddhist',\n",
       " \"gel'ziabar\",\n",
       " 'tsunis',\n",
       " 'midthunder',\n",
       " 'marianbad',\n",
       " \"'idle\",\n",
       " \"'beat\",\n",
       " 'gawfs',\n",
       " 'bloodbut',\n",
       " 'mincka',\n",
       " \"'instant\",\n",
       " \"'vagrant\",\n",
       " \"'jehennan\",\n",
       " 'titillatory',\n",
       " 'burstingly',\n",
       " 'hatzisavvas',\n",
       " \"'hunting\",\n",
       " 'ondricek',\n",
       " 'suicidees',\n",
       " \"your'e\",\n",
       " 'imdbers',\n",
       " \"'mrs\",\n",
       " 'tringtignat',\n",
       " 'fosselius',\n",
       " \"'turkish\",\n",
       " \"'weird\",\n",
       " \"'favorite\",\n",
       " 'rifif',\n",
       " 'prejudicm',\n",
       " \"'giants\",\n",
       " \"'malcom\",\n",
       " 'marengi',\n",
       " 'laydu',\n",
       " 'wertmueller',\n",
       " 'estrado',\n",
       " 'filmok',\n",
       " 'valentine4',\n",
       " 'shirne',\n",
       " 'computerizd',\n",
       " 'lowitsch',\n",
       " 'skitzoid',\n",
       " \"'movieworld\",\n",
       " 'ratherly',\n",
       " 'jane’s',\n",
       " 'excellent19th',\n",
       " 'theirstockings',\n",
       " 'hysterion',\n",
       " 'hzu',\n",
       " 'wasteof',\n",
       " \"'count\",\n",
       " 'cicatillo',\n",
       " 'bradfordonavon',\n",
       " 'guerdjou',\n",
       " \"'vapourised\",\n",
       " \"'streets\",\n",
       " 'akinshina',\n",
       " \"'transformers\",\n",
       " \"'crew\",\n",
       " 'grantness',\n",
       " 'barillet',\n",
       " \"'suburbia\",\n",
       " \"'technically\",\n",
       " \"'hook\",\n",
       " 'zaping',\n",
       " 'shakespearen',\n",
       " 'hassadeevichit',\n",
       " 'digonales',\n",
       " \"'auteur\",\n",
       " \"'moonstruck\",\n",
       " 'pasquele',\n",
       " \"'wee\",\n",
       " 'sinometric',\n",
       " 'szajda',\n",
       " 'encapsuling',\n",
       " 'burauen',\n",
       " 'cillic',\n",
       " \"'prayer\",\n",
       " \"'commando\",\n",
       " 'plane’s',\n",
       " 'merlik',\n",
       " 'lidón',\n",
       " 'maclaglen',\n",
       " 'engletine',\n",
       " 'dresdel',\n",
       " 'schwadel',\n",
       " \"l'umanoide\",\n",
       " 'shi77er',\n",
       " \"'video\",\n",
       " \"'rocket\",\n",
       " 'scarecreow',\n",
       " 'angelwas',\n",
       " 'murkwood',\n",
       " \"couldn't\",\n",
       " 'msted',\n",
       " \"'dine\",\n",
       " '10film',\n",
       " 'ryhs',\n",
       " '2737487',\n",
       " \"'southern\",\n",
       " 'nuld',\n",
       " 'spoilersalison',\n",
       " \"'weirdo\",\n",
       " 'distastefull',\n",
       " 'iskarioth',\n",
       " 'facelessly',\n",
       " \"'ah\",\n",
       " 'filmdo',\n",
       " 'warnercolor',\n",
       " 'cheeziest',\n",
       " 'drebbin',\n",
       " 'influencehow',\n",
       " 'tanyusha',\n",
       " 'grimily',\n",
       " 'pedtrchenko',\n",
       " \"'toothbrush\",\n",
       " 'mwuhahahaa',\n",
       " 'wheatlry',\n",
       " 'alllrriiiight',\n",
       " 'yellowcoats',\n",
       " 'krummernes',\n",
       " 'rosenstraße',\n",
       " 'obobo',\n",
       " 'nyatta',\n",
       " 'funerial',\n",
       " \"they'v\",\n",
       " 'naturedness',\n",
       " 'katecapshaw',\n",
       " 'sixarrows',\n",
       " 'moviehunter',\n",
       " 'tokessa',\n",
       " \"'whoever\",\n",
       " \"'depraved\",\n",
       " \"'search\",\n",
       " 'kierlaw',\n",
       " 'storylife',\n",
       " \"'karakter\",\n",
       " 'rolesthe',\n",
       " 'lobisomen',\n",
       " 'hearen',\n",
       " \"'stiff\",\n",
       " 'ambricourt',\n",
       " 'douchess',\n",
       " 'gringy',\n",
       " 'dornwinkles',\n",
       " \"'nazis\",\n",
       " 'edinbourgh',\n",
       " 'zizola',\n",
       " 'misreably',\n",
       " \"'dewey\",\n",
       " 'bahire',\n",
       " \"'fred\",\n",
       " 'bagdarasian',\n",
       " 'cansing',\n",
       " \"'dr\",\n",
       " 'villainthe',\n",
       " 'casavates',\n",
       " 'kidotai',\n",
       " 'vyeing',\n",
       " \"'not\",\n",
       " \"'nuovomondo\",\n",
       " 'murdocco',\n",
       " 'robberyi',\n",
       " \"'bones\",\n",
       " \"'european\",\n",
       " \"mf'ing\",\n",
       " \"'b\",\n",
       " 'huggaland',\n",
       " 'durenmatt',\n",
       " 'canuckteach',\n",
       " 'consevatory',\n",
       " 'incensere',\n",
       " 'referentialism',\n",
       " 'matheisen',\n",
       " \"'traffic\",\n",
       " \"acting'la\",\n",
       " 'oxenbould',\n",
       " 'horrifyingand',\n",
       " 'traditionaljapanese',\n",
       " 'astrogators',\n",
       " \"'cartoonish\",\n",
       " \"'lilo\",\n",
       " 'hischaracter',\n",
       " 'sappily',\n",
       " 'mazurski',\n",
       " 'oedipial',\n",
       " 'skelatal',\n",
       " 'umcompromising',\n",
       " 'decaune',\n",
       " 'schtriiiiings',\n",
       " 'sleetaks',\n",
       " \"'hitchcockian\",\n",
       " 'skeweredness',\n",
       " \"spooky'n'shuddery\",\n",
       " 'filmographic',\n",
       " 'nonanimated',\n",
       " 'weawwy',\n",
       " \"ebay'ing\",\n",
       " 'apocylpse',\n",
       " \"'glum\",\n",
       " 'foresay',\n",
       " \"'comedies\",\n",
       " 'euthenased',\n",
       " \"'writ\",\n",
       " 'leroy’s',\n",
       " 'laupin',\n",
       " 'marielitos',\n",
       " \"'grow\",\n",
       " 'heston’s',\n",
       " 'klopping',\n",
       " 'pertersburg',\n",
       " 'permatteo',\n",
       " 'ophanage',\n",
       " \"'emanuelle\",\n",
       " '83mins',\n",
       " \"'style\",\n",
       " 'computeranimation',\n",
       " 'wixell',\n",
       " 'spoilersalthough',\n",
       " 'christianreference',\n",
       " 'omened',\n",
       " \"'kelly\",\n",
       " 'guerchard',\n",
       " 'minghela',\n",
       " 'hedgrowed',\n",
       " 'delacroixs',\n",
       " \"'mistake\",\n",
       " 'kannes',\n",
       " \"ev'rybody\",\n",
       " 'silentbearded',\n",
       " 'chubacabra',\n",
       " 'raisulu',\n",
       " 'forgedaboutit',\n",
       " 'malysheva',\n",
       " 'mraovich',\n",
       " 'glazedly',\n",
       " \"'ralph\",\n",
       " 'sopkiw',\n",
       " 'ferdanand',\n",
       " \"'bruno\",\n",
       " \"'snake\",\n",
       " 'chiefstrangely',\n",
       " 'atrotious',\n",
       " 'dredds',\n",
       " 'sprezzzatura',\n",
       " 'metamorphsis',\n",
       " \"6'3\",\n",
       " \"'feelings\",\n",
       " 'gestaldi',\n",
       " 'storyaspect',\n",
       " 'camadrie',\n",
       " \"'empire\",\n",
       " 'badamitabh',\n",
       " 'telkovsky',\n",
       " 'spymate',\n",
       " 'kusugi',\n",
       " 'hpd2',\n",
       " \"'oasis\",\n",
       " 'buonavolontà',\n",
       " 'chuckywe',\n",
       " 'thugees',\n",
       " \"'platinum\",\n",
       " \"'saw\",\n",
       " 'tlkg',\n",
       " 'felichy',\n",
       " 'shandara',\n",
       " 'surété',\n",
       " 'floudering',\n",
       " \"'fatal\",\n",
       " 'davisplot',\n",
       " 'repirse',\n",
       " 'cheersdamian',\n",
       " \"'vapoorize\",\n",
       " 'satyajir',\n",
       " 'apallingly',\n",
       " 'favbut',\n",
       " \"'haunting\",\n",
       " \"jun'ichi\",\n",
       " 'riddlezone',\n",
       " \"'smell\",\n",
       " 'subtled',\n",
       " 'posisbly',\n",
       " \"'inferior\",\n",
       " 'haaavaad',\n",
       " 'millean',\n",
       " 'ferrot',\n",
       " \"'delight\",\n",
       " 'topenga',\n",
       " 'slumberness',\n",
       " \"'penis\",\n",
       " \"'auctions\",\n",
       " 'supernaturalthat',\n",
       " 'klapish',\n",
       " 'comesup',\n",
       " 'noirest',\n",
       " 'jaykumar',\n",
       " 'peckenpah',\n",
       " 'baffeling',\n",
       " 'dussain',\n",
       " 'soutendijk',\n",
       " 'finkus',\n",
       " 'xenophobicjust',\n",
       " 'golwyn',\n",
       " 'mcgiveth',\n",
       " 'minicoopers',\n",
       " 'zambibwia',\n",
       " 'catogoricaly',\n",
       " 'thirtysomthing',\n",
       " 'velankar',\n",
       " 'repudiee',\n",
       " 'typage',\n",
       " 'tragidian',\n",
       " 'threemen',\n",
       " 'berbson',\n",
       " 'azjazz',\n",
       " \"i'f\",\n",
       " \"'60ies\",\n",
       " 'allance',\n",
       " \"'1st\",\n",
       " 'baseheart',\n",
       " 'wrp24',\n",
       " 'grefé',\n",
       " \"'mole\",\n",
       " \"'suspenseful\",\n",
       " 'cbgbomfug',\n",
       " \"shoot'em'up\",\n",
       " 'infinnerty',\n",
       " 'demirkubuz',\n",
       " 'unembroidered',\n",
       " 'propagandistically',\n",
       " 'flagellistic',\n",
       " \"'offbeat\",\n",
       " 'stridence',\n",
       " \"'madhuri\",\n",
       " 'lerios',\n",
       " 'msrk',\n",
       " 'juryalthough',\n",
       " 'epätoivoista',\n",
       " 'dezmo',\n",
       " 'movieeverything',\n",
       " 'witherspooon',\n",
       " 'checkthen',\n",
       " 'cevic',\n",
       " 'artigot',\n",
       " 'blinkende',\n",
       " \"'objectivity\",\n",
       " 'spoilerthere',\n",
       " 'cinematoraphy',\n",
       " \"'begin\",\n",
       " 'falstaffs',\n",
       " 'collector’s',\n",
       " 'sostrong',\n",
       " 'bukhanovsky',\n",
       " 'unfortenately',\n",
       " 'connectionsmickey',\n",
       " \"'nothing\",\n",
       " ...}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(w for sentence in x_train for w in sentence.split() if w not in embeddings_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_quote(text):\n",
    "    return ' '.join(x[1:] if x.startswith(\"'\") and len(x) > 1 else x for x in text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [fix_quote(sentence) for sentence in x_train]\n",
    "x_test = [fix_quote(sentence) for sentence in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train_preprocessed = x_train\n",
    "\n",
    "x_test_preprocessed = x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['text'] = df['text'].apply(lambda x: preprocesser.text_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import pickle\\n\\nos.makedirs('pickle', exist_ok=True)\\n\\nwith open('pickle\\\\data.pickle', 'wb') as f:\\n    pickle.dump([x_test, y_test], f)\\nf.close()\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pickle\n",
    "\n",
    "os.makedirs('pickle', exist_ok=True)\n",
    "\n",
    "with open('pickle\\\\data.pickle', 'wb') as f:\n",
    "    pickle.dump([x_test, y_test], f)\n",
    "f.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_preprocessed = [preprocesser.text_preprocessing(sentence) for sentence in x_train]\n",
    "#x_test_preprocessed = [preprocesser.text_preprocessing(sentence) for sentence in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Preprocessed texts')\n",
    "#print(x_train_preprocessed[:3])\n",
    "#print(x_test_preprocessed[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['text'] = df['text'].apply(lambda x: preprocesser.text_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import pickle\\n\\nos.makedirs('pickle', exist_ok=True)\\n\\nwith open('pickle\\\\data.pickle', 'wb') as f:\\n    pickle.dump([x_test, y_test], f)\\nf.close()\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pickle\n",
    "\n",
    "os.makedirs('pickle', exist_ok=True)\n",
    "\n",
    "with open('pickle\\\\data.pickle', 'wb') as f:\n",
    "    pickle.dump([x_test, y_test], f)\n",
    "f.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.utils import shuffle\\n\\nx_train, x_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size = 0.33, shuffle = True)\\n\\nx_train = list(x_train)\\nx_test = list(x_test)\\n\\ny_train = list(y_train)\\ny_test = list(y_test)\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size = 0.33, shuffle = True)\n",
    "\n",
    "x_train = list(x_train)\n",
    "x_test = list(x_test)\n",
    "\n",
    "y_train = list(y_train)\n",
    "y_test = list(y_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88087 unique tokens.\n",
      "Shape of train data tensor: (33500, 500)\n",
      "Shape of train label tensor: (33500,)\n",
      "Shape of test data tensor: (16500, 500)\n",
      "Shape of test label tensor: (16500,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAXLEN = 50_000\n",
    "\n",
    "tokenizer = Tokenizer(MAXLEN)\n",
    "tokenizer.fit_on_texts(x_train_preprocessed)\n",
    "\n",
    "maxlen = 500\n",
    "\n",
    "#MAXLEN = len(tokenizer.word_index) + 1\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train_preprocessed)\n",
    "test_sequences = tokenizer.texts_to_sequences(x_test_preprocessed)\n",
    "\n",
    "print('Found %s unique tokens.' % len(tokenizer.word_index))\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen = maxlen)\n",
    "test_data = pad_sequences(test_sequences, maxlen = maxlen)\n",
    "\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n",
    "print('Shape of train data tensor:', train_data.shape)\n",
    "print('Shape of train label tensor:', y_train.shape)\n",
    "\n",
    "print('Shape of test data tensor:', test_data.shape)\n",
    "print('Shape of test label tensor:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average review length:\n",
      "271.5328358208955\n"
     ]
    }
   ],
   "source": [
    "print('Average review length:')\n",
    "print( sum([len(t.split()) for t in x_train_preprocessed])/len(x_train_preprocessed) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the train dataset:\n",
      "88117\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique words in the train dataset:')\n",
    "print( len(set(w for t in x_train_preprocessed for w in t.split())) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo la mia matrice per ogni parola del mio dizionario e metto la riga della matrice a tutti 0 se non\n",
    "# esiste una certa parola\n",
    "\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((MAXLEN + 1, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < MAXLEN + 1:\n",
    "        embedding_vector = embeddings_dict.get(word)\n",
    "        # Words not found in the embedding index will be all zeros.\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout, LSTM, Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=3\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.1,\n",
    "        patience=3,\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath= 'models\\\\best_model_redone_2.h5',\n",
    "        save_weights_only=False,\n",
    "        monitor='val_acc',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fitted_model():\n",
    "\n",
    "    EMBEDDING_DIM = 300\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAXLEN+1,\n",
    "                        EMBEDDING_DIM,\n",
    "                        weights=[embedding_matrix],\n",
    "                        trainable=False,\n",
    "                       input_length = maxlen))\n",
    "    model.add(Bidirectional(LSTM(100, return_sequences = True)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "    \n",
    "    history = model.fit(train_data, y_train,\n",
    "                        epochs=15,\n",
    "                        batch_size=128,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_split=0.2,\n",
    "                        verbose = 2)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/15\n",
      "26800/26800 - 20s - loss: 0.4383 - acc: 0.7858 - val_loss: 0.3061 - val_acc: 0.8718\n",
      "Epoch 2/15\n",
      "26800/26800 - 18s - loss: 0.2989 - acc: 0.8737 - val_loss: 0.3786 - val_acc: 0.8309\n",
      "Epoch 3/15\n",
      "26800/26800 - 18s - loss: 0.2448 - acc: 0.8996 - val_loss: 0.4046 - val_acc: 0.8299\n",
      "Epoch 4/15\n",
      "26800/26800 - 17s - loss: 0.2041 - acc: 0.9206 - val_loss: 0.2602 - val_acc: 0.8909\n",
      "Epoch 5/15\n",
      "26800/26800 - 17s - loss: 0.1688 - acc: 0.9349 - val_loss: 0.2715 - val_acc: 0.8878\n",
      "Epoch 6/15\n",
      "26800/26800 - 17s - loss: 0.1372 - acc: 0.9504 - val_loss: 0.2560 - val_acc: 0.9009\n",
      "Epoch 7/15\n",
      "26800/26800 - 18s - loss: 0.1045 - acc: 0.9636 - val_loss: 0.2421 - val_acc: 0.9106\n",
      "Epoch 8/15\n",
      "26800/26800 - 17s - loss: 0.0806 - acc: 0.9733 - val_loss: 0.4884 - val_acc: 0.8461\n",
      "Epoch 9/15\n",
      "26800/26800 - 18s - loss: 0.0620 - acc: 0.9797 - val_loss: 0.2676 - val_acc: 0.9136\n",
      "Epoch 10/15\n",
      "26800/26800 - 18s - loss: 0.0464 - acc: 0.9850 - val_loss: 0.3006 - val_acc: 0.9109\n",
      "Epoch 11/15\n",
      "26800/26800 - 17s - loss: 0.0115 - acc: 0.9985 - val_loss: 0.3012 - val_acc: 0.9145\n",
      "Epoch 12/15\n",
      "26800/26800 - 18s - loss: 0.0074 - acc: 0.9993 - val_loss: 0.3163 - val_acc: 0.9124\n",
      "Epoch 13/15\n",
      "26800/26800 - 17s - loss: 0.0052 - acc: 0.9997 - val_loss: 0.3267 - val_acc: 0.9122\n",
      "Epoch 14/15\n",
      "26800/26800 - 17s - loss: 0.0036 - acc: 0.9997 - val_loss: 0.3299 - val_acc: 0.9125\n"
     ]
    }
   ],
   "source": [
    "history = get_fitted_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tf.keras.models.load_model(\"models\\\\best_model_redone_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500/16500 [==============================] - 42s 3ms/sample - loss: 0.2882 - acc: 0.9176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28822651348082406, 0.91763633]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(test_data, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
