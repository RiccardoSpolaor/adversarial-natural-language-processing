{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "\n",
    "SEED = 123\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 Training and evaluating a DNN model on the IMDB Dataset\n",
    "## Downloading and data preprocessing\n",
    "\n",
    "Downloaded the dataset at http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "```\n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  month     = {June},\n",
    "  year      = {2011},\n",
    "  address   = {Portland, Oregon, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {142--150},\n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns = ['text','sentiment'])\n",
    "\n",
    "imdb_dir = \"./datasets/aclImdb\"\n",
    "\n",
    "for dir_kind in ['train','test']:\n",
    "    for label_type in ['neg', 'pos']:\n",
    "        dir_name = os.path.join(imdb_dir, dir_kind, label_type)\n",
    "        for fname in os.listdir(dir_name):\n",
    "            if fname[-4:] == '.txt':\n",
    "                f = open(os.path.join(dir_name, fname), encoding = \"utf8\")\n",
    "                df = df.append({'text': f.read(), 'sentiment': ['neg','pos'].index(label_type)}, ignore_index = True)\n",
    "                f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Story of a man who has unnatural feelings for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Airport '77 starts as a brand new luxury 747 p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This film lacked something I couldn't put my f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sorry everyone,,, I know this is supposed to b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When I was little my parents took me along to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  Story of a man who has unnatural feelings for ...         0\n",
       "1  Airport '77 starts as a brand new luxury 747 p...         0\n",
       "2  This film lacked something I couldn't put my f...         0\n",
       "3  Sorry everyone,,, I know this is supposed to b...         0\n",
       "4  When I was little my parents took me along to ...         0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative istances: 25000\n",
      "Number of positive istances: 25000\n",
      "Il dataset risulta essere bilanciato!\n"
     ]
    }
   ],
   "source": [
    "print ('Number of negative istances:', len(df[df['sentiment'] == 0]))\n",
    "print ('Number of positive istances:', len(df[df['sentiment'] == 1]))\n",
    "print ('Il dataset risulta essere bilanciato!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\n"
     ]
    }
   ],
   "source": [
    "print(df['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scripts.preprocessing import Preprocesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Preprocessed Text Example:')\n",
    "#print(Preprocesser.text_preprocessing(df['text'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing Train and Test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size = 0.33, shuffle = True)\n",
    "\n",
    "x_train = list(x_train)\n",
    "x_test = list(x_test)\n",
    "\n",
    "y_train = list(y_train)\n",
    "y_test = list(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove preprocessing\n",
    "\n",
    "https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "\n",
    "with open(\"glove\\\\glove.42B.300d.txt\", \"r\",errors ='ignore', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_dict[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.5632e-01,  7.0167e-02, -1.0856e-01,  6.3920e-02,  4.4188e-01,\n",
       "        1.6448e-01, -2.2552e+00,  4.1941e-01, -3.1636e-01, -2.8735e-01,\n",
       "       -1.0089e-01,  2.8728e-01, -1.9072e-01,  1.9813e-01,  1.4305e-01,\n",
       "       -1.9234e-02,  7.8137e-03, -2.7725e-01, -1.7461e-01, -2.7296e-02,\n",
       "        2.0745e-01, -3.8855e-02, -6.2267e-01,  2.0114e-01,  1.8017e-01,\n",
       "       -1.4309e-01,  7.3436e-03,  4.5914e-02,  1.2701e-01,  1.9567e-01,\n",
       "       -3.3800e-01, -5.2403e-02,  3.8635e-01,  3.2452e-01,  4.3314e-02,\n",
       "        5.5894e-02, -2.7400e-01,  2.3822e-01,  3.5066e-01,  9.3277e-02,\n",
       "       -2.3778e-01, -2.3854e-01, -1.3535e-01,  1.5447e-01,  9.6359e-02,\n",
       "        9.1433e-02,  2.2692e-01, -7.4975e-02, -5.9885e-01,  1.0320e-01,\n",
       "        3.8681e-01, -3.0790e-01, -9.9559e-02, -2.6215e-02, -2.2730e-01,\n",
       "       -4.7876e-01, -7.3886e-02,  1.3225e-01, -3.0348e-01,  5.2221e-01,\n",
       "        4.4130e-02, -5.5885e-02, -3.4364e-01,  2.9747e-01, -1.1198e-01,\n",
       "       -6.0315e-01, -2.7066e-01,  1.9420e-01,  1.5879e-01, -1.2067e-01,\n",
       "       -3.9149e-01, -2.2446e-01,  5.7599e-03,  1.0279e-02, -1.6890e-01,\n",
       "       -2.1680e-01, -2.0914e-01,  4.8150e-01, -3.9147e-01, -2.8953e-01,\n",
       "        2.5419e-01, -4.6174e-01,  4.4380e-01, -2.3713e-01,  7.2150e-02,\n",
       "        4.8336e-01,  9.3756e-02, -3.7705e-02,  1.4864e-01,  2.9109e-01,\n",
       "       -6.0434e-02, -5.9944e-01, -1.0500e-01, -7.4636e-02,  1.8786e-01,\n",
       "        6.8264e-01, -2.0351e+00, -4.9578e-02,  1.5642e-01, -3.9180e-02,\n",
       "       -1.4909e-01, -7.3187e-02,  5.0762e-01,  2.6983e-02, -5.5783e-01,\n",
       "        4.7270e-01,  1.0095e-01,  4.5862e-01, -6.2622e-02, -2.1382e-01,\n",
       "       -5.4910e-03,  5.2229e-02, -1.0026e-01,  1.4687e-01,  1.1650e-01,\n",
       "       -6.3272e-02,  9.9580e-02,  5.0915e-01,  1.0498e-01, -9.4182e-02,\n",
       "       -2.3452e-01, -1.8576e-02,  2.0310e-02, -1.3528e-02,  1.8450e-01,\n",
       "       -4.1091e-01,  3.9790e-02,  2.5298e-01, -5.5495e-02,  3.3062e-01,\n",
       "       -1.6355e-01,  3.5122e-01, -6.7532e-02, -1.5499e-01, -4.0283e-01,\n",
       "       -4.1311e-01, -1.0714e-01, -6.2246e-01,  7.9395e-02,  8.4307e-01,\n",
       "        3.3514e-01, -1.1906e-01, -4.9424e-01, -4.3044e-01, -1.3389e-01,\n",
       "       -5.1032e-01,  6.8153e-01, -1.2873e-01, -1.8020e-01, -1.0992e-02,\n",
       "       -3.6983e-02, -2.3680e-01, -2.1248e-01,  3.2912e-01,  1.0232e-01,\n",
       "        3.4121e-02, -2.4824e-01, -5.4069e-02,  3.1243e-01, -3.4853e-01,\n",
       "        5.6615e-02,  2.5936e-01, -1.7554e-01, -1.7332e-01,  2.2569e-03,\n",
       "       -4.3400e-01,  1.0858e-01, -1.4214e-01,  5.6738e-01, -3.7382e-01,\n",
       "       -2.7801e-01,  3.7660e-02, -8.6311e-02, -1.7032e-02,  3.4599e-01,\n",
       "        3.8407e-02,  3.9709e-01,  3.7436e-02,  2.0677e-02, -1.2647e-01,\n",
       "        9.9674e-02,  3.9934e-01,  2.5696e-01,  1.8549e-01, -1.2690e-01,\n",
       "        1.7089e-01, -1.6896e-01,  1.3489e-01,  1.8779e-02, -2.7880e-02,\n",
       "        1.3588e-01, -4.5849e-02,  3.0583e-01,  8.1116e-02,  4.4987e-02,\n",
       "       -3.3128e-01,  5.3009e-01,  4.1595e-01, -3.3131e-01, -4.0157e-02,\n",
       "        3.2195e-01,  3.7154e-01,  3.8202e-01,  1.1299e-01, -1.5610e-01,\n",
       "       -8.5718e-02,  3.3083e-01,  3.1913e-01, -3.4404e-01, -2.1760e-01,\n",
       "       -1.6266e-01, -1.1864e-01,  6.5893e-02, -3.1143e-02, -1.2251e-02,\n",
       "       -1.0847e-01, -1.4923e-01, -7.1291e-01,  5.5287e-02,  6.7633e-02,\n",
       "       -1.0804e-01, -1.2727e-03, -3.8229e-01, -6.0203e-02, -4.2665e+00,\n",
       "        4.2173e-01,  1.3024e-01,  9.4837e-02, -1.9178e-03,  1.9432e-01,\n",
       "       -2.4945e-01,  1.4912e-01,  7.1274e-02, -1.9299e-01,  3.1941e-01,\n",
       "        6.0742e-02,  9.5889e-02, -1.7302e-01, -4.8453e-01, -4.7914e-01,\n",
       "       -3.3519e-01, -3.8776e-01,  1.3326e-01,  3.5923e-01, -9.1952e-02,\n",
       "       -2.8467e-01, -2.0948e-01,  2.3090e-01, -3.5915e-01,  2.1352e-01,\n",
       "       -2.4381e-01,  6.1679e-02, -2.4739e-01, -1.2886e-01, -2.5595e-01,\n",
       "        4.4218e-01, -2.1272e-01, -1.4728e-02, -1.9043e-01,  3.4073e-01,\n",
       "       -2.8298e-01,  2.4248e-01,  8.2348e-02, -3.8178e-01,  5.7402e-01,\n",
       "       -3.1505e-01,  4.2794e-02,  1.1085e-01,  4.6410e-01, -3.4755e-01,\n",
       "       -4.3290e-01,  3.4554e-01,  2.8313e-02,  8.4112e-02,  1.5865e-01,\n",
       "        9.6038e-02, -9.5528e-02,  7.9629e-02, -1.8217e-01,  9.5811e-02,\n",
       "        2.1544e-01, -5.4085e-02,  4.0857e-01,  2.4174e-01, -8.1513e-02,\n",
       "       -2.0428e-02,  1.9264e-01, -3.6382e-01, -2.8863e-02,  1.0432e-01,\n",
       "        1.9712e-01, -3.3538e-02,  2.5351e-01, -3.0916e-01, -1.0009e-01,\n",
       "        2.8267e-01, -1.8034e-01, -5.9228e-02,  3.3472e-01, -2.1873e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dict['10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def remove_html_tags(text):\n",
    "    return BeautifulSoup(text, 'lxml').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [remove_html_tags(sentence) for sentence in x_train]\n",
    "x_test = [remove_html_tags(sentence) for sentence in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = [sentence.replace(\"\\x85\", \"\") for sentence in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in embeddings_dict.keys() if w[0].isupper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [sentence.lower() for sentence in x_train]\n",
    "x_test = [sentence.lower() for sentence in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "latin_similar = \"’'‘ÆÐƎƏƐƔĲŊŒẞÞǷȜæðǝəɛɣĳŋœĸſßþƿȝĄƁÇĐƊĘĦĮƘŁØƠŞȘŢȚŦŲƯY̨Ƴąɓçđɗęħįƙłøơşșţțŧųưy̨ƴÁÀÂÄǍĂĀÃÅǺĄÆǼǢƁĆĊĈČÇĎḌĐƊÐÉÈĖÊËĚĔĒĘẸƎƏƐĠĜǦĞĢƔáàâäǎăāãåǻąæǽǣɓćċĉčçďḍđɗðéèėêëěĕēęẹǝəɛġĝǧğģɣĤḤĦIÍÌİÎÏǏĬĪĨĮỊĲĴĶƘĹĻŁĽĿʼNŃN̈ŇÑŅŊÓÒÔÖǑŎŌÕŐỌØǾƠŒĥḥħıíìiîïǐĭīĩįịĳĵķƙĸĺļłľŀŉńn̈ňñņŋóòôöǒŏōõőọøǿơœŔŘŖŚŜŠŞȘṢẞŤŢṬŦÞÚÙÛÜǓŬŪŨŰŮŲỤƯẂẀŴẄǷÝỲŶŸȲỸƳŹŻŽẒŕřŗſśŝšşșṣßťţṭŧþúùûüǔŭūũűůųụưẃẁŵẅƿýỳŷÿȳỹƴźżžẓ\"\n",
    "safe_characters = string.ascii_letters + string.digits + latin_similar + ' '\n",
    "safe_characters += \"'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " '.',\n",
       " '\"',\n",
       " ':',\n",
       " ')',\n",
       " '(',\n",
       " '-',\n",
       " '!',\n",
       " '?',\n",
       " '|',\n",
       " ';',\n",
       " '$',\n",
       " '&',\n",
       " '/',\n",
       " '[',\n",
       " ']',\n",
       " '>',\n",
       " '%',\n",
       " '=',\n",
       " '#',\n",
       " '+',\n",
       " '@',\n",
       " '~',\n",
       " '£',\n",
       " '\\\\',\n",
       " '_',\n",
       " '{',\n",
       " '}',\n",
       " '^',\n",
       " '`',\n",
       " '<',\n",
       " '€',\n",
       " '›',\n",
       " '½',\n",
       " '…',\n",
       " '“',\n",
       " '”',\n",
       " '–',\n",
       " '¢',\n",
       " '¡',\n",
       " '¿',\n",
       " '―',\n",
       " '¥',\n",
       " '—',\n",
       " '‹',\n",
       " '¼',\n",
       " '¤',\n",
       " '¾',\n",
       " '、',\n",
       " '»',\n",
       " '。',\n",
       " '‟',\n",
       " '￥',\n",
       " '«',\n",
       " '฿',\n",
       " 'ª',\n",
       " '˚',\n",
       " 'ƒ',\n",
       " 'ˈ',\n",
       " 'ˑ',\n",
       " '⅓',\n",
       " '˜',\n",
       " '₤',\n",
       " 'ˆ',\n",
       " '￡',\n",
       " '₂',\n",
       " '˙',\n",
       " '؟',\n",
       " '˝',\n",
       " '⅛',\n",
       " '„',\n",
       " 'ɡ',\n",
       " '۞',\n",
       " '๑',\n",
       " '⅔',\n",
       " 'ˌ',\n",
       " 'ﾟ',\n",
       " '⅜',\n",
       " '‛',\n",
       " '܂',\n",
       " '⁰',\n",
       " 'ở',\n",
       " '⅝',\n",
       " 'ﬁ',\n",
       " '͡',\n",
       " '̅',\n",
       " '۩',\n",
       " 'α',\n",
       " 'ʈ',\n",
       " '⅞',\n",
       " 'ɪ',\n",
       " '￦',\n",
       " ';',\n",
       " '̣',\n",
       " '˛',\n",
       " '٠',\n",
       " '₃',\n",
       " 'ȃ',\n",
       " '‚',\n",
       " 'ν',\n",
       " '۶',\n",
       " 'ǡ',\n",
       " 'ʿ',\n",
       " 'ʃ',\n",
       " '₁',\n",
       " 'β',\n",
       " 'ʤ',\n",
       " '˘',\n",
       " '٩',\n",
       " '̵',\n",
       " '￠',\n",
       " 'в',\n",
       " '̶',\n",
       " 'ǥ',\n",
       " 'λ',\n",
       " '２',\n",
       " 'δ',\n",
       " '٤',\n",
       " '۵',\n",
       " 'ˇ',\n",
       " '۲',\n",
       " '́',\n",
       " '１',\n",
       " 'ー',\n",
       " '۰',\n",
       " 'ƃ',\n",
       " 'ɔ',\n",
       " 'ɑ',\n",
       " '̂',\n",
       " 'ǀ',\n",
       " 'ω',\n",
       " '۱',\n",
       " 'ʡ',\n",
       " 'ʊ',\n",
       " '̃',\n",
       " '日',\n",
       " '⁴',\n",
       " 'ʒ',\n",
       " '̳',\n",
       " '３',\n",
       " '։',\n",
       " 'μ',\n",
       " 'ɂ',\n",
       " '₄',\n",
       " 'θ',\n",
       " 'ɨ',\n",
       " 'ｏ',\n",
       " 'ͧ',\n",
       " '年',\n",
       " 'ǰ',\n",
       " 'φ',\n",
       " 'ȥ',\n",
       " '７',\n",
       " 'ɿ',\n",
       " 'ـ',\n",
       " 'γ',\n",
       " 'ʌ',\n",
       " 'ǂ',\n",
       " 'ʻ',\n",
       " 'ɐ',\n",
       " 'ﬂ',\n",
       " 'ǹ',\n",
       " '̿',\n",
       " '̊',\n",
       " 'ƥ',\n",
       " 'ɒ',\n",
       " 'и',\n",
       " 'π',\n",
       " '４',\n",
       " 'ɹ',\n",
       " 'а',\n",
       " 'ｓ',\n",
       " '̏',\n",
       " 'ʔ',\n",
       " 'σ',\n",
       " 'ａ',\n",
       " 'ｉ',\n",
       " 'ȡ',\n",
       " 'ǵ',\n",
       " 'は',\n",
       " 'ǩ',\n",
       " '⁵',\n",
       " '̀',\n",
       " 'ʹ',\n",
       " '５',\n",
       " 'ᴥ',\n",
       " '߂',\n",
       " '˃',\n",
       " '˹',\n",
       " 'ȣ',\n",
       " '͂',\n",
       " 'ｗ',\n",
       " '̄',\n",
       " 'ȭ',\n",
       " 'ȿ',\n",
       " 'ｍ',\n",
       " 'ɤ',\n",
       " 'ȸ',\n",
       " 'с',\n",
       " 'ƽ',\n",
       " '₀',\n",
       " 'ｃ',\n",
       " 'ạ',\n",
       " 'ε',\n",
       " '⁶',\n",
       " 'の',\n",
       " '๐',\n",
       " '月',\n",
       " '̌',\n",
       " 'ɾ',\n",
       " 'ﾞ',\n",
       " '̸',\n",
       " 'ʘ',\n",
       " 'ɸ',\n",
       " 'ȫ',\n",
       " '⁸',\n",
       " '⅕',\n",
       " '̾',\n",
       " '₆',\n",
       " 'ｖ',\n",
       " 'τ',\n",
       " 'ʕ',\n",
       " 'ȯ',\n",
       " '܀',\n",
       " 'ː',\n",
       " '΄',\n",
       " '６',\n",
       " 'ˤ',\n",
       " 'ǫ',\n",
       " '๖',\n",
       " 'ｐ',\n",
       " 'ͤ',\n",
       " '̱',\n",
       " '١',\n",
       " '۳',\n",
       " '̎',\n",
       " '⁷',\n",
       " 'ɩ',\n",
       " 'ổ',\n",
       " '̐',\n",
       " '̓',\n",
       " 'ρ',\n",
       " 'ƪ',\n",
       " '̷',\n",
       " 'ˉ',\n",
       " 'ʞ',\n",
       " '₅',\n",
       " 'ɚ',\n",
       " 'ɯ',\n",
       " 'ʺ',\n",
       " 'ɲ',\n",
       " '\\u06dd',\n",
       " 'ɻ',\n",
       " '˂',\n",
       " 'ˡ',\n",
       " '位',\n",
       " 'ʹ',\n",
       " 'ʂ',\n",
       " 'ｂ',\n",
       " '８',\n",
       " 'ǭ',\n",
       " '⅙',\n",
       " 'ʧ',\n",
       " '９',\n",
       " 'ʐ',\n",
       " 'ｑ',\n",
       " 'ʋ',\n",
       " '۴',\n",
       " '̕',\n",
       " '̗',\n",
       " 'ʱ',\n",
       " 'ƶ',\n",
       " 'ǁ',\n",
       " '˻',\n",
       " '̴',\n",
       " 'ｔ',\n",
       " 'ḳ',\n",
       " 'ｘ',\n",
       " 'ǯ',\n",
       " 'ɵ',\n",
       " 'ʀ',\n",
       " 'ȱ',\n",
       " '号']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_chars = [c for c in list(embeddings_dict.keys()) if len(c) == 1]\n",
    "glove_symbols = [c for c in glove_chars if not c in safe_characters]\n",
    "glove_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':',\n",
       " '\"',\n",
       " '+',\n",
       " '\\x8e',\n",
       " '<',\n",
       " '–',\n",
       " '\\xa0',\n",
       " '¤',\n",
       " '\\uf04a',\n",
       " 'ª',\n",
       " '£',\n",
       " '$',\n",
       " '@',\n",
       " '|',\n",
       " '、',\n",
       " '¾',\n",
       " ';',\n",
       " '”',\n",
       " '&',\n",
       " '\\x9a',\n",
       " '\\x80',\n",
       " '[',\n",
       " '…',\n",
       " '，',\n",
       " '(',\n",
       " '´',\n",
       " '{',\n",
       " '!',\n",
       " '\\x95',\n",
       " '\\\\',\n",
       " '>',\n",
       " '¿',\n",
       " '`',\n",
       " ')',\n",
       " '¨',\n",
       " '»',\n",
       " '/',\n",
       " '₤',\n",
       " '\\x9e',\n",
       " '★',\n",
       " '%',\n",
       " '*',\n",
       " '.',\n",
       " '\\x85',\n",
       " '·',\n",
       " '®',\n",
       " '°',\n",
       " '©',\n",
       " 'º',\n",
       " '¡',\n",
       " '^',\n",
       " '¦',\n",
       " ']',\n",
       " ',',\n",
       " '}',\n",
       " '«',\n",
       " '#',\n",
       " '?',\n",
       " '\\x91',\n",
       " '\\t',\n",
       " '\\uf0b7',\n",
       " '~',\n",
       " '\\x96',\n",
       " '″',\n",
       " '“',\n",
       " '_',\n",
       " '\\x8d',\n",
       " '-',\n",
       " '½',\n",
       " '=',\n",
       " '\\x84',\n",
       " '\\x97']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jigsaw_chars = set(w for sentence in x_train for w in sentence)\n",
    "jigsaw_symbols = [c for c in jigsaw_chars if not c in safe_characters]\n",
    "jigsaw_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\x8e',\n",
       " '\\xa0',\n",
       " '\\uf04a',\n",
       " '\\x9a',\n",
       " '\\x80',\n",
       " '，',\n",
       " '´',\n",
       " '\\x95',\n",
       " '¨',\n",
       " '\\x9e',\n",
       " '★',\n",
       " '*',\n",
       " '\\x85',\n",
       " '·',\n",
       " '®',\n",
       " '°',\n",
       " '©',\n",
       " 'º',\n",
       " '¦',\n",
       " '\\x91',\n",
       " '\\t',\n",
       " '\\uf0b7',\n",
       " '\\x96',\n",
       " '″',\n",
       " '\\x8d',\n",
       " '\\x84',\n",
       " '\\x97']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols_to_delete = [c for c in jigsaw_symbols if not c in glove_symbols]\n",
    "symbols_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':',\n",
       " '\"',\n",
       " '+',\n",
       " '<',\n",
       " '–',\n",
       " '¤',\n",
       " 'ª',\n",
       " '£',\n",
       " '$',\n",
       " '@',\n",
       " '|',\n",
       " '、',\n",
       " '¾',\n",
       " ';',\n",
       " '”',\n",
       " '&',\n",
       " '[',\n",
       " '…',\n",
       " '(',\n",
       " '{',\n",
       " '!',\n",
       " '\\\\',\n",
       " '>',\n",
       " '¿',\n",
       " '`',\n",
       " ')',\n",
       " '»',\n",
       " '/',\n",
       " '₤',\n",
       " '%',\n",
       " '.',\n",
       " '¡',\n",
       " '^',\n",
       " ']',\n",
       " ',',\n",
       " '}',\n",
       " '«',\n",
       " '#',\n",
       " '?',\n",
       " '~',\n",
       " '“',\n",
       " '_',\n",
       " '-',\n",
       " '½',\n",
       " '=']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols_to_isolate = [c for c in jigsaw_symbols if c in glove_symbols]\n",
    "symbols_to_isolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "    for symbol in symbols_to_delete:\n",
    "        x = x.replace(symbol, ' ')\n",
    "    for symbol in symbols_to_isolate:\n",
    "        x = x.replace(symbol, ' ' + symbol + ' ')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [clean_text(sentence) for sentence in x_train]\n",
    "x_test = [clean_text(sentence) for sentence in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(c for c in set(w for sentence in x_train for w in sentence) if not c in safe_characters) == set(symbols_to_isolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_contractions(x):\n",
    "    x = tokenizer.tokenize(x)\n",
    "    x = ' '.join(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [handle_contractions(sentence) for sentence in x_train]\n",
    "x_test = [handle_contractions(sentence) for sentence in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'briggitta',\n",
       " \"'attitude\",\n",
       " 'nanadini',\n",
       " \"'naughtiness\",\n",
       " 'eshley',\n",
       " 'reagher',\n",
       " \"'porridge\",\n",
       " 'fabulious',\n",
       " 'tellssexually',\n",
       " 'geneticell',\n",
       " \"'mammies\",\n",
       " 'underfelt',\n",
       " 'grmpfli',\n",
       " \"'catwoman\",\n",
       " 'bloodshet',\n",
       " \"'dress\",\n",
       " 'rentarô',\n",
       " 'torpidly',\n",
       " 'corringa',\n",
       " 'injustise',\n",
       " 'kremhild',\n",
       " 'dostoyevskyian',\n",
       " 'i’d',\n",
       " '1946oscars',\n",
       " \"'interesting\",\n",
       " \"'distinct\",\n",
       " 'idiotize',\n",
       " \"'bonjour\",\n",
       " 'chopppers',\n",
       " 'tsanders',\n",
       " 'revetting',\n",
       " 'penpusher',\n",
       " \"'secretly\",\n",
       " 'becherd',\n",
       " 'oppenhoff',\n",
       " \"'singing\",\n",
       " \"'lorenzo\",\n",
       " \"'mona\",\n",
       " 'o’keeffe',\n",
       " \"'silent\",\n",
       " \"'john\",\n",
       " 'tomblinson',\n",
       " 'raisouli',\n",
       " 'baffeling',\n",
       " 'henrikssen',\n",
       " \"'newest\",\n",
       " 'jäniksen',\n",
       " \"'concert\",\n",
       " 'hasslin',\n",
       " 'montagus',\n",
       " \"'hangmen\",\n",
       " 'crusoeland',\n",
       " 'gielguld',\n",
       " 'mcmovies',\n",
       " \"'irreversible\",\n",
       " \"'moebius\",\n",
       " \"'values\",\n",
       " 'physicalisation',\n",
       " \"'disciples\",\n",
       " 'cathernine',\n",
       " '35pm',\n",
       " 'wiitches',\n",
       " 'crimedies',\n",
       " 'priveghi',\n",
       " 'kenovic',\n",
       " 'chaimsaw',\n",
       " \"'intentional\",\n",
       " 'ain’t',\n",
       " \"'street\",\n",
       " 'giovon',\n",
       " 'doyeon',\n",
       " \"'sexual\",\n",
       " 'führmann',\n",
       " \"'reel\",\n",
       " \"f'n\",\n",
       " '211772166650071408',\n",
       " 'thurroughly',\n",
       " 'rosenstraße',\n",
       " \"'here\",\n",
       " 'kristophersson',\n",
       " \"'sci\",\n",
       " 'pssttt',\n",
       " \"'thirteen\",\n",
       " 'orcist',\n",
       " \"ebay'ing\",\n",
       " 'mooded',\n",
       " \"'dubbing\",\n",
       " 'scosese',\n",
       " 'overruse',\n",
       " \"'of\",\n",
       " 'smyrner',\n",
       " 'companymanwho',\n",
       " 'thirtysomthing',\n",
       " \"'suspension\",\n",
       " \"'fackin\",\n",
       " 'filmhas',\n",
       " 'ollllld',\n",
       " 'cerimonee',\n",
       " 'krutcher',\n",
       " 'batbot',\n",
       " 'wolodarksy',\n",
       " 'koerschgen',\n",
       " \"'revolutionized\",\n",
       " 'pressuburger',\n",
       " 'baastard',\n",
       " 'computerizd',\n",
       " \"'platinum\",\n",
       " 'logophobic',\n",
       " \"'liar\",\n",
       " 'chilklis',\n",
       " 'gallindo',\n",
       " \"'sependipity\",\n",
       " 'titallition',\n",
       " 'incompassionate',\n",
       " 'wadget',\n",
       " 'ludicrosity',\n",
       " 'goodgfellas',\n",
       " 'curiculums',\n",
       " \"'welcome\",\n",
       " 'yugonostalgic',\n",
       " \"'frolics\",\n",
       " 'kerkor',\n",
       " 'reidelsheimer',\n",
       " 'shaymalan',\n",
       " 'yukfest',\n",
       " 'salliwan',\n",
       " 'keuck',\n",
       " 'groovythere',\n",
       " \"'crossing\",\n",
       " 'overglamorize',\n",
       " 'mahalovic',\n",
       " \"'cash\",\n",
       " \"'upset\",\n",
       " \"'misbegotten\",\n",
       " 'repetitevness',\n",
       " 'johnl3d',\n",
       " 'mainsequence',\n",
       " 'körkarlen',\n",
       " \"'ladrock\",\n",
       " 'mcchildren',\n",
       " 'whitezombie',\n",
       " 'belivably',\n",
       " \"'can\",\n",
       " 'filmograpghy',\n",
       " \"'followers\",\n",
       " 'lactée',\n",
       " 'jrchovský',\n",
       " 'sincronicity',\n",
       " \"'hitman\",\n",
       " \"'digga\",\n",
       " 'suprsingly',\n",
       " 'horrormoviejournal',\n",
       " 'birthdaypresent',\n",
       " 'coldshitaction',\n",
       " \"'lawrence\",\n",
       " 'trinklets',\n",
       " 'fowlmouth',\n",
       " 'pointlss',\n",
       " 'brianiac',\n",
       " 'schelsinger',\n",
       " \"'twelve\",\n",
       " 'momentsit',\n",
       " 'horsecoach4hire',\n",
       " 'furtermore',\n",
       " 'monetegna',\n",
       " 'asidelightly',\n",
       " \"'any\",\n",
       " 'argentophile',\n",
       " \"'rubber\",\n",
       " 'kubozuka',\n",
       " \"blow'em\",\n",
       " 'dureyea',\n",
       " 'originaland',\n",
       " \"'sexism\",\n",
       " 'frankenscience',\n",
       " 'galitzien',\n",
       " \"6'7\",\n",
       " 'meaneys',\n",
       " 'unutilzed',\n",
       " 'umecki',\n",
       " \"'christiany\",\n",
       " 'carayiannis',\n",
       " 'gonifs',\n",
       " \"'fantasia\",\n",
       " 'cbgbomfug',\n",
       " 'malenkaya',\n",
       " 'riefenstahls',\n",
       " \"'ahlan\",\n",
       " 'medusans',\n",
       " \"u'an\",\n",
       " 'zazvorkova',\n",
       " '10recommendation',\n",
       " 'polick',\n",
       " 'margoyles',\n",
       " \"'boy\",\n",
       " 'brodzki',\n",
       " 'sexploitational',\n",
       " 'despotovich',\n",
       " 'mauvorneen',\n",
       " 'bonisseur',\n",
       " \"'invaded\",\n",
       " \"'guilty\",\n",
       " 'castis',\n",
       " 'relentlessy',\n",
       " 'washanded',\n",
       " 'lineyuck',\n",
       " 'toturro',\n",
       " 'begyndere',\n",
       " \"'dodge\",\n",
       " \"'sleeper\",\n",
       " 'wisards',\n",
       " 'revealedi',\n",
       " \"'tittybangbang\",\n",
       " 'hallbless',\n",
       " 'sexscenes',\n",
       " 'mummifies',\n",
       " 'despoticshakespearean',\n",
       " \"'coast\",\n",
       " 'vyjantimala',\n",
       " 'asagoro',\n",
       " 'fuccon',\n",
       " 'urusevsky',\n",
       " 'scenesadded',\n",
       " 'ausrock',\n",
       " 'siriaque',\n",
       " 'sluzbenom',\n",
       " 'obwat',\n",
       " \"tart'n'tangy\",\n",
       " 'dibler',\n",
       " 'alselmo',\n",
       " \"'porno\",\n",
       " \"'poets\",\n",
       " 'okerland',\n",
       " 'kameej',\n",
       " 'fuuuuunnnnnyyyyy',\n",
       " 'mendelito',\n",
       " 'toentertain',\n",
       " 'funjatta',\n",
       " 'allllllllllllllllllllllllllllll',\n",
       " \"wand'ring\",\n",
       " 'notsogood',\n",
       " 'wowzors',\n",
       " \"selle's\",\n",
       " 'ejiofer',\n",
       " 'alheimsins',\n",
       " 'turtledom',\n",
       " 'shiris',\n",
       " 'whince',\n",
       " 'goldwyns',\n",
       " 'smittened',\n",
       " 'ferrot',\n",
       " 'unsubtlety',\n",
       " \"'cheap\",\n",
       " \"'leaves\",\n",
       " 'inpermanance',\n",
       " 'scrumptiolicious',\n",
       " 'willreally',\n",
       " \"'unpatriotic\",\n",
       " 'cahiil',\n",
       " 'moarn',\n",
       " 'filmdaraar',\n",
       " 'unkiddy',\n",
       " 'tcampo23',\n",
       " \"'detective\",\n",
       " \"'sara\",\n",
       " 'dagma',\n",
       " 'crispon',\n",
       " 'crimpton',\n",
       " '332960073452',\n",
       " 'mecbeths',\n",
       " 'tetsurô',\n",
       " 'srfollowing',\n",
       " 'hortyon',\n",
       " 'wallece',\n",
       " 'devagan',\n",
       " 'noirest',\n",
       " \"'rejseholdet\",\n",
       " \"'fighting\",\n",
       " 'suuuuuuuuuuuucks',\n",
       " 'ghibi',\n",
       " \"'herself\",\n",
       " 'dresdel',\n",
       " \"'underworld\",\n",
       " \"'few\",\n",
       " 'protosovich',\n",
       " 'satired',\n",
       " 'bitchdom',\n",
       " 'disharmoniously',\n",
       " 'chihuahuawoman',\n",
       " \"'reloaded\",\n",
       " \"'trod\",\n",
       " 'shrieff',\n",
       " 'policticly',\n",
       " 'grainyblack',\n",
       " \"'advisor\",\n",
       " 'watchnincompoops',\n",
       " 'scctm',\n",
       " 'skirtales',\n",
       " 'eastwestdvd',\n",
       " \"'nazarin\",\n",
       " 'trieshard',\n",
       " \"'fill\",\n",
       " 'desperatation',\n",
       " 'influencehow',\n",
       " \"'sophisticated\",\n",
       " 'mikcey',\n",
       " 'doctresses',\n",
       " \"'eureka\",\n",
       " \"'depth\",\n",
       " 'trappus',\n",
       " \"'noble\",\n",
       " \"'fur\",\n",
       " 'slipknotian',\n",
       " 'blixens',\n",
       " 'oolama',\n",
       " 'overheaded',\n",
       " 'xuejian',\n",
       " \"'escape\",\n",
       " \"'jacob\",\n",
       " 'palasteine',\n",
       " 'luckinbull',\n",
       " \"people'watch\",\n",
       " \"'towering\",\n",
       " 'emblemized',\n",
       " 'prophess',\n",
       " 'nerdlebrain',\n",
       " 'booooh',\n",
       " \"'gladys\",\n",
       " 'demonoïd',\n",
       " \"'memorable\",\n",
       " 'gesellich',\n",
       " \"'chill\",\n",
       " \"'burbs\",\n",
       " \"'bout\",\n",
       " 'trivialboring',\n",
       " 'thenewamerican',\n",
       " 'forshadow',\n",
       " 'bladrick',\n",
       " \"'jew\",\n",
       " 'ceremonyand',\n",
       " 'duhanel',\n",
       " 'screwiest',\n",
       " \"'buddy\",\n",
       " 'azaleigh',\n",
       " \"have't\",\n",
       " 'thadol',\n",
       " 'maligners',\n",
       " 'headachelara',\n",
       " 'mst3king',\n",
       " 'jesminder',\n",
       " 'felcity',\n",
       " 'bravoda',\n",
       " 'konstadinou',\n",
       " 'larrakin',\n",
       " \"'satya\",\n",
       " 'altioklar',\n",
       " 'afest',\n",
       " 'ubasti',\n",
       " 'greyzone',\n",
       " 'jarchovský',\n",
       " \"'doppleganger\",\n",
       " 'sebeva',\n",
       " \"'hell\",\n",
       " \"'lethal\",\n",
       " 'thefilmmakers',\n",
       " \"'sink\",\n",
       " \"'ole\",\n",
       " \"'eskimo\",\n",
       " 'curdeling',\n",
       " \"'torture\",\n",
       " 'jannuci',\n",
       " 'stuffhi',\n",
       " 'patresi',\n",
       " 'lestrad',\n",
       " 'lehch',\n",
       " 'stunningthe',\n",
       " 'sacrine',\n",
       " \"'power\",\n",
       " 'enormenent',\n",
       " \"'thelittlesongbird\",\n",
       " 'trekkish',\n",
       " 'sordie',\n",
       " \"trite'n'turgid\",\n",
       " 'killedand',\n",
       " 'geeeeeeeeeeeeez',\n",
       " 'guétary',\n",
       " 'harummpf',\n",
       " 'grossbach',\n",
       " 'rabbitdr',\n",
       " 'yakmallah',\n",
       " 'camarary',\n",
       " 'arsonistresponsible',\n",
       " 'flimi',\n",
       " 'zwrite',\n",
       " 'zindulka',\n",
       " \"'enlightened\",\n",
       " 'dumblaine',\n",
       " \"'fort\",\n",
       " 'heeeeaaarrt',\n",
       " 'pestario',\n",
       " \"'morality\",\n",
       " 'hollywoodised',\n",
       " 'numbingness',\n",
       " 'enviormentally',\n",
       " \"'rotoscope\",\n",
       " 'bastketball',\n",
       " \"'dean\",\n",
       " 'ceislerová',\n",
       " 'tatumi',\n",
       " 'chaleuruex',\n",
       " 'krelle',\n",
       " 'tt0098238',\n",
       " \"'was\",\n",
       " \"'wrestling\",\n",
       " \"'goblin\",\n",
       " 'cornily',\n",
       " 'liveif',\n",
       " '10segment',\n",
       " 'slurrings',\n",
       " 'zanatos',\n",
       " \"'marty\",\n",
       " 'bressonian',\n",
       " \"'soul\",\n",
       " \"'greedy\",\n",
       " \"'karate\",\n",
       " \"'existenz\",\n",
       " \"'please\",\n",
       " \"berkley'ish\",\n",
       " \"'murderous\",\n",
       " \"'debacle\",\n",
       " \"'wendigo\",\n",
       " 'brooklyners',\n",
       " \"'sarkar\",\n",
       " 'gorylicious',\n",
       " \"'whistler\",\n",
       " 'halholbrook',\n",
       " 'lawnmowerman',\n",
       " 'alllriiiiight',\n",
       " 'heroine’s',\n",
       " 'hisaichi',\n",
       " 'olshanskey',\n",
       " 'beuregard',\n",
       " \"'peoples\",\n",
       " 'possiblei',\n",
       " 'temptate',\n",
       " \"'oliver\",\n",
       " 'nyaako',\n",
       " 'aleopathic',\n",
       " 'checkan',\n",
       " 'hiblade',\n",
       " 'umbopa',\n",
       " 'hackism',\n",
       " 'dorday',\n",
       " 'bocabonita',\n",
       " 'consilation',\n",
       " 'brilliantwarning',\n",
       " \"'side\",\n",
       " 'eezma',\n",
       " 'alexcanr',\n",
       " 'autorenfilm',\n",
       " 'eisenstien',\n",
       " 'whistlerian',\n",
       " 'afictionado',\n",
       " 'cinemaphotography',\n",
       " 'clockworkish',\n",
       " 'mahhahahahah',\n",
       " 'seinfeldish',\n",
       " \"'bluebeard\",\n",
       " \"'ss\",\n",
       " 'hhe1',\n",
       " \"'actresses\",\n",
       " 'anthropoligist',\n",
       " 'unfunnily',\n",
       " 'lakawanna',\n",
       " \"'lake\",\n",
       " \"'lighter\",\n",
       " \"'armageddon\",\n",
       " \"'reality\",\n",
       " 'hilcox',\n",
       " 'staccioli',\n",
       " \"side'scenario\",\n",
       " 'leeli',\n",
       " \"'troubled\",\n",
       " 'corck',\n",
       " 'svennberg',\n",
       " 'agolden',\n",
       " \"'step\",\n",
       " 'mediocreland',\n",
       " \"'studio\",\n",
       " 'zorkin',\n",
       " 'issuey',\n",
       " 'amrapurkars',\n",
       " 'tt0087072',\n",
       " 'lesboes',\n",
       " 'phenomonauts',\n",
       " 'huldre',\n",
       " \"'cure\",\n",
       " 'topdany',\n",
       " 'beatened',\n",
       " 'eurofilm',\n",
       " 'isthar',\n",
       " '5mixed',\n",
       " 'tonelessness',\n",
       " 'sttion',\n",
       " 'fleuretty',\n",
       " \"'shiny\",\n",
       " \"'twilight\",\n",
       " 'zorie',\n",
       " 'himeshemraan',\n",
       " \"'proprietary\",\n",
       " 'sadomania',\n",
       " 'allosauros',\n",
       " \"'notting\",\n",
       " 'languagerating',\n",
       " 'plainland',\n",
       " 'vasilikov',\n",
       " 'sweeking',\n",
       " \"'into\",\n",
       " \"'upgrade\",\n",
       " 'ninetysomething',\n",
       " 'bubban',\n",
       " 'nihlan',\n",
       " 'endedness',\n",
       " \"'hamage\",\n",
       " \"'witty\",\n",
       " 'belush',\n",
       " 'laughablejesus',\n",
       " 'geislerová',\n",
       " 'legionairres',\n",
       " 'kudso',\n",
       " 'sesech',\n",
       " \"'bub\",\n",
       " 'bilgey',\n",
       " 'smorsgabord',\n",
       " 'rumpity',\n",
       " \"'learn\",\n",
       " 'mediocreamongst',\n",
       " 'kristels',\n",
       " 'celticism',\n",
       " 'coartship',\n",
       " \"'fuck\",\n",
       " 'eniemy',\n",
       " 'serafian',\n",
       " 'amargas',\n",
       " 'schizophernia',\n",
       " 'supervisoring',\n",
       " 'cyrilnik',\n",
       " 'frightner',\n",
       " 'amithab',\n",
       " \"'ard\",\n",
       " \"'memories\",\n",
       " \"'goodbye\",\n",
       " 'kratina',\n",
       " \"'mutant\",\n",
       " \"'forced\",\n",
       " 'unmystied',\n",
       " 'referend',\n",
       " 'pafific',\n",
       " 'stuntpeople',\n",
       " \"'injured\",\n",
       " \"'craig\",\n",
       " 'hillybillyish',\n",
       " 'hamwork',\n",
       " 'tatoya',\n",
       " \"'eastern\",\n",
       " \"'prince\",\n",
       " 'ungeneral',\n",
       " 'phainomena',\n",
       " 'huuuuuuuuuuge',\n",
       " 'phasered',\n",
       " 'andhormones',\n",
       " \"'adult\",\n",
       " \"'balderdash\",\n",
       " \"'captain\",\n",
       " 'gilliamesque',\n",
       " 'amateurly',\n",
       " 'undoubtledly',\n",
       " \"'hiccups\",\n",
       " 'guiccioli',\n",
       " \"'harris\",\n",
       " 'someup',\n",
       " 'viscontis',\n",
       " 'dolphs',\n",
       " 'pulasky',\n",
       " 'ridcardo',\n",
       " \"'aladdin\",\n",
       " 'claustraphobia',\n",
       " \"'strip\",\n",
       " 'professionaled',\n",
       " \"'characters\",\n",
       " \"'transylvania\",\n",
       " 'jaregard',\n",
       " 'alterated',\n",
       " 'reeeealllly',\n",
       " 'blörg',\n",
       " \"'cartoonish\",\n",
       " 'magellan33',\n",
       " 'ennnnnnnnndddddd',\n",
       " 'absoulely',\n",
       " \"'girlfriend\",\n",
       " 'sorrano',\n",
       " 'economises',\n",
       " 'moviehunter',\n",
       " 'unfortuntaely',\n",
       " 'disquilification',\n",
       " 'beswicke',\n",
       " 'peckingly',\n",
       " \"'prayer\",\n",
       " 'iwerk',\n",
       " 'abominibal',\n",
       " 'happpeniiiinngggg',\n",
       " 'zaphoidps',\n",
       " 'caligary',\n",
       " 'schlockiness',\n",
       " 'zarkorr',\n",
       " 'guilliotined',\n",
       " 'coatamundis',\n",
       " \"'erasurehead\",\n",
       " 'fehmiu',\n",
       " 'feinstones',\n",
       " 'chaeles',\n",
       " \"'charmed\",\n",
       " 'alterns',\n",
       " \"'glass\",\n",
       " 'compositionactually',\n",
       " 'shoenumber',\n",
       " 'trampiness',\n",
       " 'futuremove',\n",
       " 'zomcoms',\n",
       " \"'frankenstein\",\n",
       " 'mousiness',\n",
       " \"'modernized\",\n",
       " 'pistoleers',\n",
       " 'cosmotos',\n",
       " 'layfields',\n",
       " 'doxen',\n",
       " 'overactors',\n",
       " '33bc',\n",
       " 'drivvle',\n",
       " 'paravasam',\n",
       " \"'near\",\n",
       " 'monoan',\n",
       " 'studentpolitical',\n",
       " \"'innocent\",\n",
       " 'koqs',\n",
       " 'nukkin',\n",
       " \"'st\",\n",
       " \"'devdas\",\n",
       " 'piss3d',\n",
       " \"'wacky\",\n",
       " \"'law\",\n",
       " 'emabrrassed',\n",
       " 'thoughtfor',\n",
       " \"'jeepers\",\n",
       " 'benatatos',\n",
       " 'napir',\n",
       " 'hisaiahi',\n",
       " 'haruhai',\n",
       " 'jaglon',\n",
       " 'gagsmith',\n",
       " \"'fa\",\n",
       " 'sinhue',\n",
       " 'tilneys',\n",
       " 'hupping',\n",
       " \"'creates\",\n",
       " 'jerkingly',\n",
       " \"'entrance\",\n",
       " \"'lizzie\",\n",
       " 'borowczyks',\n",
       " 'ingor',\n",
       " 'unparticular',\n",
       " 'kristevian',\n",
       " 'yasudo',\n",
       " 'sopranoes',\n",
       " \"'grittier\",\n",
       " 'buxomed',\n",
       " 'machism',\n",
       " 'ahet',\n",
       " \"'wee\",\n",
       " 'incredibilus',\n",
       " 'poutily',\n",
       " \"'miss\",\n",
       " 'shadowfrank',\n",
       " 'reichstagsbuilding',\n",
       " 'plotas',\n",
       " 'introductionne',\n",
       " 'azfel',\n",
       " 'aylet',\n",
       " \"'wizard\",\n",
       " 'kathly',\n",
       " 'murkwood',\n",
       " 'mastantonio',\n",
       " 'défroqué',\n",
       " 'puzzu',\n",
       " \"'charm\",\n",
       " 'vieila',\n",
       " 'catelain’s',\n",
       " 'colorised',\n",
       " \"'plot\",\n",
       " 'huiang',\n",
       " 'warecki',\n",
       " 'momoselle',\n",
       " \"'cathy\",\n",
       " 'merlik',\n",
       " \"'madonna\",\n",
       " 'mocumentaries',\n",
       " 'fabrazio',\n",
       " 'ambientation',\n",
       " \"'idea\",\n",
       " 'excrucriatingly',\n",
       " 'fircombe',\n",
       " 'dimitru',\n",
       " \"'games\",\n",
       " 'piscapo',\n",
       " 'bloodsurf',\n",
       " 'deloiselle',\n",
       " 'spredakos',\n",
       " 'rrratman',\n",
       " 'strageness',\n",
       " \"'housewife\",\n",
       " 'nothingcons',\n",
       " 'lartigau',\n",
       " 'realityshowfictionnal',\n",
       " 'matterthis',\n",
       " 'manigot',\n",
       " 'manhating',\n",
       " 'breastage',\n",
       " \"'sin\",\n",
       " 'mandylan',\n",
       " 'folksforget',\n",
       " 'bergonzini',\n",
       " \"'surreal\",\n",
       " '1945when',\n",
       " 'cyborgish',\n",
       " 'rollercaoster',\n",
       " \"'believers\",\n",
       " 'overcaution',\n",
       " 'artfilm',\n",
       " \"'farce\",\n",
       " 'eastood',\n",
       " 'deelan',\n",
       " 'illinear',\n",
       " 'yellowcoats',\n",
       " \"sant'angelo\",\n",
       " 'triviata',\n",
       " 'couterie',\n",
       " 'frommage',\n",
       " \"'solve\",\n",
       " 'edmednson',\n",
       " \"'preservatives\",\n",
       " \"'victory\",\n",
       " 'slaone',\n",
       " 'teamnot',\n",
       " 'arreté',\n",
       " 'shachnovelle',\n",
       " 'incrediblycandid',\n",
       " 'treech',\n",
       " 'othenin',\n",
       " 'tbor',\n",
       " 'unpretencious',\n",
       " 'staphani',\n",
       " \"'attack\",\n",
       " 'ketin',\n",
       " \"'telly\",\n",
       " 'melenzana',\n",
       " 'ryhs',\n",
       " 'decorsia',\n",
       " 'someafter',\n",
       " 'whomevers',\n",
       " 'rentalrack',\n",
       " 'tribunesan',\n",
       " 'harrloe',\n",
       " \"seen'em\",\n",
       " 'lassander',\n",
       " '10cheesiness',\n",
       " 'autobiograhical',\n",
       " 'birgette',\n",
       " 'traquees',\n",
       " \"'necklace\",\n",
       " \"'reign\",\n",
       " 'demonous',\n",
       " \"'nerds\",\n",
       " 'rehmaan',\n",
       " 'exitenz',\n",
       " \"'grand\",\n",
       " 'breakumentarions',\n",
       " 'lesking',\n",
       " 'bashirov',\n",
       " \"'religious\",\n",
       " \"'language\",\n",
       " 'godisnowhere',\n",
       " 'mäx',\n",
       " 'loveability',\n",
       " \"'price\",\n",
       " \"'events\",\n",
       " 'leguizemo',\n",
       " \"i'ld\",\n",
       " \"'ultimate\",\n",
       " 'esbjørn',\n",
       " \"'exotic\",\n",
       " \"'brutus\",\n",
       " \"'hair\",\n",
       " 'greeniwch',\n",
       " \"'scotties\",\n",
       " 'rolleball',\n",
       " \"a'79\",\n",
       " 'hrstka',\n",
       " \"conclusion'vampires\",\n",
       " \"'things\",\n",
       " \"'steamboat\",\n",
       " \"'gritty\",\n",
       " 'chihwasun',\n",
       " 'papamoschou',\n",
       " 'k9pi',\n",
       " 'oblowitz',\n",
       " 'sagebrusher',\n",
       " \"'domino\",\n",
       " 'havegotten',\n",
       " 'hysterion',\n",
       " 'greatestthis',\n",
       " \"'shoe\",\n",
       " 'rodential',\n",
       " 'dataeven',\n",
       " \"'popular\",\n",
       " 'silvermann',\n",
       " 'mornell',\n",
       " 'baytan',\n",
       " \"'gabby\",\n",
       " 'blindpassasjer',\n",
       " 'subtiteld',\n",
       " \"'but\",\n",
       " 'dmtryk',\n",
       " 'helgeberger',\n",
       " 'towney',\n",
       " 'filmedthe',\n",
       " 'abanazer',\n",
       " 'checkcameras',\n",
       " 'shiktak',\n",
       " 'atridgemont',\n",
       " \"'d'amato\",\n",
       " \"'snow\",\n",
       " \"'ventures\",\n",
       " \"'acting\",\n",
       " 'commitophobe',\n",
       " \"'films\",\n",
       " 'aheadwhy',\n",
       " \"'ashanti\",\n",
       " 'dillweeds',\n",
       " \"'him\",\n",
       " 'conchatta',\n",
       " 'favorize',\n",
       " 'nuovomondo',\n",
       " 'mrudul',\n",
       " 'carpentered',\n",
       " 'hasfurnished',\n",
       " 'kalirai',\n",
       " \"'kid\",\n",
       " 'järvilaturi',\n",
       " 'keital',\n",
       " 'euthenased',\n",
       " \"'scarecrows\",\n",
       " '21849889for',\n",
       " 'parasarolophus',\n",
       " 'leinson',\n",
       " 'mcmusicnotes',\n",
       " 'vitavetavegamin',\n",
       " \"'dynasty\",\n",
       " \"drool'athon\",\n",
       " 'slapi',\n",
       " 'leeeze',\n",
       " 'blaxsploitation',\n",
       " \"i'l\",\n",
       " 'toumania',\n",
       " 'moviesalso',\n",
       " \"'somewhere\",\n",
       " 'suicidees',\n",
       " 'hollywoodize',\n",
       " 'mned',\n",
       " \"d'obici\",\n",
       " 'addytown',\n",
       " 'story’s',\n",
       " 'veging',\n",
       " 'filmwhile',\n",
       " \"'caboose\",\n",
       " \"'war\",\n",
       " 'zaniacs',\n",
       " 'danelia',\n",
       " 'frickkin',\n",
       " 'acquaintaces',\n",
       " \"'thine\",\n",
       " 'lavud',\n",
       " \"6'5\",\n",
       " \"'hare\",\n",
       " 'bogdanoviches',\n",
       " \"'kal\",\n",
       " \"'discover\",\n",
       " 'yaarrrghhh',\n",
       " 'riiiiiike',\n",
       " 'robespierres',\n",
       " 'uuuuaaa',\n",
       " 'exculsivley',\n",
       " 'hahagruner',\n",
       " \"'find\",\n",
       " \"'basically\",\n",
       " 'whpat',\n",
       " '10umney',\n",
       " 'gridge',\n",
       " 'godamit',\n",
       " \"'documentary\",\n",
       " 'thomajan',\n",
       " 'exploitave',\n",
       " \"'dillinger\",\n",
       " 'babalicious',\n",
       " 'obrow',\n",
       " \"'cobra\",\n",
       " 'segrain',\n",
       " \"'flicker\",\n",
       " 'shrifts',\n",
       " \"'caca\",\n",
       " \"'stand\",\n",
       " 'lygter',\n",
       " 'kywildflower16',\n",
       " \"'gahden\",\n",
       " 'protrolling',\n",
       " 'cetniks',\n",
       " 'oedpius',\n",
       " \"'voltando\",\n",
       " 'canrun',\n",
       " 'mastobatory',\n",
       " 'enterieur',\n",
       " 'tiags',\n",
       " 'htmit',\n",
       " 'sverák',\n",
       " \"'thunderball\",\n",
       " 'metschurat',\n",
       " 'belengur',\n",
       " 'midthunder',\n",
       " 'unmodest',\n",
       " 'nana’s',\n",
       " 'durenmatt',\n",
       " 'gorewhores',\n",
       " \"'disgustingly\",\n",
       " 'inferenced',\n",
       " 'shoates',\n",
       " 'ecchhhh',\n",
       " \"'rosemary\",\n",
       " 'ishoos',\n",
       " 'rahoooooool',\n",
       " 'yamika',\n",
       " 'sunbacked',\n",
       " 'framke',\n",
       " \"'restyling\",\n",
       " \"'marion\",\n",
       " 'accidentee',\n",
       " 'avoidthis',\n",
       " \"'saying\",\n",
       " 'armsted',\n",
       " 'gooping',\n",
       " \"'lazarus\",\n",
       " '66er',\n",
       " \"'frantic\",\n",
       " \"'singh\",\n",
       " 'dissapearence',\n",
       " 'wanish',\n",
       " 'megaeuros',\n",
       " \"'would\",\n",
       " 'zonfeld',\n",
       " \"'elephants\",\n",
       " 'pickhaver',\n",
       " 'necroborg',\n",
       " 'leastthe',\n",
       " 'jejeune',\n",
       " 'msyterious',\n",
       " \"'raider\",\n",
       " 'wurtzle',\n",
       " 'yeiks',\n",
       " 'shammer',\n",
       " 'the1940',\n",
       " 'senaglese',\n",
       " 'bamjust',\n",
       " 'underimpressed',\n",
       " 'scarllett',\n",
       " 'intimatelook',\n",
       " \"'fat\",\n",
       " 'ishkanani',\n",
       " 'cowboyoutfit',\n",
       " 'highing',\n",
       " 'chasidik',\n",
       " \"'evils\",\n",
       " 'tt0060164',\n",
       " \"did't\",\n",
       " 'intertyne',\n",
       " 'diahnn',\n",
       " 'iguanadons',\n",
       " 'departmentn',\n",
       " 'cheirel',\n",
       " 'kanh',\n",
       " \"'jane\",\n",
       " 'futuramafan1987',\n",
       " 'devonsville',\n",
       " ...}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(w for sentence in x_train for w in sentence.split() if w not in embeddings_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_quote(text):\n",
    "    return ' '.join(x[1:] if x.startswith(\"'\") and len(x) > 1 else x for x in text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [fix_quote(sentence) for sentence in x_train]\n",
    "x_test = [fix_quote(sentence) for sentence in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train_preprocessed = x_train\n",
    "\n",
    "x_test_preprocessed = x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['text'] = df['text'].apply(lambda x: preprocesser.text_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import pickle\\n\\nos.makedirs('pickle', exist_ok=True)\\n\\nwith open('pickle\\\\data.pickle', 'wb') as f:\\n    pickle.dump([x_test, y_test], f)\\nf.close()\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pickle\n",
    "\n",
    "os.makedirs('pickle', exist_ok=True)\n",
    "\n",
    "with open('pickle\\\\data.pickle', 'wb') as f:\n",
    "    pickle.dump([x_test, y_test], f)\n",
    "f.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_preprocessed = [preprocesser.text_preprocessing(sentence) for sentence in x_train]\n",
    "#x_test_preprocessed = [preprocesser.text_preprocessing(sentence) for sentence in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Preprocessed texts')\n",
    "#print(x_train_preprocessed[:3])\n",
    "#print(x_test_preprocessed[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['text'] = df['text'].apply(lambda x: preprocesser.text_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import pickle\\n\\nos.makedirs('pickle', exist_ok=True)\\n\\nwith open('pickle\\\\data.pickle', 'wb') as f:\\n    pickle.dump([x_test, y_test], f)\\nf.close()\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pickle\n",
    "\n",
    "os.makedirs('pickle', exist_ok=True)\n",
    "\n",
    "with open('pickle\\\\data.pickle', 'wb') as f:\n",
    "    pickle.dump([x_test, y_test], f)\n",
    "f.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.utils import shuffle\\n\\nx_train, x_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size = 0.33, shuffle = True)\\n\\nx_train = list(x_train)\\nx_test = list(x_test)\\n\\ny_train = list(y_train)\\ny_test = list(y_test)\\n\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size = 0.33, shuffle = True)\n",
    "\n",
    "x_train = list(x_train)\n",
    "x_test = list(x_test)\n",
    "\n",
    "y_train = list(y_train)\n",
    "y_test = list(y_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88087 unique tokens.\n",
      "Shape of train data tensor: (33500, 2640)\n",
      "Shape of train label tensor: (33500,)\n",
      "Shape of test data tensor: (16500, 2640)\n",
      "Shape of test label tensor: (16500,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train_preprocessed)\n",
    "\n",
    "maxlen = max([len(t.split()) for t in x_train_preprocessed])\n",
    "\n",
    "words_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train_preprocessed)\n",
    "test_sequences = tokenizer.texts_to_sequences(x_test_preprocessed)\n",
    "\n",
    "print('Found %s unique tokens.' % len(tokenizer.word_index))\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen = maxlen)\n",
    "test_data = pad_sequences(test_sequences, maxlen = maxlen)\n",
    "\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n",
    "print('Shape of train data tensor:', train_data.shape)\n",
    "print('Shape of train label tensor:', y_train.shape)\n",
    "\n",
    "print('Shape of test data tensor:', test_data.shape)\n",
    "print('Shape of test label tensor:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import pickle\\n\\nos.makedirs('pickle', exist_ok=True)\\n\\nwith open('pickle\\\\tokenizer.pickle', 'wb') as f:\\n    pickle.dump([tokenizer, maxlen], f)\\nf.close()\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pickle\n",
    "\n",
    "os.makedirs('pickle', exist_ok=True)\n",
    "\n",
    "with open('pickle\\\\tokenizer.pickle', 'wb') as f:\n",
    "    pickle.dump([tokenizer, maxlen], f)\n",
    "f.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  461,  853, 1289],\n",
       "       [   0,    0,    0, ...,  191,  191,   58],\n",
       "       [   0,    0,    0, ...,   44,    4,  158],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  132,   23,   76],\n",
       "       [   0,    0,    0, ...,    5,  261,    7],\n",
       "       [   0,    0,    0, ...,    6,  419,  364]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%store test_data\\n%store x_test\\n%store y_test\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%store test_data\n",
    "%store x_test\n",
    "%store y_test\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=3\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.1,\n",
    "        patience=3,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fitted_model(dropout = 0.5, layer_num = 1, init_mode='uniform', batch_size = 128):\n",
    "    \n",
    "    print('\\n', f'Training Model with:', '\\n',\n",
    "    f'* dropout = {dropout};', '\\n',\n",
    "    f'* number of hidden layers = {layer_num};', '\\n',\n",
    "    f'* init mode = {init_mode};', '\\n',\n",
    "    f'* batch size = {batch_size}')\n",
    "    \n",
    "    def add_layers():\n",
    "        for i in range (0, layer_num):\n",
    "            model.add(Dense(64, kernel_initializer=init_mode, activation='relu'))\n",
    "            model.add(Dropout(rate=dropout))\n",
    "    \n",
    "    EMBEDDING_DIM = 100\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(words_size, EMBEDDING_DIM, input_length=maxlen))\n",
    "    model.add(Flatten())\n",
    "    add_layers()\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "    history = model.fit(train_data, y_train,\n",
    "                        epochs=10,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_split=0.2,\n",
    "                        verbose = 2)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict(dropout = [0.2, 0.5, 0.65, 0.8],\n",
    "                       layer_num = [1,2,3],\n",
    "                       batch_size =[128,512],\n",
    "                       init_mode = ['uniform', 'lecun_uniform', 'normal', \n",
    "                                    'glorot_normal', 'glorot_uniform']\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Model with: \n",
      " * dropout = 0.2; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = uniform; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 24s - loss: 0.8932 - acc: 0.5036 - val_loss: 0.6918 - val_acc: 0.6207\n",
      "Epoch 2/10\n",
      "26800/26800 - 22s - loss: 0.4863 - acc: 0.7517 - val_loss: 0.4468 - val_acc: 0.8010\n",
      "Epoch 3/10\n",
      "26800/26800 - 22s - loss: 0.2094 - acc: 0.9231 - val_loss: 0.2615 - val_acc: 0.8964\n",
      "Epoch 4/10\n",
      "26800/26800 - 23s - loss: 0.0914 - acc: 0.9713 - val_loss: 0.3450 - val_acc: 0.8866\n",
      "Epoch 5/10\n",
      "26800/26800 - 23s - loss: 0.0316 - acc: 0.9904 - val_loss: 0.3813 - val_acc: 0.8822\n",
      "Epoch 6/10\n",
      "26800/26800 - 22s - loss: 0.0161 - acc: 0.9952 - val_loss: 0.4847 - val_acc: 0.8919\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.5; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = uniform; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 22s - loss: 0.9220 - acc: 0.4999 - val_loss: 0.6931 - val_acc: 0.5069\n",
      "Epoch 2/10\n",
      "26800/26800 - 22s - loss: 0.6978 - acc: 0.5138 - val_loss: 0.6966 - val_acc: 0.5045\n",
      "Epoch 3/10\n",
      "26800/26800 - 23s - loss: 0.4175 - acc: 0.8115 - val_loss: 0.2636 - val_acc: 0.8876\n",
      "Epoch 4/10\n",
      "26800/26800 - 23s - loss: 0.1720 - acc: 0.9366 - val_loss: 0.3943 - val_acc: 0.8540\n",
      "Epoch 5/10\n",
      "26800/26800 - 23s - loss: 0.0760 - acc: 0.9735 - val_loss: 0.3227 - val_acc: 0.8869\n",
      "Epoch 6/10\n",
      "26800/26800 - 23s - loss: 0.0530 - acc: 0.9874 - val_loss: 0.4243 - val_acc: 0.8916\n",
      "Epoch 7/10\n",
      "26800/26800 - 23s - loss: 0.0025 - acc: 0.9996 - val_loss: 0.4719 - val_acc: 0.8933\n",
      "Epoch 8/10\n",
      "26800/26800 - 23s - loss: 0.0016 - acc: 0.9997 - val_loss: 0.4741 - val_acc: 0.8927\n",
      "Epoch 9/10\n",
      "26800/26800 - 23s - loss: 0.0013 - acc: 0.9997 - val_loss: 0.4837 - val_acc: 0.8975\n",
      "Epoch 10/10\n",
      "26800/26800 - 23s - loss: 0.0010 - acc: 0.9999 - val_loss: 0.4880 - val_acc: 0.8978\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.65; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = uniform; \n",
      " * batch size = 128\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "26800/26800 - 23s - loss: 0.9087 - acc: 0.4975 - val_loss: 0.6931 - val_acc: 0.5067\n",
      "Epoch 2/10\n",
      "26800/26800 - 23s - loss: 0.7014 - acc: 0.5011 - val_loss: 0.6914 - val_acc: 0.4940\n",
      "Epoch 3/10\n",
      "26800/26800 - 23s - loss: 0.5178 - acc: 0.7317 - val_loss: 0.3014 - val_acc: 0.8809\n",
      "Epoch 4/10\n",
      "26800/26800 - 23s - loss: 0.2291 - acc: 0.9106 - val_loss: 0.2574 - val_acc: 0.8922\n",
      "Epoch 5/10\n",
      "26800/26800 - 23s - loss: 0.1050 - acc: 0.9625 - val_loss: 0.2959 - val_acc: 0.8872\n",
      "Epoch 6/10\n",
      "26800/26800 - 23s - loss: 0.0443 - acc: 0.9844 - val_loss: 0.3829 - val_acc: 0.8894\n",
      "Epoch 7/10\n",
      "26800/26800 - 24s - loss: 0.0312 - acc: 0.9914 - val_loss: 0.4284 - val_acc: 0.8939\n",
      "Epoch 8/10\n",
      "26800/26800 - 24s - loss: 0.0020 - acc: 0.9996 - val_loss: 0.4957 - val_acc: 0.8945\n",
      "Epoch 9/10\n",
      "26800/26800 - 24s - loss: 0.0017 - acc: 0.9999 - val_loss: 0.5082 - val_acc: 0.8949\n",
      "Epoch 10/10\n",
      "26800/26800 - 24s - loss: 0.0011 - acc: 0.9999 - val_loss: 0.5193 - val_acc: 0.8948\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = uniform; \n",
      " * batch size = 128\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "26800/26800 - 25s - loss: 0.7972 - acc: 0.4932 - val_loss: 0.6931 - val_acc: 0.4934\n",
      "Epoch 2/10\n",
      "26800/26800 - 24s - loss: 0.6971 - acc: 0.5022 - val_loss: 0.6948 - val_acc: 0.4934\n",
      "Epoch 3/10\n",
      "26800/26800 - 24s - loss: 0.7014 - acc: 0.5017 - val_loss: 0.6926 - val_acc: 0.4966\n",
      "Epoch 4/10\n",
      "26800/26800 - 24s - loss: 0.6479 - acc: 0.6033 - val_loss: 0.4092 - val_acc: 0.8279\n",
      "Epoch 5/10\n",
      "26800/26800 - 24s - loss: 0.3562 - acc: 0.8521 - val_loss: 0.2693 - val_acc: 0.8857\n",
      "Epoch 6/10\n",
      "26800/26800 - 24s - loss: 0.2054 - acc: 0.9230 - val_loss: 0.2713 - val_acc: 0.8970\n",
      "Epoch 7/10\n",
      "26800/26800 - 24s - loss: 0.1260 - acc: 0.9549 - val_loss: 0.2831 - val_acc: 0.9009\n",
      "Epoch 8/10\n",
      "26800/26800 - 24s - loss: 0.0642 - acc: 0.9776 - val_loss: 0.4907 - val_acc: 0.8763\n",
      "Epoch 9/10\n",
      "26800/26800 - 24s - loss: 0.0241 - acc: 0.9926 - val_loss: 0.4075 - val_acc: 0.9006\n",
      "Epoch 10/10\n",
      "26800/26800 - 24s - loss: 0.0176 - acc: 0.9953 - val_loss: 0.4129 - val_acc: 0.9001\n"
     ]
    }
   ],
   "source": [
    "dict_dropout_histories = {}\n",
    "best_dropout = 0.5\n",
    "best_dropout_acc = 0\n",
    "for i in hyperparameters['dropout']:\n",
    "    history = get_fitted_model(dropout = i)\n",
    "    if max(history.history['val_acc']) > best_dropout_acc:\n",
    "        best_dropout = i\n",
    "        best_dropout_acc = max(history.history['val_acc'])\n",
    "    dict_dropout_histories[str(i)] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90089554\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "print(max(dict_dropout_histories[str(best_dropout)].history['val_acc']))\n",
    "print(best_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = uniform; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 24s - loss: 0.8369 - acc: 0.4969 - val_loss: 0.6931 - val_acc: 0.5070\n",
      "Epoch 2/10\n",
      "26800/26800 - 24s - loss: 0.6959 - acc: 0.4992 - val_loss: 0.6931 - val_acc: 0.4928\n",
      "Epoch 3/10\n",
      "26800/26800 - 24s - loss: 0.7007 - acc: 0.5032 - val_loss: 0.6929 - val_acc: 0.5161\n",
      "Epoch 4/10\n",
      "26800/26800 - 24s - loss: 0.6723 - acc: 0.5695 - val_loss: 0.5363 - val_acc: 0.8094\n",
      "Epoch 5/10\n",
      "26800/26800 - 24s - loss: 0.3927 - acc: 0.8282 - val_loss: 0.2689 - val_acc: 0.8896\n",
      "Epoch 6/10\n",
      "26800/26800 - 24s - loss: 0.2292 - acc: 0.9143 - val_loss: 0.3029 - val_acc: 0.8769\n",
      "Epoch 7/10\n",
      "26800/26800 - 24s - loss: 0.1371 - acc: 0.9504 - val_loss: 0.2903 - val_acc: 0.8928\n",
      "Epoch 8/10\n",
      "26800/26800 - 24s - loss: 0.0775 - acc: 0.9722 - val_loss: 0.3932 - val_acc: 0.8867\n",
      "Epoch 9/10\n",
      "26800/26800 - 24s - loss: 0.0268 - acc: 0.9923 - val_loss: 0.3642 - val_acc: 0.9027\n",
      "Epoch 10/10\n",
      "26800/26800 - 24s - loss: 0.0240 - acc: 0.9932 - val_loss: 0.3647 - val_acc: 0.9016\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 2; \n",
      " * init mode = uniform; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 24s - loss: 0.7493 - acc: 0.4992 - val_loss: 0.6931 - val_acc: 0.5067\n",
      "Epoch 2/10\n",
      "26800/26800 - 24s - loss: 0.6975 - acc: 0.4975 - val_loss: 0.6931 - val_acc: 0.5027\n",
      "Epoch 3/10\n",
      "26800/26800 - 24s - loss: 0.6945 - acc: 0.5007 - val_loss: 0.6930 - val_acc: 0.5284\n",
      "Epoch 4/10\n",
      "26800/26800 - 24s - loss: 0.6947 - acc: 0.5021 - val_loss: 0.6926 - val_acc: 0.5649\n",
      "Epoch 5/10\n",
      "26800/26800 - 24s - loss: 0.6898 - acc: 0.5277 - val_loss: 0.6765 - val_acc: 0.6910\n",
      "Epoch 6/10\n",
      "26800/26800 - 24s - loss: 0.6328 - acc: 0.6184 - val_loss: 0.4650 - val_acc: 0.8328\n",
      "Epoch 7/10\n",
      "26800/26800 - 24s - loss: 0.4176 - acc: 0.8234 - val_loss: 0.2814 - val_acc: 0.8900\n",
      "Epoch 8/10\n",
      "26800/26800 - 24s - loss: 0.2846 - acc: 0.8987 - val_loss: 0.3447 - val_acc: 0.8788\n",
      "Epoch 9/10\n",
      "26800/26800 - 24s - loss: 0.2008 - acc: 0.9315 - val_loss: 0.2983 - val_acc: 0.8972\n",
      "Epoch 10/10\n",
      "26800/26800 - 24s - loss: 0.1457 - acc: 0.9549 - val_loss: 0.4083 - val_acc: 0.8893\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 3; \n",
      " * init mode = uniform; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 24s - loss: 0.7048 - acc: 0.4984 - val_loss: 0.6931 - val_acc: 0.5067\n",
      "Epoch 2/10\n",
      "26800/26800 - 24s - loss: 0.6942 - acc: 0.4981 - val_loss: 0.6932 - val_acc: 0.4925\n",
      "Epoch 3/10\n",
      "26800/26800 - 24s - loss: 0.6953 - acc: 0.5035 - val_loss: 0.6931 - val_acc: 0.5070\n",
      "Epoch 4/10\n",
      "26800/26800 - 24s - loss: 0.6943 - acc: 0.5053 - val_loss: 0.6932 - val_acc: 0.4933\n",
      "Epoch 5/10\n",
      "26800/26800 - 25s - loss: 0.6934 - acc: 0.5008 - val_loss: 0.6932 - val_acc: 0.4933\n",
      "Epoch 6/10\n",
      "26800/26800 - 24s - loss: 0.6933 - acc: 0.5019 - val_loss: 0.6932 - val_acc: 0.4933\n"
     ]
    }
   ],
   "source": [
    "dict_layers_num_histories = {}\n",
    "best_layer_num = 1\n",
    "best_layer_num_acc = 0\n",
    "for i in hyperparameters['layer_num']:\n",
    "    history = get_fitted_model(dropout = best_dropout, layer_num = i)\n",
    "    if max(history.history['val_acc']) > best_layer_num_acc:\n",
    "        best_layer_num = i\n",
    "        best_layer_num_acc = max(history.history['val_acc'])\n",
    "    dict_layers_num_histories[str(i)] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9026866\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(max(dict_layers_num_histories[str(best_layer_num)].history['val_acc']))\n",
    "print(best_layer_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = uniform; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 24s - loss: 0.8725 - acc: 0.4993 - val_loss: 0.6931 - val_acc: 0.5066\n",
      "Epoch 2/10\n",
      "26800/26800 - 24s - loss: 0.7035 - acc: 0.4997 - val_loss: 0.6933 - val_acc: 0.4922\n",
      "Epoch 3/10\n",
      "26800/26800 - 24s - loss: 0.7029 - acc: 0.5096 - val_loss: 0.6906 - val_acc: 0.5739\n",
      "Epoch 4/10\n",
      "26800/26800 - 24s - loss: 0.6187 - acc: 0.6384 - val_loss: 0.3721 - val_acc: 0.8519\n",
      "Epoch 5/10\n",
      "26800/26800 - 24s - loss: 0.3225 - acc: 0.8694 - val_loss: 0.2793 - val_acc: 0.8827\n",
      "Epoch 6/10\n",
      "26800/26800 - 24s - loss: 0.1860 - acc: 0.9310 - val_loss: 0.2715 - val_acc: 0.8930\n",
      "Epoch 7/10\n",
      "26800/26800 - 24s - loss: 0.1105 - acc: 0.9613 - val_loss: 0.2745 - val_acc: 0.8999\n",
      "Epoch 8/10\n",
      "26800/26800 - 24s - loss: 0.0599 - acc: 0.9791 - val_loss: 0.4513 - val_acc: 0.8772\n",
      "Epoch 9/10\n",
      "26800/26800 - 24s - loss: 0.0308 - acc: 0.9897 - val_loss: 0.4388 - val_acc: 0.8894\n",
      "Epoch 10/10\n",
      "26800/26800 - 24s - loss: 0.0107 - acc: 0.9971 - val_loss: 0.4732 - val_acc: 0.8994\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = lecun_uniform; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 24s - loss: 0.8676 - acc: 0.5007 - val_loss: 0.6931 - val_acc: 0.5067\n",
      "Epoch 2/10\n",
      "26800/26800 - 24s - loss: 0.7023 - acc: 0.4990 - val_loss: 0.6931 - val_acc: 0.5079\n",
      "Epoch 3/10\n",
      "26800/26800 - 24s - loss: 0.6732 - acc: 0.5601 - val_loss: 0.4752 - val_acc: 0.7973\n",
      "Epoch 4/10\n",
      "26800/26800 - 24s - loss: 0.3588 - acc: 0.8523 - val_loss: 0.2722 - val_acc: 0.8890\n",
      "Epoch 5/10\n",
      "26800/26800 - 24s - loss: 0.2085 - acc: 0.9226 - val_loss: 0.2829 - val_acc: 0.8897\n",
      "Epoch 6/10\n",
      "26800/26800 - 24s - loss: 0.1311 - acc: 0.9532 - val_loss: 0.3172 - val_acc: 0.8921\n",
      "Epoch 7/10\n",
      "26800/26800 - 24s - loss: 0.0745 - acc: 0.9743 - val_loss: 0.3348 - val_acc: 0.9030\n",
      "Epoch 8/10\n",
      "26800/26800 - 24s - loss: 0.0270 - acc: 0.9929 - val_loss: 0.4006 - val_acc: 0.9030\n",
      "Epoch 9/10\n",
      "26800/26800 - 24s - loss: 0.0219 - acc: 0.9935 - val_loss: 0.3972 - val_acc: 0.9028\n",
      "Epoch 10/10\n",
      "26800/26800 - 24s - loss: 0.0197 - acc: 0.9949 - val_loss: 0.3984 - val_acc: 0.9025\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = normal; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 24s - loss: 0.8741 - acc: 0.4890 - val_loss: 0.6931 - val_acc: 0.5069\n",
      "Epoch 2/10\n",
      "26800/26800 - 24s - loss: 0.6996 - acc: 0.4953 - val_loss: 0.6932 - val_acc: 0.4930\n",
      "Epoch 3/10\n",
      "26800/26800 - 24s - loss: 0.6969 - acc: 0.5035 - val_loss: 0.6939 - val_acc: 0.5073\n",
      "Epoch 4/10\n",
      "26800/26800 - 24s - loss: 0.6961 - acc: 0.5169 - val_loss: 0.6756 - val_acc: 0.6407\n",
      "Epoch 5/10\n",
      "26800/26800 - 24s - loss: 0.5817 - acc: 0.6722 - val_loss: 0.3590 - val_acc: 0.8540\n",
      "Epoch 6/10\n",
      "26800/26800 - 24s - loss: 0.3347 - acc: 0.8586 - val_loss: 0.2635 - val_acc: 0.8936\n",
      "Epoch 7/10\n",
      "26800/26800 - 24s - loss: 0.2158 - acc: 0.9188 - val_loss: 0.2527 - val_acc: 0.9004\n",
      "Epoch 8/10\n",
      "26800/26800 - 24s - loss: 0.1321 - acc: 0.9512 - val_loss: 0.3730 - val_acc: 0.8694\n",
      "Epoch 9/10\n",
      "26800/26800 - 24s - loss: 0.0797 - acc: 0.9735 - val_loss: 0.3277 - val_acc: 0.8978\n",
      "Epoch 10/10\n",
      "26800/26800 - 24s - loss: 0.0455 - acc: 0.9840 - val_loss: 0.4259 - val_acc: 0.8937\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = glorot_normal; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 24s - loss: 0.8386 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5067\n",
      "Epoch 2/10\n",
      "26800/26800 - 24s - loss: 0.7005 - acc: 0.4992 - val_loss: 0.6931 - val_acc: 0.4933\n",
      "Epoch 3/10\n",
      "26800/26800 - 24s - loss: 0.6954 - acc: 0.5007 - val_loss: 0.6917 - val_acc: 0.5796\n",
      "Epoch 4/10\n",
      "26800/26800 - 24s - loss: 0.5697 - acc: 0.6906 - val_loss: 0.3283 - val_acc: 0.8639\n",
      "Epoch 5/10\n",
      "26800/26800 - 24s - loss: 0.2769 - acc: 0.8953 - val_loss: 0.2653 - val_acc: 0.8937\n",
      "Epoch 6/10\n",
      "26800/26800 - 24s - loss: 0.1691 - acc: 0.9377 - val_loss: 0.2682 - val_acc: 0.9051\n",
      "Epoch 7/10\n",
      "26800/26800 - 24s - loss: 0.1007 - acc: 0.9643 - val_loss: 0.3012 - val_acc: 0.8954\n",
      "Epoch 8/10\n",
      "26800/26800 - 24s - loss: 0.0533 - acc: 0.9814 - val_loss: 0.3929 - val_acc: 0.9027\n",
      "Epoch 9/10\n",
      "26800/26800 - 24s - loss: 0.0165 - acc: 0.9957 - val_loss: 0.4305 - val_acc: 0.9034\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = glorot_uniform; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 24s - loss: 0.8379 - acc: 0.4943 - val_loss: 0.6931 - val_acc: 0.5067\n",
      "Epoch 2/10\n",
      "26800/26800 - 24s - loss: 0.7035 - acc: 0.4995 - val_loss: 0.6932 - val_acc: 0.4933\n",
      "Epoch 3/10\n",
      "26800/26800 - 24s - loss: 0.7016 - acc: 0.4983 - val_loss: 0.6932 - val_acc: 0.4933\n",
      "Epoch 4/10\n",
      "26800/26800 - 24s - loss: 0.6973 - acc: 0.5198 - val_loss: 0.6604 - val_acc: 0.5230\n",
      "Epoch 5/10\n",
      "26800/26800 - 24s - loss: 0.4240 - acc: 0.8099 - val_loss: 0.2897 - val_acc: 0.8740\n",
      "Epoch 6/10\n",
      "26800/26800 - 24s - loss: 0.2310 - acc: 0.9132 - val_loss: 0.2750 - val_acc: 0.8937\n",
      "Epoch 7/10\n",
      "26800/26800 - 24s - loss: 0.1361 - acc: 0.9511 - val_loss: 0.2875 - val_acc: 0.9018\n",
      "Epoch 8/10\n",
      "26800/26800 - 24s - loss: 0.0745 - acc: 0.9737 - val_loss: 0.3974 - val_acc: 0.8937\n",
      "Epoch 9/10\n",
      "26800/26800 - 24s - loss: 0.0390 - acc: 0.9867 - val_loss: 0.4460 - val_acc: 0.8957\n",
      "Epoch 10/10\n",
      "26800/26800 - 24s - loss: 0.0123 - acc: 0.9965 - val_loss: 0.4803 - val_acc: 0.8969\n"
     ]
    }
   ],
   "source": [
    "dict_init_mode_histories = {}\n",
    "best_init_mode = 'uniform'\n",
    "best_init_mode_acc = 0\n",
    "for i in hyperparameters['init_mode']:\n",
    "    history = get_fitted_model(dropout = best_dropout, layer_num = best_layer_num, init_mode = i)\n",
    "    if max(history.history['val_acc']) > best_init_mode_acc:\n",
    "        best_init_mode = i\n",
    "        best_init_mode_acc = max(history.history['val_acc'])\n",
    "    dict_init_mode_histories[str(i)] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90507466\n",
      "glorot_normal\n"
     ]
    }
   ],
   "source": [
    "print(max(dict_init_mode_histories[str(best_init_mode)].history['val_acc']))\n",
    "print(best_init_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = glorot_normal; \n",
      " * batch size = 128\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 23s - loss: 0.7815 - acc: 0.4925 - val_loss: 0.6931 - val_acc: 0.5067\n",
      "Epoch 2/10\n",
      "26800/26800 - 23s - loss: 0.6961 - acc: 0.4986 - val_loss: 0.6932 - val_acc: 0.4933\n",
      "Epoch 3/10\n",
      "26800/26800 - 23s - loss: 0.6987 - acc: 0.4971 - val_loss: 0.6932 - val_acc: 0.4936\n",
      "Epoch 4/10\n",
      "26800/26800 - 23s - loss: 0.6326 - acc: 0.6016 - val_loss: 0.3993 - val_acc: 0.8261\n",
      "Epoch 5/10\n",
      "26800/26800 - 24s - loss: 0.3192 - acc: 0.8741 - val_loss: 0.2728 - val_acc: 0.8870\n",
      "Epoch 6/10\n",
      "26800/26800 - 24s - loss: 0.1895 - acc: 0.9306 - val_loss: 0.2896 - val_acc: 0.8888\n",
      "Epoch 7/10\n",
      "26800/26800 - 24s - loss: 0.1131 - acc: 0.9593 - val_loss: 0.2787 - val_acc: 0.9016\n",
      "Epoch 8/10\n",
      "26800/26800 - 24s - loss: 0.0637 - acc: 0.9787 - val_loss: 0.4394 - val_acc: 0.8828\n",
      "Epoch 9/10\n",
      "26800/26800 - 24s - loss: 0.0228 - acc: 0.9936 - val_loss: 0.4159 - val_acc: 0.8985\n",
      "Epoch 10/10\n",
      "26800/26800 - 24s - loss: 0.0200 - acc: 0.9950 - val_loss: 0.4215 - val_acc: 0.8969\n",
      "\n",
      " Training Model with: \n",
      " * dropout = 0.8; \n",
      " * number of hidden layers = 1; \n",
      " * init mode = glorot_normal; \n",
      " * batch size = 512\n",
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 17s - loss: 1.4291 - acc: 0.4974 - val_loss: 0.6932 - val_acc: 0.4934\n",
      "Epoch 2/10\n",
      "26800/26800 - 16s - loss: 0.7129 - acc: 0.4940 - val_loss: 0.6932 - val_acc: 0.4933\n",
      "Epoch 3/10\n",
      "26800/26800 - 16s - loss: 0.7010 - acc: 0.4990 - val_loss: 0.6932 - val_acc: 0.4933\n",
      "Epoch 4/10\n",
      "26800/26800 - 17s - loss: 0.6934 - acc: 0.4967 - val_loss: 0.6932 - val_acc: 0.4933\n"
     ]
    }
   ],
   "source": [
    "dict_batch_size_histories = {}\n",
    "best_batch_size = 128\n",
    "best_batch_size_acc = 0\n",
    "for i in hyperparameters['batch_size']:\n",
    "    history = get_fitted_model(dropout = best_dropout, layer_num = best_layer_num, \n",
    "                              init_mode = best_init_mode, batch_size = i)\n",
    "    if max(history.history['val_acc']) > best_batch_size_acc:\n",
    "        best_batch_size = i\n",
    "        best_batch_size_acc = max(history.history['val_acc'])\n",
    "    dict_batch_size_histories[str(i)] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9016418\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "print(max(dict_batch_size_histories[str(best_batch_size)].history['val_acc']))\n",
    "print(best_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "callbacks_list.append(\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath= 'models\\\\best_model_redone.h5',\n",
    "        save_weights_only=False,\n",
    "        monitor='val_acc',\n",
    "        save_best_only=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26800 samples, validate on 6700 samples\n",
      "Epoch 1/10\n",
      "26800/26800 - 25s - loss: 0.8556 - acc: 0.4916 - val_loss: 0.6931 - val_acc: 0.5067\n",
      "Epoch 2/10\n",
      "26800/26800 - 24s - loss: 0.6972 - acc: 0.5008 - val_loss: 0.6932 - val_acc: 0.4933\n",
      "Epoch 3/10\n",
      "26800/26800 - 24s - loss: 0.6977 - acc: 0.4999 - val_loss: 0.6932 - val_acc: 0.4933\n",
      "Epoch 4/10\n",
      "26800/26800 - 24s - loss: 0.7023 - acc: 0.5109 - val_loss: 0.6759 - val_acc: 0.5087\n",
      "Epoch 5/10\n",
      "26800/26800 - 24s - loss: 0.4609 - acc: 0.7788 - val_loss: 0.3065 - val_acc: 0.8687\n",
      "Epoch 6/10\n",
      "26800/26800 - 24s - loss: 0.2361 - acc: 0.9093 - val_loss: 0.2786 - val_acc: 0.8904\n",
      "Epoch 7/10\n",
      "26800/26800 - 24s - loss: 0.1415 - acc: 0.9491 - val_loss: 0.2854 - val_acc: 0.8991\n",
      "Epoch 8/10\n",
      "26800/26800 - 24s - loss: 0.0774 - acc: 0.9730 - val_loss: 0.3776 - val_acc: 0.8984\n",
      "Epoch 9/10\n",
      "26800/26800 - 24s - loss: 0.0394 - acc: 0.9865 - val_loss: 0.4137 - val_acc: 0.8973\n",
      "Epoch 10/10\n",
      "26800/26800 - 24s - loss: 0.0140 - acc: 0.9965 - val_loss: 0.4750 - val_acc: 0.8979\n"
     ]
    }
   ],
   "source": [
    "def get_best_model(dropout = 0.5, layer_num = 1, init_mode='uniform', batch_size = 128):\n",
    "\n",
    "    def add_layers():\n",
    "        for i in range (0, layer_num):\n",
    "            model.add(Dense(64, kernel_initializer=init_mode, activation='relu'))\n",
    "            model.add(Dropout(rate=dropout))\n",
    "    \n",
    "    EMBEDDING_DIM = 100\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(words_size, EMBEDDING_DIM, input_length=maxlen))\n",
    "    model.add(Flatten())\n",
    "    add_layers()\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(words_size, EMBEDDING_DIM, input_length=maxlen))\n",
    "    model.add(Flatten())\n",
    "    add_layers()\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "    history = model.fit(train_data, y_train,\n",
    "                        epochs=10,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_split=0.2,\n",
    "                        verbose=2)\n",
    "    #model.load_weights('./models/best_model.h5')\n",
    "    \n",
    "    #return model\n",
    "    return tf.keras.models.load_model(\"models\\\\best_model_redone.h5\" )\n",
    "\n",
    "best_model = get_best_model(dropout = best_dropout, layer_num = best_layer_num, \n",
    "                            init_mode = best_init_mode, batch_size = best_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500/16500 [==============================] - 5s 280us/sample - loss: 0.2882 - acc: 0.9032\n",
      "accuracy: 0.90321213%\n"
     ]
    }
   ],
   "source": [
    "#Testing the accuracy of the model\n",
    "\n",
    "test_result = best_model.evaluate(test_data, y_test)\n",
    "\n",
    "print ('accuracy: ' + str(test_result[1]) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16500, 2640)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tf.keras.models.load_model(\"models\\\\best_model_redone.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500/16500 [==============================] - 5s 279us/sample - loss: 0.2882 - acc: 0.9032s - loss: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28816987042354814, 0.90321213]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(test_data, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the black box algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('scripts', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/blackBox.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/blackBox.py\n",
    "\n",
    "import tensorflow as tf\n",
    "from scripts.preprocessing import Preprocesser\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "class BlackBox:\n",
    "    \n",
    "    def __init__(self):\n",
    "        with open('pickle\\\\tokenizer.pickle', 'rb') as f:\n",
    "            tokenizer, maxlen = pickle.load(f)\n",
    "            self.__tokenizer = tokenizer\n",
    "            self.__maxlen = maxlen\n",
    "        f.close()\n",
    "        self.__model = tf.keras.models.load_model(\"models\\\\best_model.h5\")\n",
    "        \n",
    "    def __text_preprocessing(self, text):\n",
    "        return Preprocesser.text_preprocessing(text)      \n",
    "        \n",
    "    def __tokenize(self, text):\n",
    "        sequences = self.__tokenizer.texts_to_sequences(text)\n",
    "        return pad_sequences(sequences, maxlen = self.__maxlen)\n",
    "        \n",
    "    def predict_sentiment(self, text):\n",
    "        text = self.__text_preprocessing(text)\n",
    "        seq = self.__tokenize([text])\n",
    "        return self.__model.predict(seq).take(0)\n",
    "    \n",
    "    def evaluate(self, test, label):\n",
    "        self.__model.evaluate(test,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.blackBox import BlackBox\n",
    "\n",
    "#import scripts.blackBox as blackbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "black_box = BlackBox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('pickle\\\\data.pickle', 'rb') as f:\n",
    "    x_test, y_test = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#black_box.evaluate(test_data, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[y_test[7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8256238"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "black_box.predict_sentiment(x_test[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
